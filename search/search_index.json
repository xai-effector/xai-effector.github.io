{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p><code>Effector</code> is a python package for global and regional effect analysis.</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p><code>Effector</code> is compatible with <code>Python 3.7+</code>. We recommend to first create a virtual environment with <code>conda</code>:</p> <pre><code>conda create -n effector python=3.7\nconda activate effector\n</code></pre> <p>and then install <code>Effector</code> via <code>pip</code>:</p> <pre><code>pip install effector\n</code></pre>"},{"location":"#motivation","title":"Motivation","text":""},{"location":"#global-effect","title":"Global Effect","text":"<p>Global effect is one the simplest ways to interpret a black-box model; it simply shows how a particular feature relates to the model's output. Given the dataset <code>X</code> (<code>np.ndarray</code>) and the black-box predictive function <code>model</code> (<code>callable</code>),  you can use <code>Effector</code> to get the global effect of a <code>feature</code> in a single line of code:</p> <pre><code># for Robust and Heterogeneity-aware ALE (RHALE)\nRHALE(data=X, model=model).plot(feature)\n</code></pre> <p>For example, the following code shows the global effect of the feature hour (<code>hr</code>) on the  number of bikes (<code>cnt</code>) rent within a day (check this  notebook for more details). It is easy to interpret what the black-box model has learned: There are two peaks in rentals during a day, one in the morning and one in the evening, where people go to work and return home, respectively:</p> <p></p>"},{"location":"#heterogeneity","title":"Heterogeneity","text":"<p>However, there are cases where the global effect can be misleading. This happens  when there are many particular instances that deviate from the global effect. In <code>Effector</code>, the user can understand where the global effect is misleading,  using the argument <code>heterogeneity</code>, while plotting:</p> <pre><code># for RHALE\nRHALE(data=X, model=model).plot(feature, heterogeneity=True)\n</code></pre> <p></p> <p>For more details, check the global effect tutorial.</p>"},{"location":"#regional-effect","title":"Regional Effect","text":"<p>In this cases, it is useful to search if there are subspaces where the effect. In <code>Effector</code> this can be also done in a single line of code:</p> <pre><code>RegionalRHALE(data=X, model=model).plot(feature=0, node_idx=1, heterogeneity=True)\nRegionalRHALE(data=X, model=model).plot(feature=0, node_idx=2, heterogeneity=True)\n</code></pre> <p>In the bike sharing dataset, we can see that the effect of the feature hour (<code>hr</code>)  follows to different patterns (regional effects) depending on the day of the week (subspaces). In weekdays, the effect is similar to the global effect, while in weekends, the effect is completely different; there is a single peak in the morning when people rent bikes to go for sightseeing.</p> <p> </p> <p>For more details, check the regional effect tutorial.</p>"},{"location":"#methods-and-publications","title":"Methods and Publications","text":""},{"location":"#methods","title":"Methods","text":"<p><code>Effector</code> implements the following methods:</p> Method Global Effect Regional Effect PDP <code>PDP</code> <code>RegionalPDP</code> d-PDP <code>DerivativePDP</code> <code>RegionalDerivativePDP</code> ALE <code>ALE</code> <code>RegionalALE</code> RHALE <code>RHALE</code> <code>RegionalRHALE</code> SHAP-DP <code>SHAPDependence</code> <code>RegionalSHAP</code>"},{"location":"#publications","title":"Publications","text":"<p>The methods above are based on the following publications:</p> <ul> <li>PDP and d-PDP: Friedman, Jerome H. \"Greedy function approximation: a gradient boosting machine.\" Annals of statistics (2001): 1189-1232.</li> <li>ALE: Apley, Daniel W. \"Visualizing the effects of predictor variables in black box supervised learning models.\" arXiv preprint arXiv:1612.08468 (2016).</li> <li>RHALE: Gkolemis, Vasilis, \"RHALE</li> <li> <p>SHAP-DP: Lundberg, Scott M., and Su-In Lee. \"A unified approach to interpreting model predictions.\" Advances in neural information processing systems. 2017.</p> </li> <li> <p>Regional Effect:</p> </li> <li> <p>REPID: Regional Effect Plots with implicit Interaction Detection</p> </li> <li>Decomposing Global Feature Effects Based on Feature Interactions</li> <li>Regionally Additive Models: Explainable-by-design models minimizing feature interactions</li> </ul>"},{"location":"api/","title":"API reference","text":""},{"location":"api/#feature-effect-methods","title":"Feature Effect Methods","text":""},{"location":"api/#effector.global_effect_ale.ALEBase","title":"<code>effector.global_effect_ale.ALEBase</code>","text":"<p>         Bases: <code>GlobalEffectBase</code></p> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_ale.py</code> <pre><code>class ALEBase(GlobalEffectBase):\n\n    def __init__(\n            self,\n            data: np.ndarray,\n            model: callable,\n            nof_instances: Union[int, str] = \"all\",\n            axis_limits: Optional[np.ndarray] = None,\n            avg_output: Optional[float] = None,\n            feature_names: Optional[List] = None,\n            target_name: Optional[str] = None,\n            method_name: str = \"ALE\",\n    ):\n        self.method_name = method_name\n        super(ALEBase, self).__init__(\n            method_name,\n            data,\n            model,\n            nof_instances,\n            axis_limits,\n            avg_output,\n            feature_names,\n            target_name\n        )\n\n    @abstractmethod\n    def _fit_feature(self,\n                     feature: int,\n                     binning_method: typing.Union[str, bm.DynamicProgramming, bm.Greedy, bm.Fixed] = \"greedy\"\n                     ) -&gt; typing.Dict:\n        raise NotImplementedError\n\n    @abstractmethod\n    def fit(self,\n            features: typing.Union[int, str, list] = \"all\",\n            **kwargs) -&gt; None:\n        raise NotImplementedError\n\n    def _compute_norm_const(\n        self, feature: int, method: str = \"zero_integral\", nof_points: int = 100\n    ) -&gt; float:\n\"\"\"Compute the normalization constant.\"\"\"\n        assert method in [\"zero_integral\", \"zero_start\"]\n\n        def create_partial_eval(feat):\n            return lambda x: self._eval_unnorm(feat, x, heterogeneity=False)\n\n        partial_eval = create_partial_eval(feature)\n        start = self.axis_limits[0, feature]\n        stop = self.axis_limits[1, feature]\n\n        if method == \"zero_integral\":\n            z = utils_integrate.mean_1d_linspace(partial_eval, start, stop, nof_points)\n        else:\n            z = partial_eval(np.array([start])).item()\n        return z\n\n    def _fit_loop(self, features, binning_method, centering):\n        features = helpers.prep_features(features, self.dim)\n        centering = helpers.prep_centering(centering)\n        for s in features:\n            # compute all information required for plotting and evaluating the feature effect\n            self.feature_effect[\"feature_\" + str(s)] = self._fit_feature(\n                s, binning_method\n            )\n\n            # append the \"norm_const\" to the feature effect if centering is not False\n            if centering is not False:\n                self.feature_effect[\"feature_\" + str(s)][\"norm_const\"] = self._compute_norm_const(s, method=centering)\n            else:\n                self.feature_effect[\"feature_\" + str(s)][\"norm_const\"] = self.empty_symbol\n\n            self.is_fitted[s] = True\n            self.method_args[\"feature_\" + str(s)] = {\n                \"centering\": centering,\n            }\n\n    def _eval_unnorm(self, feature: int, x: np.ndarray, heterogeneity: bool = False):\n        params = self.feature_effect[\"feature_\" + str(feature)]\n        y = utils.compute_accumulated_effect(\n            x, limits=params[\"limits\"], bin_effect=params[\"bin_effect\"], dx=params[\"dx\"]\n        )\n        if heterogeneity:\n            std = utils.compute_accumulated_effect(\n                x,\n                limits=params[\"limits\"],\n                bin_effect=np.sqrt(params[\"bin_variance\"]),\n                dx=params[\"dx\"],\n            )\n\n            return y, std\n        else:\n            return y\n\n    def eval(\n        self,\n        feature: int,\n        xs: np.ndarray,\n        heterogeneity: bool = False,\n        centering: typing.Union[bool, str] = False,\n    ) -&gt; typing.Union[np.ndarray, typing.Tuple[np.ndarray, np.ndarray]]:\n\"\"\"Evalueate the (RH)ALE feature effect of feature `feature` at points `xs`.\n\n        Notes:\n            This is a common method inherited by both ALE and RHALE.\n\n        Args:\n            feature: index of feature of interest\n            xs: the points along the s-th axis to evaluate the FE plot\n              - `np.ndarray` of shape `(T, )`\n            heterogeneity: whether to return heterogeneity:\n\n                  - `False`, returns the mean effect `y` at the given `xs`\n                  - `True`, returns a tuple `(y, H)` of two `ndarrays`; `y` is the mean effect and `H` is the\n                  heterogeneity evaluated at `xs`\n\n            centering: whether to center the plot:\n\n                - `False` means no centering\n                - `True` or `zero_integral` centers around the `y` axis.\n                - `zero_start` starts the plot from `y=0`.\n        Returns:\n            the mean effect `y`, if `heterogeneity=False` (default) or a tuple `(y, std)` otherwise\n\n        \"\"\"\n        centering = helpers.prep_centering(centering)\n\n        if self.refit(feature, centering):\n            self.fit(features=feature, centering=centering)\n\n        # Check if the lower bound is less than the upper bound\n        assert self.axis_limits[0, feature] &lt; self.axis_limits[1, feature]\n\n        # Evaluate the feature\n        yy = self._eval_unnorm(feature, xs, heterogeneity=heterogeneity)\n        y, std = yy if heterogeneity else (yy, None)\n\n        # Center if asked\n        y = (\n            y - self.feature_effect[\"feature_\" + str(feature)][\"norm_const\"]\n            if centering\n            else y\n        )\n\n        return (y, std) if heterogeneity is not False else y\n\n    def plot(\n            self,\n            feature: int,\n            heterogeneity: bool = False,\n            centering: Union[bool, str] = False,\n            scale_x: Optional[dict] = None,\n            scale_y: Optional[dict] = None,\n            show_avg_output: bool = False,\n            y_limits: Optional[List] = None,\n            dy_limits: Optional[List] = None\n    ):\n\"\"\"\n        Plot the (RH)ALE feature effect of feature `feature`.\n\n        Notes:\n            This is a common method inherited by both ALE and RHALE.\n\n        Parameters:\n            feature: the feature to plot\n            heterogeneity: whether to plot the heterogeneity\n\n                  - `False`, plots only the mean effect\n                  - `True`, the std of the bin-effects will be plotted using a red vertical bar\n\n            centering: whether to center the plot:\n\n                - `False` means no centering\n                - `True` or `zero_integral` centers around the `y` axis.\n                - `zero_start` starts the plot from `y=0`.\n\n            scale_x: None or Dict with keys ['std', 'mean']\n\n                - If set to None, no scaling will be applied.\n                - If set to a dict, the x-axis will be scaled by the standard deviation and the mean.\n            scale_y: None or Dict with keys ['std', 'mean']\n\n                - If set to None, no scaling will be applied.\n                - If set to a dict, the y-axis will be scaled by the standard deviation and the mean.\n            show_avg_output: if True, the average output will be shown as a horizontal line.\n            y_limits: None or tuple, the limits of the y-axis\n\n                - If set to None, the limits of the y-axis are set automatically\n                - If set to a tuple, the limits are manually set\n\n            dy_limits: None or tuple, the limits of the dy-axis\n\n                - If set to None, the limits of the dy-axis are set automatically\n                - If set to a tuple, the limits are manually set\n        \"\"\"\n        heterogeneity = helpers.prep_confidence_interval(heterogeneity)\n        centering = helpers.prep_centering(centering)\n\n        # hack to fit the feature if not fitted\n        self.eval(\n            feature, np.array([self.axis_limits[0, feature]]), centering=centering\n        )\n\n        if show_avg_output:\n            avg_output = helpers.prep_avg_output(self.data, self.model, self.avg_output, scale_y)\n        else:\n            avg_output = None\n\n        vis.ale_plot(\n            self.feature_effect[\"feature_\" + str(feature)],\n            self.eval,\n            feature,\n            centering=centering,\n            error=heterogeneity,\n            scale_x=scale_x,\n            scale_y=scale_y,\n            title=self.method_name + \" plot\",\n            avg_output=avg_output,\n            feature_names=self.feature_names,\n            target_name=self.target_name,\n            y_limits=y_limits,\n            dy_limits=dy_limits\n        )\n</code></pre>"},{"location":"api/#effector.global_effect_ale.ALEBase.eval","title":"<code>eval(feature, xs, heterogeneity=False, centering=False)</code>","text":"<p>Evalueate the (RH)ALE feature effect of feature <code>feature</code> at points <code>xs</code>.</p> Notes <p>This is a common method inherited by both ALE and RHALE.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>int</code> <p>index of feature of interest</p> required <code>xs</code> <code>np.ndarray</code> <p>the points along the s-th axis to evaluate the FE plot - <code>np.ndarray</code> of shape <code>(T, )</code></p> required <code>heterogeneity</code> <code>bool</code> <p>whether to return heterogeneity:</p> <ul> <li><code>False</code>, returns the mean effect <code>y</code> at the given <code>xs</code></li> <li><code>True</code>, returns a tuple <code>(y, H)</code> of two <code>ndarrays</code>; <code>y</code> is the mean effect and <code>H</code> is the   heterogeneity evaluated at <code>xs</code></li> </ul> <code>False</code> <code>centering</code> <code>typing.Union[bool, str]</code> <p>whether to center the plot:</p> <ul> <li><code>False</code> means no centering</li> <li><code>True</code> or <code>zero_integral</code> centers around the <code>y</code> axis.</li> <li><code>zero_start</code> starts the plot from <code>y=0</code>.</li> </ul> <code>False</code> <p>Returns:</p> Type Description <code>typing.Union[np.ndarray, typing.Tuple[np.ndarray, np.ndarray]]</code> <p>the mean effect <code>y</code>, if <code>heterogeneity=False</code> (default) or a tuple <code>(y, std)</code> otherwise</p> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_ale.py</code> <pre><code>def eval(\n    self,\n    feature: int,\n    xs: np.ndarray,\n    heterogeneity: bool = False,\n    centering: typing.Union[bool, str] = False,\n) -&gt; typing.Union[np.ndarray, typing.Tuple[np.ndarray, np.ndarray]]:\n\"\"\"Evalueate the (RH)ALE feature effect of feature `feature` at points `xs`.\n\n    Notes:\n        This is a common method inherited by both ALE and RHALE.\n\n    Args:\n        feature: index of feature of interest\n        xs: the points along the s-th axis to evaluate the FE plot\n          - `np.ndarray` of shape `(T, )`\n        heterogeneity: whether to return heterogeneity:\n\n              - `False`, returns the mean effect `y` at the given `xs`\n              - `True`, returns a tuple `(y, H)` of two `ndarrays`; `y` is the mean effect and `H` is the\n              heterogeneity evaluated at `xs`\n\n        centering: whether to center the plot:\n\n            - `False` means no centering\n            - `True` or `zero_integral` centers around the `y` axis.\n            - `zero_start` starts the plot from `y=0`.\n    Returns:\n        the mean effect `y`, if `heterogeneity=False` (default) or a tuple `(y, std)` otherwise\n\n    \"\"\"\n    centering = helpers.prep_centering(centering)\n\n    if self.refit(feature, centering):\n        self.fit(features=feature, centering=centering)\n\n    # Check if the lower bound is less than the upper bound\n    assert self.axis_limits[0, feature] &lt; self.axis_limits[1, feature]\n\n    # Evaluate the feature\n    yy = self._eval_unnorm(feature, xs, heterogeneity=heterogeneity)\n    y, std = yy if heterogeneity else (yy, None)\n\n    # Center if asked\n    y = (\n        y - self.feature_effect[\"feature_\" + str(feature)][\"norm_const\"]\n        if centering\n        else y\n    )\n\n    return (y, std) if heterogeneity is not False else y\n</code></pre>"},{"location":"api/#effector.global_effect_ale.ALEBase.plot","title":"<code>plot(feature, heterogeneity=False, centering=False, scale_x=None, scale_y=None, show_avg_output=False, y_limits=None, dy_limits=None)</code>","text":"<p>Plot the (RH)ALE feature effect of feature <code>feature</code>.</p> Notes <p>This is a common method inherited by both ALE and RHALE.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>int</code> <p>the feature to plot</p> required <code>heterogeneity</code> <code>bool</code> <p>whether to plot the heterogeneity</p> <ul> <li><code>False</code>, plots only the mean effect</li> <li><code>True</code>, the std of the bin-effects will be plotted using a red vertical bar</li> </ul> <code>False</code> <code>centering</code> <code>Union[bool, str]</code> <p>whether to center the plot:</p> <ul> <li><code>False</code> means no centering</li> <li><code>True</code> or <code>zero_integral</code> centers around the <code>y</code> axis.</li> <li><code>zero_start</code> starts the plot from <code>y=0</code>.</li> </ul> <code>False</code> <code>scale_x</code> <code>Optional[dict]</code> <p>None or Dict with keys ['std', 'mean']</p> <ul> <li>If set to None, no scaling will be applied.</li> <li>If set to a dict, the x-axis will be scaled by the standard deviation and the mean.</li> </ul> <code>None</code> <code>scale_y</code> <code>Optional[dict]</code> <p>None or Dict with keys ['std', 'mean']</p> <ul> <li>If set to None, no scaling will be applied.</li> <li>If set to a dict, the y-axis will be scaled by the standard deviation and the mean.</li> </ul> <code>None</code> <code>show_avg_output</code> <code>bool</code> <p>if True, the average output will be shown as a horizontal line.</p> <code>False</code> <code>y_limits</code> <code>Optional[List]</code> <p>None or tuple, the limits of the y-axis</p> <ul> <li>If set to None, the limits of the y-axis are set automatically</li> <li>If set to a tuple, the limits are manually set</li> </ul> <code>None</code> <code>dy_limits</code> <code>Optional[List]</code> <p>None or tuple, the limits of the dy-axis</p> <ul> <li>If set to None, the limits of the dy-axis are set automatically</li> <li>If set to a tuple, the limits are manually set</li> </ul> <code>None</code> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_ale.py</code> <pre><code>def plot(\n        self,\n        feature: int,\n        heterogeneity: bool = False,\n        centering: Union[bool, str] = False,\n        scale_x: Optional[dict] = None,\n        scale_y: Optional[dict] = None,\n        show_avg_output: bool = False,\n        y_limits: Optional[List] = None,\n        dy_limits: Optional[List] = None\n):\n\"\"\"\n    Plot the (RH)ALE feature effect of feature `feature`.\n\n    Notes:\n        This is a common method inherited by both ALE and RHALE.\n\n    Parameters:\n        feature: the feature to plot\n        heterogeneity: whether to plot the heterogeneity\n\n              - `False`, plots only the mean effect\n              - `True`, the std of the bin-effects will be plotted using a red vertical bar\n\n        centering: whether to center the plot:\n\n            - `False` means no centering\n            - `True` or `zero_integral` centers around the `y` axis.\n            - `zero_start` starts the plot from `y=0`.\n\n        scale_x: None or Dict with keys ['std', 'mean']\n\n            - If set to None, no scaling will be applied.\n            - If set to a dict, the x-axis will be scaled by the standard deviation and the mean.\n        scale_y: None or Dict with keys ['std', 'mean']\n\n            - If set to None, no scaling will be applied.\n            - If set to a dict, the y-axis will be scaled by the standard deviation and the mean.\n        show_avg_output: if True, the average output will be shown as a horizontal line.\n        y_limits: None or tuple, the limits of the y-axis\n\n            - If set to None, the limits of the y-axis are set automatically\n            - If set to a tuple, the limits are manually set\n\n        dy_limits: None or tuple, the limits of the dy-axis\n\n            - If set to None, the limits of the dy-axis are set automatically\n            - If set to a tuple, the limits are manually set\n    \"\"\"\n    heterogeneity = helpers.prep_confidence_interval(heterogeneity)\n    centering = helpers.prep_centering(centering)\n\n    # hack to fit the feature if not fitted\n    self.eval(\n        feature, np.array([self.axis_limits[0, feature]]), centering=centering\n    )\n\n    if show_avg_output:\n        avg_output = helpers.prep_avg_output(self.data, self.model, self.avg_output, scale_y)\n    else:\n        avg_output = None\n\n    vis.ale_plot(\n        self.feature_effect[\"feature_\" + str(feature)],\n        self.eval,\n        feature,\n        centering=centering,\n        error=heterogeneity,\n        scale_x=scale_x,\n        scale_y=scale_y,\n        title=self.method_name + \" plot\",\n        avg_output=avg_output,\n        feature_names=self.feature_names,\n        target_name=self.target_name,\n        y_limits=y_limits,\n        dy_limits=dy_limits\n    )\n</code></pre>"},{"location":"api/#effector.global_effect_ale.ALE","title":"<code>effector.global_effect_ale.ALE</code>","text":"<p>         Bases: <code>ALEBase</code></p> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_ale.py</code> <pre><code>class ALE(ALEBase):\n    def __init__(\n            self,\n            data: np.ndarray,\n            model: callable,\n            nof_instances: Union[int, str] = \"all\",\n            axis_limits: Optional[np.ndarray] = None,\n            avg_output: Optional[float] = None,\n            feature_names: Optional[List] = None,\n            target_name: Optional[str] = None,\n    ):\n\"\"\"\n        Constructor for the ALE plot.\n\n        Definition:\n            ALE is defined as:\n            $$\n            \\hat{f}^{ALE}(x_s) = TODO\n            $$\n\n            The heterogeneity is:\n            $$\n            TODO\n            $$\n\n            The std of the bin-effects is:\n            $$\n            TODO\n            $$\n\n        Notes:\n            - The required parameters are `data` and `model`. The rest are optional.\n\n        Args:\n            data: the design matrix\n\n                - shape: `(N,D)`\n            model: the black-box model. Must be a `Callable` with:\n\n                - input: `ndarray` of shape `(N, D)`\n                - output: `ndarray` of shape `(N, )`\n\n            nof_instances: the number of instances to use for the explanation\n\n                - use an `int`, to specify the number of instances\n                - use `\"all\"`, to use all the instances\n\n            axis_limits: The limits of the feature effect plot along each axis\n\n                - use a `ndarray` of shape `(2, D)`, to specify them manually\n                - use `None`, to be inferred from the data\n\n            avg_output: the average output of the model on the data\n\n                - use a `float`, to specify it manually\n                - use `None`, to be inferred as `np.mean(model(data))`\n\n            feature_names: The names of the features\n\n                - use a `list` of `str`, to specify the name manually. For example: `                  [\"age\", \"weight\", ...]`\n                - use `None`, to keep the default names: `[\"x_0\", \"x_1\", ...]`\n\n            target_name: The name of the target variable\n\n                - use a `str`, to specify it name manually. For example: `\"price\"`\n                - use `None`, to keep the default name: `\"y\"`\n        \"\"\"\n        super(ALE, self).__init__(\n            data, model, nof_instances, axis_limits, avg_output, feature_names, target_name, \"ALE\"\n        )\n\n    def _fit_feature(self, feature: int, binning_method=\"fixed\") -&gt; typing.Dict:\n\n        # drop points outside of limits\n        ind = np.logical_and(\n            self.data[:, feature] &gt;= self.axis_limits[0, feature],\n            self.data[:, feature] &lt;= self.axis_limits[1, feature],\n        )\n        data = self.data[ind, :]\n\n        # assertion\n        assert binning_method == \"fixed\" or isinstance(\n            binning_method, bm.Fixed\n        ), \"ALE can work only with the fixed binning method!\"\n\n        if isinstance(binning_method, str):\n            binning_method = bm.Fixed(nof_bins=20, min_points_per_bin=0)\n        bin_est = bm.find_limits(data, None, feature, self.axis_limits, binning_method)\n        bin_name = bin_est.__class__.__name__\n\n        # assert bins can be computed else raise error\n        assert bin_est.limits is not False, (\n            \"Impossible to compute bins with enough points for feature \"\n            + str(feature + 1)\n            + \" and binning strategy: \"\n            + bin_name\n            + \". Change bin strategy or \"\n            \"the parameters of the method\"\n        )\n\n        # compute data effect on bin limits\n        data_effect = utils.compute_local_effects(\n            data, self.model, bin_est.limits, feature\n        )\n\n        # compute the bin effect\n        dale_params = utils.compute_ale_params(\n            data[:, feature], data_effect, bin_est.limits\n        )\n        dale_params[\"alg_params\"] = \"fixed\"\n        return dale_params\n\n    def fit(\n        self,\n        features: typing.Union[int, str, list] = \"all\",\n        binning_method: typing.Union[str, bm.Fixed] = \"fixed\",\n        centering: typing.Union[bool, str] = \"zero_integral\",\n    ) -&gt; None:\n\"\"\"Fit the ALE plot.\n\n        Args:\n            features: the features to fit. If set to \"all\", all the features will be fitted.\n\n            binning_method:\n\n                - If set to `\"fixed\"`, the ALE plot will be computed with the  default values, which are\n                `20` bins with at least `10` points per bin and the featue is considered as categorical if it has\n                less than `15` unique values.\n                - If you want to change the parameters of the method, you pass an instance of the\n                class `effector.binning_methods.Fixed` with the desired parameters.\n                For example: `Fixed(nof_bins=20, min_points_per_bin=0, cat_limit=10)`\n\n            centering: whether to compute the normalization constant for centering the plot:\n\n                - `False` means no centering\n                - `True` or `zero_integral` centers around the `y` axis.\n                - `zero_start` starts the plot from `y=0`.\n        \"\"\"\n        assert binning_method == \"fixed\" or isinstance(\n            binning_method, bm.Fixed\n        ), \"ALE can work only with the fixed binning method!\"\n\n        self._fit_loop(features, binning_method, centering)\n</code></pre>"},{"location":"api/#effector.global_effect_ale.ALE.__init__","title":"<code>__init__(data, model, nof_instances='all', axis_limits=None, avg_output=None, feature_names=None, target_name=None)</code>","text":"<p>Constructor for the ALE plot.</p> Definition <p>ALE is defined as: $$ \\hat{f}^{ALE}(x_s) = TODO $$</p> <p>The heterogeneity is: $$ TODO $$</p> <p>The std of the bin-effects is: $$ TODO $$</p> Notes <ul> <li>The required parameters are <code>data</code> and <code>model</code>. The rest are optional.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray</code> <p>the design matrix</p> <ul> <li>shape: <code>(N,D)</code></li> </ul> required <code>model</code> <code>callable</code> <p>the black-box model. Must be a <code>Callable</code> with:</p> <ul> <li>input: <code>ndarray</code> of shape <code>(N, D)</code></li> <li>output: <code>ndarray</code> of shape <code>(N, )</code></li> </ul> required <code>nof_instances</code> <code>Union[int, str]</code> <p>the number of instances to use for the explanation</p> <ul> <li>use an <code>int</code>, to specify the number of instances</li> <li>use <code>\"all\"</code>, to use all the instances</li> </ul> <code>'all'</code> <code>axis_limits</code> <code>Optional[np.ndarray]</code> <p>The limits of the feature effect plot along each axis</p> <ul> <li>use a <code>ndarray</code> of shape <code>(2, D)</code>, to specify them manually</li> <li>use <code>None</code>, to be inferred from the data</li> </ul> <code>None</code> <code>avg_output</code> <code>Optional[float]</code> <p>the average output of the model on the data</p> <ul> <li>use a <code>float</code>, to specify it manually</li> <li>use <code>None</code>, to be inferred as <code>np.mean(model(data))</code></li> </ul> <code>None</code> <code>feature_names</code> <code>Optional[List]</code> <p>The names of the features</p> <ul> <li>use a <code>list</code> of <code>str</code>, to specify the name manually. For example: <code>[\"age\", \"weight\", ...]</code></li> <li>use <code>None</code>, to keep the default names: <code>[\"x_0\", \"x_1\", ...]</code></li> </ul> <code>None</code> <code>target_name</code> <code>Optional[str]</code> <p>The name of the target variable</p> <ul> <li>use a <code>str</code>, to specify it name manually. For example: <code>\"price\"</code></li> <li>use <code>None</code>, to keep the default name: <code>\"y\"</code></li> </ul> <code>None</code> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_ale.py</code> <pre><code>def __init__(\n        self,\n        data: np.ndarray,\n        model: callable,\n        nof_instances: Union[int, str] = \"all\",\n        axis_limits: Optional[np.ndarray] = None,\n        avg_output: Optional[float] = None,\n        feature_names: Optional[List] = None,\n        target_name: Optional[str] = None,\n):\n\"\"\"\n    Constructor for the ALE plot.\n\n    Definition:\n        ALE is defined as:\n        $$\n        \\hat{f}^{ALE}(x_s) = TODO\n        $$\n\n        The heterogeneity is:\n        $$\n        TODO\n        $$\n\n        The std of the bin-effects is:\n        $$\n        TODO\n        $$\n\n    Notes:\n        - The required parameters are `data` and `model`. The rest are optional.\n\n    Args:\n        data: the design matrix\n\n            - shape: `(N,D)`\n        model: the black-box model. Must be a `Callable` with:\n\n            - input: `ndarray` of shape `(N, D)`\n            - output: `ndarray` of shape `(N, )`\n\n        nof_instances: the number of instances to use for the explanation\n\n            - use an `int`, to specify the number of instances\n            - use `\"all\"`, to use all the instances\n\n        axis_limits: The limits of the feature effect plot along each axis\n\n            - use a `ndarray` of shape `(2, D)`, to specify them manually\n            - use `None`, to be inferred from the data\n\n        avg_output: the average output of the model on the data\n\n            - use a `float`, to specify it manually\n            - use `None`, to be inferred as `np.mean(model(data))`\n\n        feature_names: The names of the features\n\n            - use a `list` of `str`, to specify the name manually. For example: `                  [\"age\", \"weight\", ...]`\n            - use `None`, to keep the default names: `[\"x_0\", \"x_1\", ...]`\n\n        target_name: The name of the target variable\n\n            - use a `str`, to specify it name manually. For example: `\"price\"`\n            - use `None`, to keep the default name: `\"y\"`\n    \"\"\"\n    super(ALE, self).__init__(\n        data, model, nof_instances, axis_limits, avg_output, feature_names, target_name, \"ALE\"\n    )\n</code></pre>"},{"location":"api/#effector.global_effect_ale.ALE.fit","title":"<code>fit(features='all', binning_method='fixed', centering='zero_integral')</code>","text":"<p>Fit the ALE plot.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>typing.Union[int, str, list]</code> <p>the features to fit. If set to \"all\", all the features will be fitted.</p> <code>'all'</code> <code>binning_method</code> <code>typing.Union[str, bm.Fixed]</code> <ul> <li>If set to <code>\"fixed\"</code>, the ALE plot will be computed with the  default values, which are <code>20</code> bins with at least <code>10</code> points per bin and the featue is considered as categorical if it has less than <code>15</code> unique values.</li> <li>If you want to change the parameters of the method, you pass an instance of the class <code>effector.binning_methods.Fixed</code> with the desired parameters. For example: <code>Fixed(nof_bins=20, min_points_per_bin=0, cat_limit=10)</code></li> </ul> <code>'fixed'</code> <code>centering</code> <code>typing.Union[bool, str]</code> <p>whether to compute the normalization constant for centering the plot:</p> <ul> <li><code>False</code> means no centering</li> <li><code>True</code> or <code>zero_integral</code> centers around the <code>y</code> axis.</li> <li><code>zero_start</code> starts the plot from <code>y=0</code>.</li> </ul> <code>'zero_integral'</code> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_ale.py</code> <pre><code>def fit(\n    self,\n    features: typing.Union[int, str, list] = \"all\",\n    binning_method: typing.Union[str, bm.Fixed] = \"fixed\",\n    centering: typing.Union[bool, str] = \"zero_integral\",\n) -&gt; None:\n\"\"\"Fit the ALE plot.\n\n    Args:\n        features: the features to fit. If set to \"all\", all the features will be fitted.\n\n        binning_method:\n\n            - If set to `\"fixed\"`, the ALE plot will be computed with the  default values, which are\n            `20` bins with at least `10` points per bin and the featue is considered as categorical if it has\n            less than `15` unique values.\n            - If you want to change the parameters of the method, you pass an instance of the\n            class `effector.binning_methods.Fixed` with the desired parameters.\n            For example: `Fixed(nof_bins=20, min_points_per_bin=0, cat_limit=10)`\n\n        centering: whether to compute the normalization constant for centering the plot:\n\n            - `False` means no centering\n            - `True` or `zero_integral` centers around the `y` axis.\n            - `zero_start` starts the plot from `y=0`.\n    \"\"\"\n    assert binning_method == \"fixed\" or isinstance(\n        binning_method, bm.Fixed\n    ), \"ALE can work only with the fixed binning method!\"\n\n    self._fit_loop(features, binning_method, centering)\n</code></pre>"},{"location":"api/#effector.global_effect_ale.RHALE","title":"<code>effector.global_effect_ale.RHALE</code>","text":"<p>         Bases: <code>ALEBase</code></p> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_ale.py</code> <pre><code>class RHALE(ALEBase):\n    def __init__(\n            self,\n            data: np.ndarray,\n            model: callable,\n            model_jac: typing.Union[None, callable] = None,\n            nof_instances: typing.Union[int, str] = \"all\",\n            axis_limits: typing.Optional[np.ndarray] = None,\n            data_effect: typing.Optional[np.ndarray] = None,\n            avg_output: typing.Optional[float] = None,\n            feature_names: typing.Optional[list] = None,\n            target_name: typing.Optional[str] = None,\n    ):\n\"\"\"\n        Constructor for RHALE.\n\n        Definition:\n            RHALE is defined as:\n            $$\n            \\hat{f}^{RHALE}(x_s) = TODO\n            $$\n\n            The heterogeneity is:\n            $$\n            TODO\n            $$\n\n            The std of the bin-effects is:\n            $$\n            TODO\n            $$\n\n        Notes:\n            The required parameters are `data` and `model`. The rest are optional.\n\n        Args:\n            data: the design matrix\n\n                - shape: `(N,D)`\n            model: the black-box model. Must be a `Callable` with:\n\n                - input: `ndarray` of shape `(N, D)`\n                - output: `ndarray` of shape `(N, )`\n\n            model_jac: the Jacobian of the model. Must be a `Callable` with:\n\n                - input: `ndarray` of shape `(N, D)`\n                - output: `ndarray` of shape `(N, D)`\n\n            nof_instances: the number of instances to use for the explanation\n\n                - use an `int`, to specify the number of instances\n                - use `\"all\"`, to use all the instances\n\n            axis_limits: The limits of the feature effect plot along each axis\n\n                - use a `ndarray` of shape `(2, D)`, to specify them manually\n                - use `None`, to be inferred from the data\n\n            data_effect:\n                - if np.ndarray, the model Jacobian computed on the `data`\n                - if None, the Jacobian will be computed using model_jac\n\n            avg_output: the average output of the model on the data\n\n                - use a `float`, to specify it manually\n                - use `None`, to be inferred as `np.mean(model(data))`\n\n            feature_names: The names of the features\n\n                - use a `list` of `str`, to specify the name manually. For example: `[\"age\", \"weight\", ...]`\n                - use `None`, to keep the default names: `[\"x_0\", \"x_1\", ...]`\n\n            target_name: The name of the target variable\n\n                - use a `str`, to specify it name manually. For example: `\"price\"`\n                - use `None`, to keep the default name: `\"y\"`\n        \"\"\"\n        self.model_jac = model_jac\n\n        # select nof_instances from the data\n        nof_instances, indices = helpers.prep_nof_instances(nof_instances, data.shape[0])\n        data = data[indices, :]\n        data_effect = data_effect[indices, :] if data_effect is not None else None\n        self.data_effect = data_effect\n\n        super(RHALE, self).__init__(\n            data, model, \"all\", axis_limits, avg_output, feature_names, target_name, \"RHALE\"\n        )\n\n    def compile(self):\n\"\"\"Prepare everything for fitting, i.e., compute the gradients on data points.\n        \"\"\"\n        if self.data_effect is None and self.model_jac is not None:\n            self.data_effect = self.model_jac(self.data)\n        elif self.data_effect is None and self.model_jac is None:\n            self.data_effect = utils.compute_jacobian_numerically(self.model, self.data)\n\n    def _fit_feature(\n            self,\n            feature: int,\n            binning_method: Union[str, bm.DynamicProgramming, bm.Greedy, bm.Fixed] = \"greedy\"\n    ) -&gt; typing.Dict:\n        if self.data_effect is None:\n            self.compile()\n\n        # drop points outside of limits\n        ind = np.logical_and(\n            self.data[:, feature] &gt;= self.axis_limits[0, feature],\n            self.data[:, feature] &lt;= self.axis_limits[1, feature],\n        )\n        data = self.data[ind, :]\n        data_effect = self.data_effect[ind, :]\n\n        # bin estimation\n        bin_est = bm.find_limits(\n            data, data_effect, feature, self.axis_limits, binning_method\n        )\n        bin_name = bin_est.__class__.__name__\n\n        # assert bins can be computed else raise error\n        assert bin_est.limits is not False, (\n            \"Impossible to compute bins with enough points for feature \"\n            + str(feature + 1)\n            + \" and binning strategy: \"\n            + bin_name\n            + \". Change bin strategy or \"\n            \"the parameters of the method\"\n        )\n\n        # compute the bin effect\n        dale_params = utils.compute_ale_params(\n            data[:, feature], data_effect[:, feature], bin_est.limits\n        )\n        dale_params[\"alg_params\"] = binning_method\n        return dale_params\n\n    def fit(\n        self,\n        features: typing.Union[int, str, list] = \"all\",\n        binning_method: typing.Union[str, bm.DynamicProgramming, bm.Greedy, bm.Fixed] = \"greedy\",\n        centering: typing.Union[bool, str] = False,\n    ) -&gt; None:\n\"\"\"Fit the model.\n\n        Args:\n            features (int, str, list): the features to fit.\n\n                - If set to \"all\", all the features will be fitted.\n\n            binning_method (str): the binning method to use.\n\n                - Use `\"greedy\"` for using the Greedy binning solution with the default parameters.\n                  For custom parameters initialize a `binning_methods.Greedy` object\n                - Use `\"dp\"` for using a Dynamic Programming binning solution with the default parameters.\n                  For custom parameters initialize a `binning_methods.DynamicProgramming` object\n                - Use `\"fixed\"` for using a Fixed binning solution with the default parameters.\n                  For custom parameters initialize a `binning_methods.Fixed` object\n\n            centering: whether to compute the normalization constant for centering the plot:\n\n                - `False` means no centering\n                - `True` or `zero_integral` centers around the `y` axis\n                - `zero_start` starts the plot from `y=0`\n        \"\"\"\n        assert binning_method in [\n            \"greedy\",\n            \"dynamic\",\n            \"fixed\"\n        ] or isinstance(\n            binning_method, bm.Greedy\n        ) or isinstance(\n            binning_method, bm.DynamicProgramming\n        ) or isinstance(\n            binning_method, bm.Fixed\n        ), \"Unknown binning method!\"\n\n        self._fit_loop(features, binning_method, centering)\n</code></pre>"},{"location":"api/#effector.global_effect_ale.RHALE.__init__","title":"<code>__init__(data, model, model_jac=None, nof_instances='all', axis_limits=None, data_effect=None, avg_output=None, feature_names=None, target_name=None)</code>","text":"<p>Constructor for RHALE.</p> Definition <p>RHALE is defined as: $$ \\hat{f}^{RHALE}(x_s) = TODO $$</p> <p>The heterogeneity is: $$ TODO $$</p> <p>The std of the bin-effects is: $$ TODO $$</p> Notes <p>The required parameters are <code>data</code> and <code>model</code>. The rest are optional.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray</code> <p>the design matrix</p> <ul> <li>shape: <code>(N,D)</code></li> </ul> required <code>model</code> <code>callable</code> <p>the black-box model. Must be a <code>Callable</code> with:</p> <ul> <li>input: <code>ndarray</code> of shape <code>(N, D)</code></li> <li>output: <code>ndarray</code> of shape <code>(N, )</code></li> </ul> required <code>model_jac</code> <code>typing.Union[None, callable]</code> <p>the Jacobian of the model. Must be a <code>Callable</code> with:</p> <ul> <li>input: <code>ndarray</code> of shape <code>(N, D)</code></li> <li>output: <code>ndarray</code> of shape <code>(N, D)</code></li> </ul> <code>None</code> <code>nof_instances</code> <code>typing.Union[int, str]</code> <p>the number of instances to use for the explanation</p> <ul> <li>use an <code>int</code>, to specify the number of instances</li> <li>use <code>\"all\"</code>, to use all the instances</li> </ul> <code>'all'</code> <code>axis_limits</code> <code>typing.Optional[np.ndarray]</code> <p>The limits of the feature effect plot along each axis</p> <ul> <li>use a <code>ndarray</code> of shape <code>(2, D)</code>, to specify them manually</li> <li>use <code>None</code>, to be inferred from the data</li> </ul> <code>None</code> <code>data_effect</code> <code>typing.Optional[np.ndarray]</code> <ul> <li>if np.ndarray, the model Jacobian computed on the <code>data</code></li> <li>if None, the Jacobian will be computed using model_jac</li> </ul> <code>None</code> <code>avg_output</code> <code>typing.Optional[float]</code> <p>the average output of the model on the data</p> <ul> <li>use a <code>float</code>, to specify it manually</li> <li>use <code>None</code>, to be inferred as <code>np.mean(model(data))</code></li> </ul> <code>None</code> <code>feature_names</code> <code>typing.Optional[list]</code> <p>The names of the features</p> <ul> <li>use a <code>list</code> of <code>str</code>, to specify the name manually. For example: <code>[\"age\", \"weight\", ...]</code></li> <li>use <code>None</code>, to keep the default names: <code>[\"x_0\", \"x_1\", ...]</code></li> </ul> <code>None</code> <code>target_name</code> <code>typing.Optional[str]</code> <p>The name of the target variable</p> <ul> <li>use a <code>str</code>, to specify it name manually. For example: <code>\"price\"</code></li> <li>use <code>None</code>, to keep the default name: <code>\"y\"</code></li> </ul> <code>None</code> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_ale.py</code> <pre><code>def __init__(\n        self,\n        data: np.ndarray,\n        model: callable,\n        model_jac: typing.Union[None, callable] = None,\n        nof_instances: typing.Union[int, str] = \"all\",\n        axis_limits: typing.Optional[np.ndarray] = None,\n        data_effect: typing.Optional[np.ndarray] = None,\n        avg_output: typing.Optional[float] = None,\n        feature_names: typing.Optional[list] = None,\n        target_name: typing.Optional[str] = None,\n):\n\"\"\"\n    Constructor for RHALE.\n\n    Definition:\n        RHALE is defined as:\n        $$\n        \\hat{f}^{RHALE}(x_s) = TODO\n        $$\n\n        The heterogeneity is:\n        $$\n        TODO\n        $$\n\n        The std of the bin-effects is:\n        $$\n        TODO\n        $$\n\n    Notes:\n        The required parameters are `data` and `model`. The rest are optional.\n\n    Args:\n        data: the design matrix\n\n            - shape: `(N,D)`\n        model: the black-box model. Must be a `Callable` with:\n\n            - input: `ndarray` of shape `(N, D)`\n            - output: `ndarray` of shape `(N, )`\n\n        model_jac: the Jacobian of the model. Must be a `Callable` with:\n\n            - input: `ndarray` of shape `(N, D)`\n            - output: `ndarray` of shape `(N, D)`\n\n        nof_instances: the number of instances to use for the explanation\n\n            - use an `int`, to specify the number of instances\n            - use `\"all\"`, to use all the instances\n\n        axis_limits: The limits of the feature effect plot along each axis\n\n            - use a `ndarray` of shape `(2, D)`, to specify them manually\n            - use `None`, to be inferred from the data\n\n        data_effect:\n            - if np.ndarray, the model Jacobian computed on the `data`\n            - if None, the Jacobian will be computed using model_jac\n\n        avg_output: the average output of the model on the data\n\n            - use a `float`, to specify it manually\n            - use `None`, to be inferred as `np.mean(model(data))`\n\n        feature_names: The names of the features\n\n            - use a `list` of `str`, to specify the name manually. For example: `[\"age\", \"weight\", ...]`\n            - use `None`, to keep the default names: `[\"x_0\", \"x_1\", ...]`\n\n        target_name: The name of the target variable\n\n            - use a `str`, to specify it name manually. For example: `\"price\"`\n            - use `None`, to keep the default name: `\"y\"`\n    \"\"\"\n    self.model_jac = model_jac\n\n    # select nof_instances from the data\n    nof_instances, indices = helpers.prep_nof_instances(nof_instances, data.shape[0])\n    data = data[indices, :]\n    data_effect = data_effect[indices, :] if data_effect is not None else None\n    self.data_effect = data_effect\n\n    super(RHALE, self).__init__(\n        data, model, \"all\", axis_limits, avg_output, feature_names, target_name, \"RHALE\"\n    )\n</code></pre>"},{"location":"api/#effector.global_effect_ale.RHALE.fit","title":"<code>fit(features='all', binning_method='greedy', centering=False)</code>","text":"<p>Fit the model.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>int, str, list</code> <p>the features to fit.</p> <ul> <li>If set to \"all\", all the features will be fitted.</li> </ul> <code>'all'</code> <code>binning_method</code> <code>str</code> <p>the binning method to use.</p> <ul> <li>Use <code>\"greedy\"</code> for using the Greedy binning solution with the default parameters.   For custom parameters initialize a <code>binning_methods.Greedy</code> object</li> <li>Use <code>\"dp\"</code> for using a Dynamic Programming binning solution with the default parameters.   For custom parameters initialize a <code>binning_methods.DynamicProgramming</code> object</li> <li>Use <code>\"fixed\"</code> for using a Fixed binning solution with the default parameters.   For custom parameters initialize a <code>binning_methods.Fixed</code> object</li> </ul> <code>'greedy'</code> <code>centering</code> <code>typing.Union[bool, str]</code> <p>whether to compute the normalization constant for centering the plot:</p> <ul> <li><code>False</code> means no centering</li> <li><code>True</code> or <code>zero_integral</code> centers around the <code>y</code> axis</li> <li><code>zero_start</code> starts the plot from <code>y=0</code></li> </ul> <code>False</code> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_ale.py</code> <pre><code>def fit(\n    self,\n    features: typing.Union[int, str, list] = \"all\",\n    binning_method: typing.Union[str, bm.DynamicProgramming, bm.Greedy, bm.Fixed] = \"greedy\",\n    centering: typing.Union[bool, str] = False,\n) -&gt; None:\n\"\"\"Fit the model.\n\n    Args:\n        features (int, str, list): the features to fit.\n\n            - If set to \"all\", all the features will be fitted.\n\n        binning_method (str): the binning method to use.\n\n            - Use `\"greedy\"` for using the Greedy binning solution with the default parameters.\n              For custom parameters initialize a `binning_methods.Greedy` object\n            - Use `\"dp\"` for using a Dynamic Programming binning solution with the default parameters.\n              For custom parameters initialize a `binning_methods.DynamicProgramming` object\n            - Use `\"fixed\"` for using a Fixed binning solution with the default parameters.\n              For custom parameters initialize a `binning_methods.Fixed` object\n\n        centering: whether to compute the normalization constant for centering the plot:\n\n            - `False` means no centering\n            - `True` or `zero_integral` centers around the `y` axis\n            - `zero_start` starts the plot from `y=0`\n    \"\"\"\n    assert binning_method in [\n        \"greedy\",\n        \"dynamic\",\n        \"fixed\"\n    ] or isinstance(\n        binning_method, bm.Greedy\n    ) or isinstance(\n        binning_method, bm.DynamicProgramming\n    ) or isinstance(\n        binning_method, bm.Fixed\n    ), \"Unknown binning method!\"\n\n    self._fit_loop(features, binning_method, centering)\n</code></pre>"},{"location":"api/#effector.global_effect_pdp.PDPBase","title":"<code>effector.global_effect_pdp.PDPBase</code>","text":"<p>         Bases: <code>GlobalEffectBase</code></p> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_pdp.py</code> <pre><code>class PDPBase(GlobalEffectBase):\n    def __init__(\n        self,\n        data: np.ndarray,\n        model: Callable,\n        model_jac: Optional[Callable] = None,\n        axis_limits: Optional[np.ndarray] = None,\n        avg_output: Optional[float] = None,\n        nof_instances: Union[int, str] = 300,\n        feature_names: Optional[List] = None,\n        target_name: Optional[str] = None,\n        method_name: str = \"PDP\",\n    ):\n\"\"\"\n        Constructor of the PDPBase class.\n        \"\"\"\n\n        self.model_jac = model_jac\n\n        super(PDPBase, self).__init__(\n            method_name,\n            data,\n            model, nof_instances, axis_limits, avg_output, feature_names, target_name\n        )\n\n    def _predict(self, data, xx, feature):\n        if self.method_name == \"pdp\":\n            y = pdp_1d_vectorized(\n                self.model, data, xx, feature, False, False, True\n            )\n        else:\n            if self.model_jac is not None:\n                y = pdp_1d_vectorized(self.model_jac, self.data, xx, feature, False, True, True)\n            else:\n                y = pdp_1d_vectorized(self.model, self.data, xx, feature, False, False, True, True)\n        return y\n\n    def _fit_feature(\n        self,\n        feature: int,\n        centering: Union[bool, str] = False,\n        points_for_centering: int = 100,\n    ) -&gt; dict:\n\n        # drop points outside of limits\n        self.data = self.data[self.data[:, feature] &gt;= self.axis_limits[0, feature]]\n        self.data = self.data[self.data[:, feature] &lt;= self.axis_limits[1, feature]]\n        data = self.data\n\n        if centering is True or centering == \"zero_integral\":\n            xx = np.linspace(\n                self.axis_limits[0, feature],\n                self.axis_limits[1, feature],\n                points_for_centering,\n            )\n            y = self._predict(data, xx, feature)\n            norm_const = np.mean(y, axis=0)\n            fe = {\"norm_const\": norm_const}\n        elif centering == \"zero_start\":\n            xx = self.axis_limits[0, feature, np.newaxis]\n            y = self._predict(data, xx, feature)\n            fe = {\"norm_const\": y[0]}\n        else:\n            fe = {\"norm_const\": helpers.EMPTY_SYMBOL}\n        return fe\n\n    def fit(\n        self,\n        features: Union[int, str, list] = \"all\",\n        centering: Union[bool, str] = True,\n        points_for_centering: int = 100,\n    ):\n\"\"\"\n        Fit the PDP or d-PDP.\n\n        Notes:\n            You can use `.eval` or `.plot` without calling `.fit` explicitly.\n            The only thing that `.fit` does is to compute the normalization constant for centering the PDP and ICE plots.\n            This will be automatically done when calling `eval` or `plot`, so there is no need to call `fit` explicitly.\n\n        Args:\n            features: the features to fit.\n                - If set to \"all\", all the features will be fitted.\n\n            centering: whether to center the plot:\n\n                - `False` means no centering\n                - `True` or `zero_integral` centers around the `y` axis.\n                - `zero_start` starts the plot from `y=0`.\n\n            points_for_centering: number of linspaced points along the feature axis used for centering.\n\n                - If set to `\"all\"`, all the dataset points will be used.\n\n        \"\"\"\n        centering = helpers.prep_centering(centering)\n        features = helpers.prep_features(features, self.dim)\n\n        for s in features:\n            self.feature_effect[\"feature_\" + str(s)] = self._fit_feature(\n                s, centering, points_for_centering\n            )\n            self.is_fitted[s] = True\n            self.method_args[\"feature_\" + str(s)] = {\n                \"centering\": centering,\n                \"points_for_centering\": points_for_centering,\n            }\n\n    def eval(\n        self,\n        feature: int,\n        xs: np.ndarray,\n        heterogeneity: bool = False,\n        centering: typing.Union[bool, str] = False,\n        return_all: bool = False,\n    ) -&gt; typing.Union[np.ndarray, typing.Tuple[np.ndarray, np.ndarray]]:\n\"\"\"Evaluate the effect of the s-th feature at positions `xs`.\n\n        Args:\n            feature: index of feature of interest\n            xs: the points along the s-th axis to evaluate the FE plot\n\n              - `np.ndarray` of shape `(T, )`\n\n            heterogeneity: whether to return the heterogeneity measures.\n\n                  - if `heterogeneity=False`, the function returns the mean effect at the given `xs`\n                  - If `heterogeneity=True`, the function returns `(y, std)` where `y` is the mean effect and `std` is the standard deviation of the mean effect\n\n            centering: whether to center the PDP\n\n                - If `centering` is `False`, the PDP not centered\n                - If `centering` is `True` or `zero_integral`, the PDP is centered around the `y` axis.\n                - If `centering` is `zero_start`, the PDP starts from `y=0`.\n\n            return_all: whether to return PDP and ICE plots evaluated at `xs`\n\n                - If `return_all=False`, the function returns the mean effect at the given `xs`\n                - If `return_all=True`, the function returns a `ndarray` of shape `(T, N)` with the `N` ICE plots evaluated at `xs`\n\n        Returns:\n            the mean effect `y`, if `heterogeneity=False` (default) or a tuple `(y, std)` otherwise\n\n        \"\"\"\n        centering = helpers.prep_centering(centering)\n\n        if self.refit(feature, centering):\n            self.fit(features=feature, centering=centering)\n\n        # Check if the lower bound is less than the upper bound\n        assert self.axis_limits[0, feature] &lt; self.axis_limits[1, feature]\n\n        # new implementation\n        yy = self._predict(self.data, xs, feature)\n\n        if centering:\n            norm_consts = np.expand_dims(\n                self.feature_effect[\"feature_\" + str(feature)][\"norm_const\"], axis=0\n            )\n            yy = yy - norm_consts\n\n        y_pdp = np.mean(yy, axis=1)\n\n        if return_all:\n            return yy\n\n        if heterogeneity:\n            std = np.std(yy, axis=1)\n            return y_pdp, std\n        else:\n            return y_pdp\n\n    def plot(\n        self,\n        feature: int,\n        heterogeneity: Union[bool, str] = False,\n        centering: Union[bool, str] = False,\n        nof_points: int = 30,\n        scale_x: Optional[dict] = None,\n        scale_y: Optional[dict] = None,\n        nof_ice: Union[int, str] = \"all\",\n        show_avg_output: bool = False,\n        y_limits: Optional[List] = None,\n    ):\n\"\"\"\n        Plot the PDP or d-PDP.\n\n        Args:\n            feature: index of the plotted feature\n            heterogeneity: whether to output the heterogeneity of the SHAP values\n\n                - If `heterogeneity` is `False`, no heterogeneity is plotted\n                - If `heterogeneity` is `True` or `\"std\"`, the standard deviation of the shap values is plotted\n                - If `heterogeneity` is `ice`, the ICE plots are plotted\n\n            centering: whether to center the PDP\n\n                - If `centering` is `False`, the PDP not centered\n                - If `centering` is `True` or `zero_integral`, the PDP is centered around the `y` axis.\n                - If `centering` is `zero_start`, the PDP starts from `y=0`.\n\n            nof_points: number of points to evaluate the SDP plot\n            scale_x: dictionary with keys \"mean\" and \"std\" for scaling the x-axis\n            scale_y: dictionary with keys \"mean\" and \"std\" for scaling the y-axis\n            nof_ice: number of shap values to show on top of the SHAP curve\n            show_avg_output: whether to show the average output of the model\n            y_limits: limits of the y-axis\n        \"\"\"\n        heterogeneity = helpers.prep_confidence_interval(heterogeneity)\n        centering = helpers.prep_centering(centering)\n\n        x = np.linspace(\n            self.axis_limits[0, feature], self.axis_limits[1, feature], nof_points\n        )\n\n        yy = self.eval(\n            feature, x, heterogeneity=False, centering=centering, return_all=True\n        )\n\n        if show_avg_output:\n            avg_output = helpers.prep_avg_output(self.data, self.model, self.avg_output, scale_y)\n        else:\n            avg_output = None\n\n        title = \"PDP\" if self.method_name == \"pdp\" else \"d-PDP\"\n        vis.plot_pdp_ice(\n            x,\n            feature,\n            yy=yy,\n            title=title,\n            confidence_interval=heterogeneity,\n            y_pdp_label=\"PDP\" if self.method_name == \"pdp\" else \"d-PDP\",\n            y_ice_label=\"ICE\" if self.method_name == \"pdp\" else \"d-ICE\",\n            scale_x=scale_x,\n            scale_y=scale_y,\n            avg_output=avg_output,\n            feature_names=self.feature_names,\n            target_name=self.target_name,\n            nof_ice=nof_ice,\n            y_limits=y_limits,\n        )\n</code></pre>"},{"location":"api/#effector.global_effect_pdp.PDPBase.fit","title":"<code>fit(features='all', centering=True, points_for_centering=100)</code>","text":"<p>Fit the PDP or d-PDP.</p> Notes <p>You can use <code>.eval</code> or <code>.plot</code> without calling <code>.fit</code> explicitly. The only thing that <code>.fit</code> does is to compute the normalization constant for centering the PDP and ICE plots. This will be automatically done when calling <code>eval</code> or <code>plot</code>, so there is no need to call <code>fit</code> explicitly.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Union[int, str, list]</code> <p>the features to fit. - If set to \"all\", all the features will be fitted.</p> <code>'all'</code> <code>centering</code> <code>Union[bool, str]</code> <p>whether to center the plot:</p> <ul> <li><code>False</code> means no centering</li> <li><code>True</code> or <code>zero_integral</code> centers around the <code>y</code> axis.</li> <li><code>zero_start</code> starts the plot from <code>y=0</code>.</li> </ul> <code>True</code> <code>points_for_centering</code> <code>int</code> <p>number of linspaced points along the feature axis used for centering.</p> <ul> <li>If set to <code>\"all\"</code>, all the dataset points will be used.</li> </ul> <code>100</code> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_pdp.py</code> <pre><code>def fit(\n    self,\n    features: Union[int, str, list] = \"all\",\n    centering: Union[bool, str] = True,\n    points_for_centering: int = 100,\n):\n\"\"\"\n    Fit the PDP or d-PDP.\n\n    Notes:\n        You can use `.eval` or `.plot` without calling `.fit` explicitly.\n        The only thing that `.fit` does is to compute the normalization constant for centering the PDP and ICE plots.\n        This will be automatically done when calling `eval` or `plot`, so there is no need to call `fit` explicitly.\n\n    Args:\n        features: the features to fit.\n            - If set to \"all\", all the features will be fitted.\n\n        centering: whether to center the plot:\n\n            - `False` means no centering\n            - `True` or `zero_integral` centers around the `y` axis.\n            - `zero_start` starts the plot from `y=0`.\n\n        points_for_centering: number of linspaced points along the feature axis used for centering.\n\n            - If set to `\"all\"`, all the dataset points will be used.\n\n    \"\"\"\n    centering = helpers.prep_centering(centering)\n    features = helpers.prep_features(features, self.dim)\n\n    for s in features:\n        self.feature_effect[\"feature_\" + str(s)] = self._fit_feature(\n            s, centering, points_for_centering\n        )\n        self.is_fitted[s] = True\n        self.method_args[\"feature_\" + str(s)] = {\n            \"centering\": centering,\n            \"points_for_centering\": points_for_centering,\n        }\n</code></pre>"},{"location":"api/#effector.global_effect_pdp.PDPBase.eval","title":"<code>eval(feature, xs, heterogeneity=False, centering=False, return_all=False)</code>","text":"<p>Evaluate the effect of the s-th feature at positions <code>xs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>int</code> <p>index of feature of interest</p> required <code>xs</code> <code>np.ndarray</code> <p>the points along the s-th axis to evaluate the FE plot</p> <ul> <li><code>np.ndarray</code> of shape <code>(T, )</code></li> </ul> required <code>heterogeneity</code> <code>bool</code> <p>whether to return the heterogeneity measures.</p> <ul> <li>if <code>heterogeneity=False</code>, the function returns the mean effect at the given <code>xs</code></li> <li>If <code>heterogeneity=True</code>, the function returns <code>(y, std)</code> where <code>y</code> is the mean effect and <code>std</code> is the standard deviation of the mean effect</li> </ul> <code>False</code> <code>centering</code> <code>typing.Union[bool, str]</code> <p>whether to center the PDP</p> <ul> <li>If <code>centering</code> is <code>False</code>, the PDP not centered</li> <li>If <code>centering</code> is <code>True</code> or <code>zero_integral</code>, the PDP is centered around the <code>y</code> axis.</li> <li>If <code>centering</code> is <code>zero_start</code>, the PDP starts from <code>y=0</code>.</li> </ul> <code>False</code> <code>return_all</code> <code>bool</code> <p>whether to return PDP and ICE plots evaluated at <code>xs</code></p> <ul> <li>If <code>return_all=False</code>, the function returns the mean effect at the given <code>xs</code></li> <li>If <code>return_all=True</code>, the function returns a <code>ndarray</code> of shape <code>(T, N)</code> with the <code>N</code> ICE plots evaluated at <code>xs</code></li> </ul> <code>False</code> <p>Returns:</p> Type Description <code>typing.Union[np.ndarray, typing.Tuple[np.ndarray, np.ndarray]]</code> <p>the mean effect <code>y</code>, if <code>heterogeneity=False</code> (default) or a tuple <code>(y, std)</code> otherwise</p> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_pdp.py</code> <pre><code>def eval(\n    self,\n    feature: int,\n    xs: np.ndarray,\n    heterogeneity: bool = False,\n    centering: typing.Union[bool, str] = False,\n    return_all: bool = False,\n) -&gt; typing.Union[np.ndarray, typing.Tuple[np.ndarray, np.ndarray]]:\n\"\"\"Evaluate the effect of the s-th feature at positions `xs`.\n\n    Args:\n        feature: index of feature of interest\n        xs: the points along the s-th axis to evaluate the FE plot\n\n          - `np.ndarray` of shape `(T, )`\n\n        heterogeneity: whether to return the heterogeneity measures.\n\n              - if `heterogeneity=False`, the function returns the mean effect at the given `xs`\n              - If `heterogeneity=True`, the function returns `(y, std)` where `y` is the mean effect and `std` is the standard deviation of the mean effect\n\n        centering: whether to center the PDP\n\n            - If `centering` is `False`, the PDP not centered\n            - If `centering` is `True` or `zero_integral`, the PDP is centered around the `y` axis.\n            - If `centering` is `zero_start`, the PDP starts from `y=0`.\n\n        return_all: whether to return PDP and ICE plots evaluated at `xs`\n\n            - If `return_all=False`, the function returns the mean effect at the given `xs`\n            - If `return_all=True`, the function returns a `ndarray` of shape `(T, N)` with the `N` ICE plots evaluated at `xs`\n\n    Returns:\n        the mean effect `y`, if `heterogeneity=False` (default) or a tuple `(y, std)` otherwise\n\n    \"\"\"\n    centering = helpers.prep_centering(centering)\n\n    if self.refit(feature, centering):\n        self.fit(features=feature, centering=centering)\n\n    # Check if the lower bound is less than the upper bound\n    assert self.axis_limits[0, feature] &lt; self.axis_limits[1, feature]\n\n    # new implementation\n    yy = self._predict(self.data, xs, feature)\n\n    if centering:\n        norm_consts = np.expand_dims(\n            self.feature_effect[\"feature_\" + str(feature)][\"norm_const\"], axis=0\n        )\n        yy = yy - norm_consts\n\n    y_pdp = np.mean(yy, axis=1)\n\n    if return_all:\n        return yy\n\n    if heterogeneity:\n        std = np.std(yy, axis=1)\n        return y_pdp, std\n    else:\n        return y_pdp\n</code></pre>"},{"location":"api/#effector.global_effect_pdp.PDPBase.plot","title":"<code>plot(feature, heterogeneity=False, centering=False, nof_points=30, scale_x=None, scale_y=None, nof_ice='all', show_avg_output=False, y_limits=None)</code>","text":"<p>Plot the PDP or d-PDP.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>int</code> <p>index of the plotted feature</p> required <code>heterogeneity</code> <code>Union[bool, str]</code> <p>whether to output the heterogeneity of the SHAP values</p> <ul> <li>If <code>heterogeneity</code> is <code>False</code>, no heterogeneity is plotted</li> <li>If <code>heterogeneity</code> is <code>True</code> or <code>\"std\"</code>, the standard deviation of the shap values is plotted</li> <li>If <code>heterogeneity</code> is <code>ice</code>, the ICE plots are plotted</li> </ul> <code>False</code> <code>centering</code> <code>Union[bool, str]</code> <p>whether to center the PDP</p> <ul> <li>If <code>centering</code> is <code>False</code>, the PDP not centered</li> <li>If <code>centering</code> is <code>True</code> or <code>zero_integral</code>, the PDP is centered around the <code>y</code> axis.</li> <li>If <code>centering</code> is <code>zero_start</code>, the PDP starts from <code>y=0</code>.</li> </ul> <code>False</code> <code>nof_points</code> <code>int</code> <p>number of points to evaluate the SDP plot</p> <code>30</code> <code>scale_x</code> <code>Optional[dict]</code> <p>dictionary with keys \"mean\" and \"std\" for scaling the x-axis</p> <code>None</code> <code>scale_y</code> <code>Optional[dict]</code> <p>dictionary with keys \"mean\" and \"std\" for scaling the y-axis</p> <code>None</code> <code>nof_ice</code> <code>Union[int, str]</code> <p>number of shap values to show on top of the SHAP curve</p> <code>'all'</code> <code>show_avg_output</code> <code>bool</code> <p>whether to show the average output of the model</p> <code>False</code> <code>y_limits</code> <code>Optional[List]</code> <p>limits of the y-axis</p> <code>None</code> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_pdp.py</code> <pre><code>def plot(\n    self,\n    feature: int,\n    heterogeneity: Union[bool, str] = False,\n    centering: Union[bool, str] = False,\n    nof_points: int = 30,\n    scale_x: Optional[dict] = None,\n    scale_y: Optional[dict] = None,\n    nof_ice: Union[int, str] = \"all\",\n    show_avg_output: bool = False,\n    y_limits: Optional[List] = None,\n):\n\"\"\"\n    Plot the PDP or d-PDP.\n\n    Args:\n        feature: index of the plotted feature\n        heterogeneity: whether to output the heterogeneity of the SHAP values\n\n            - If `heterogeneity` is `False`, no heterogeneity is plotted\n            - If `heterogeneity` is `True` or `\"std\"`, the standard deviation of the shap values is plotted\n            - If `heterogeneity` is `ice`, the ICE plots are plotted\n\n        centering: whether to center the PDP\n\n            - If `centering` is `False`, the PDP not centered\n            - If `centering` is `True` or `zero_integral`, the PDP is centered around the `y` axis.\n            - If `centering` is `zero_start`, the PDP starts from `y=0`.\n\n        nof_points: number of points to evaluate the SDP plot\n        scale_x: dictionary with keys \"mean\" and \"std\" for scaling the x-axis\n        scale_y: dictionary with keys \"mean\" and \"std\" for scaling the y-axis\n        nof_ice: number of shap values to show on top of the SHAP curve\n        show_avg_output: whether to show the average output of the model\n        y_limits: limits of the y-axis\n    \"\"\"\n    heterogeneity = helpers.prep_confidence_interval(heterogeneity)\n    centering = helpers.prep_centering(centering)\n\n    x = np.linspace(\n        self.axis_limits[0, feature], self.axis_limits[1, feature], nof_points\n    )\n\n    yy = self.eval(\n        feature, x, heterogeneity=False, centering=centering, return_all=True\n    )\n\n    if show_avg_output:\n        avg_output = helpers.prep_avg_output(self.data, self.model, self.avg_output, scale_y)\n    else:\n        avg_output = None\n\n    title = \"PDP\" if self.method_name == \"pdp\" else \"d-PDP\"\n    vis.plot_pdp_ice(\n        x,\n        feature,\n        yy=yy,\n        title=title,\n        confidence_interval=heterogeneity,\n        y_pdp_label=\"PDP\" if self.method_name == \"pdp\" else \"d-PDP\",\n        y_ice_label=\"ICE\" if self.method_name == \"pdp\" else \"d-ICE\",\n        scale_x=scale_x,\n        scale_y=scale_y,\n        avg_output=avg_output,\n        feature_names=self.feature_names,\n        target_name=self.target_name,\n        nof_ice=nof_ice,\n        y_limits=y_limits,\n    )\n</code></pre>"},{"location":"api/#effector.global_effect_pdp.PDP","title":"<code>effector.global_effect_pdp.PDP</code>","text":"<p>         Bases: <code>PDPBase</code></p> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_pdp.py</code> <pre><code>class PDP(PDPBase):\n    def __init__(\n        self,\n        data: np.ndarray,\n        model: Callable,\n        axis_limits: Optional[np.ndarray] = None,\n        nof_instances: Union[int, str] = 300,\n        avg_output: Optional[float] = None,\n        feature_names: Optional[List] = None,\n        target_name: Optional[str] = None,\n    ):\n\"\"\"\n        Constructor of the PDP class.\n\n        Definition:\n            PDP is defined as:\n            $$\n            \\hat{f}^{PDP}(x_s) = {1 \\over N} \\sum_{i=1}^N f(x_s, x_C^{(i)})b\n            $$\n\n            The ICE plots are:\n            $$\n            \\hat{f}^{(i)}(x_s) = f(x_s, x_C^{(i)}), \\quad i=1, \\dots, N\n            $$\n\n            The heterogeneity is:\n            $$\n            \\mathcal{H}^{PDP}(x_s) = \\sqrt {{1 \\over N} \\sum_{i=1}^N ( \\hat{f}^{(i)}(x_s) - \\hat{f}^{PDP}(x_s) )^2}\n            $$\n\n        Notes:\n            The required parameters are `data` and `model`. The rest are optional.\n\n        Args:\n            data: the design matrix\n\n                - shape: `(N,D)`\n            model: the black-box model. Must be a `Callable` with:\n\n                - input: `ndarray` of shape `(N, D)`\n                - output: `ndarray` of shape `(N, )`\n\n            axis_limits: The limits of the feature effect plot along each axis\n\n                - use a `ndarray` of shape `(2, D)`, to specify them manually\n                - use `None`, to be inferred from the data\n\n            nof_instances: maximum number of instances to be used for PDP.\n\n                - use \"all\", for using all instances.\n                - use an `int`, for using `nof_instances` instances.\n\n            avg_output: The average output of the model.\n\n                - use a `float`, to specify it manually\n                - use `None`, to be inferred as `np.mean(model(data))`\n\n            feature_names: The names of the features\n\n                - use a `list` of `str`, to specify the name manually. For example: `                  [\"age\", \"weight\", ...]`\n                - use `None`, to keep the default names: `[\"x_0\", \"x_1\", ...]`\n\n            target_name: The name of the target variable\n\n                - use a `str`, to specify it name manually. For example: `\"price\"`\n                - use `None`, to keep the default name: `\"y\"`\n        \"\"\"\n\n        super(PDP, self).__init__(\n            data, model, None, axis_limits, avg_output, nof_instances, feature_names, target_name, method_name=\"PDP\"\n        )\n</code></pre>"},{"location":"api/#effector.global_effect_pdp.PDP.__init__","title":"<code>__init__(data, model, axis_limits=None, nof_instances=300, avg_output=None, feature_names=None, target_name=None)</code>","text":"<p>Constructor of the PDP class.</p> Definition <p>PDP is defined as: $$ \\hat{f}^{PDP}(x_s) = {1 \\over N} \\sum_{i=1}^N f(x_s, x_C^{(i)})b $$</p> <p>The ICE plots are: $$ \\hat{f}^{(i)}(x_s) = f(x_s, x_C^{(i)}), \\quad i=1, \\dots, N $$</p> <p>The heterogeneity is: $$ \\mathcal{H}^{PDP}(x_s) = \\sqrt {{1 \\over N} \\sum_{i=1}^N ( \\hat{f}^{(i)}(x_s) - \\hat{f}^{PDP}(x_s) )^2} $$</p> Notes <p>The required parameters are <code>data</code> and <code>model</code>. The rest are optional.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray</code> <p>the design matrix</p> <ul> <li>shape: <code>(N,D)</code></li> </ul> required <code>model</code> <code>Callable</code> <p>the black-box model. Must be a <code>Callable</code> with:</p> <ul> <li>input: <code>ndarray</code> of shape <code>(N, D)</code></li> <li>output: <code>ndarray</code> of shape <code>(N, )</code></li> </ul> required <code>axis_limits</code> <code>Optional[np.ndarray]</code> <p>The limits of the feature effect plot along each axis</p> <ul> <li>use a <code>ndarray</code> of shape <code>(2, D)</code>, to specify them manually</li> <li>use <code>None</code>, to be inferred from the data</li> </ul> <code>None</code> <code>nof_instances</code> <code>Union[int, str]</code> <p>maximum number of instances to be used for PDP.</p> <ul> <li>use \"all\", for using all instances.</li> <li>use an <code>int</code>, for using <code>nof_instances</code> instances.</li> </ul> <code>300</code> <code>avg_output</code> <code>Optional[float]</code> <p>The average output of the model.</p> <ul> <li>use a <code>float</code>, to specify it manually</li> <li>use <code>None</code>, to be inferred as <code>np.mean(model(data))</code></li> </ul> <code>None</code> <code>feature_names</code> <code>Optional[List]</code> <p>The names of the features</p> <ul> <li>use a <code>list</code> of <code>str</code>, to specify the name manually. For example: <code>[\"age\", \"weight\", ...]</code></li> <li>use <code>None</code>, to keep the default names: <code>[\"x_0\", \"x_1\", ...]</code></li> </ul> <code>None</code> <code>target_name</code> <code>Optional[str]</code> <p>The name of the target variable</p> <ul> <li>use a <code>str</code>, to specify it name manually. For example: <code>\"price\"</code></li> <li>use <code>None</code>, to keep the default name: <code>\"y\"</code></li> </ul> <code>None</code> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_pdp.py</code> <pre><code>def __init__(\n    self,\n    data: np.ndarray,\n    model: Callable,\n    axis_limits: Optional[np.ndarray] = None,\n    nof_instances: Union[int, str] = 300,\n    avg_output: Optional[float] = None,\n    feature_names: Optional[List] = None,\n    target_name: Optional[str] = None,\n):\n\"\"\"\n    Constructor of the PDP class.\n\n    Definition:\n        PDP is defined as:\n        $$\n        \\hat{f}^{PDP}(x_s) = {1 \\over N} \\sum_{i=1}^N f(x_s, x_C^{(i)})b\n        $$\n\n        The ICE plots are:\n        $$\n        \\hat{f}^{(i)}(x_s) = f(x_s, x_C^{(i)}), \\quad i=1, \\dots, N\n        $$\n\n        The heterogeneity is:\n        $$\n        \\mathcal{H}^{PDP}(x_s) = \\sqrt {{1 \\over N} \\sum_{i=1}^N ( \\hat{f}^{(i)}(x_s) - \\hat{f}^{PDP}(x_s) )^2}\n        $$\n\n    Notes:\n        The required parameters are `data` and `model`. The rest are optional.\n\n    Args:\n        data: the design matrix\n\n            - shape: `(N,D)`\n        model: the black-box model. Must be a `Callable` with:\n\n            - input: `ndarray` of shape `(N, D)`\n            - output: `ndarray` of shape `(N, )`\n\n        axis_limits: The limits of the feature effect plot along each axis\n\n            - use a `ndarray` of shape `(2, D)`, to specify them manually\n            - use `None`, to be inferred from the data\n\n        nof_instances: maximum number of instances to be used for PDP.\n\n            - use \"all\", for using all instances.\n            - use an `int`, for using `nof_instances` instances.\n\n        avg_output: The average output of the model.\n\n            - use a `float`, to specify it manually\n            - use `None`, to be inferred as `np.mean(model(data))`\n\n        feature_names: The names of the features\n\n            - use a `list` of `str`, to specify the name manually. For example: `                  [\"age\", \"weight\", ...]`\n            - use `None`, to keep the default names: `[\"x_0\", \"x_1\", ...]`\n\n        target_name: The name of the target variable\n\n            - use a `str`, to specify it name manually. For example: `\"price\"`\n            - use `None`, to keep the default name: `\"y\"`\n    \"\"\"\n\n    super(PDP, self).__init__(\n        data, model, None, axis_limits, avg_output, nof_instances, feature_names, target_name, method_name=\"PDP\"\n    )\n</code></pre>"},{"location":"api/#effector.global_effect_pdp.DerivativePDP","title":"<code>effector.global_effect_pdp.DerivativePDP</code>","text":"<p>         Bases: <code>PDPBase</code></p> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_pdp.py</code> <pre><code>class DerivativePDP(PDPBase):\n    def __init__(\n            self,\n            data: np.ndarray,\n            model: Callable,\n            model_jac: Optional[Callable] = None,\n            axis_limits: Optional[np.ndarray] = None,\n            nof_instances: Union[int, str] = 300,\n            avg_output: Optional[float] = None,\n            feature_names: Optional[List] = None,\n            target_name: Optional[str] = None,\n    ):\n\"\"\"\n        Constructor of the DerivativePDP class.\n\n        Definition:\n            d-PDP is defined as:\n            $$\n            \\hat{f}^{d-PDP}(x_s) = {1 \\over N} \\sum_{i=1}^N {df \\over d x_s} (x_s, x_C^i)\n            $$\n\n            The d-ICE plots are:\n            $$\n            \\hat{f}^i(x_s) = {df \\over d x_s}(x_s, x_C^i), \\quad i=1, \\dots, N\n            $$\n\n            The heterogeneity is:\n            $$\n            \\mathcal{H}^{d-PDP}(x_s) = \\sqrt {{1 \\over N} \\sum_{i=1}^N ( \\hat{f}^i(x_s) - \\hat{f}^{d-PDP}(x_s) )^2}\n            $$\n\n        Notes:\n            - The required parameters are `data` and `model`. The rest are optional.\n            - The `model_jac` is the Jacobian of the model. If `None`, the Jacobian will be computed numerically.\n\n        Args:\n            data: the design matrix\n\n                - shape: `(N,D)`\n            model: the black-box model. Must be a `Callable` with:\n\n                - input: `ndarray` of shape `(N, D)`\n                - output: `ndarray` of shape `(N, )`\n\n            model_jac: the black-box model Jacobian. Must be a `Callable` with:\n\n                - input: `ndarray` of shape `(N, D)`\n                - output: `ndarray` of shape `(N, D)`\n\n            axis_limits: The limits of the feature effect plot along each axis\n\n                - use a `ndarray` of shape `(2, D)`, to specify them manually\n                - use `None`, to be inferred from the data\n\n            nof_instances: maximum number of instances to be used for PDP.\n\n                - use \"all\", for using all instances.\n                - use an `int`, for using `nof_instances` instances.\n\n            avg_output: The average output of the model.\n\n                - use a `float`, to specify it manually\n                - use `None`, to be inferred as `np.mean(model(data))`\n\n            feature_names: The names of the features\n\n                - use a `list` of `str`, to specify the name manually. For example: `[\"age\", \"weight\", ...]`\n                - use `None`, to keep the default names: `[\"x_0\", \"x_1\", ...]`\n\n            target_name: The name of the target variable\n\n                - use a `str`, to specify it name manually. For example: `\"price\"`\n                - use `None`, to keep the default name: `\"y\"`\n        \"\"\"\n\n        super(DerivativePDP, self).__init__(\n            data, model, model_jac, axis_limits, avg_output, nof_instances, feature_names, target_name, method_name=\"d-PDP\"\n        )\n</code></pre>"},{"location":"api/#effector.global_effect_pdp.DerivativePDP.__init__","title":"<code>__init__(data, model, model_jac=None, axis_limits=None, nof_instances=300, avg_output=None, feature_names=None, target_name=None)</code>","text":"<p>Constructor of the DerivativePDP class.</p> Definition <p>d-PDP is defined as: $$ \\hat{f}^{d-PDP}(x_s) = {1 \\over N} \\sum_{i=1}^N {df \\over d x_s} (x_s, x_C^i) $$</p> <p>The d-ICE plots are: $$ \\hat{f}^i(x_s) = {df \\over d x_s}(x_s, x_C^i), \\quad i=1, \\dots, N $$</p> <p>The heterogeneity is: $$ \\mathcal{H}^{d-PDP}(x_s) = \\sqrt {{1 \\over N} \\sum_{i=1}^N ( \\hat{f}^i(x_s) - \\hat{f}^{d-PDP}(x_s) )^2} $$</p> Notes <ul> <li>The required parameters are <code>data</code> and <code>model</code>. The rest are optional.</li> <li>The <code>model_jac</code> is the Jacobian of the model. If <code>None</code>, the Jacobian will be computed numerically.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray</code> <p>the design matrix</p> <ul> <li>shape: <code>(N,D)</code></li> </ul> required <code>model</code> <code>Callable</code> <p>the black-box model. Must be a <code>Callable</code> with:</p> <ul> <li>input: <code>ndarray</code> of shape <code>(N, D)</code></li> <li>output: <code>ndarray</code> of shape <code>(N, )</code></li> </ul> required <code>model_jac</code> <code>Optional[Callable]</code> <p>the black-box model Jacobian. Must be a <code>Callable</code> with:</p> <ul> <li>input: <code>ndarray</code> of shape <code>(N, D)</code></li> <li>output: <code>ndarray</code> of shape <code>(N, D)</code></li> </ul> <code>None</code> <code>axis_limits</code> <code>Optional[np.ndarray]</code> <p>The limits of the feature effect plot along each axis</p> <ul> <li>use a <code>ndarray</code> of shape <code>(2, D)</code>, to specify them manually</li> <li>use <code>None</code>, to be inferred from the data</li> </ul> <code>None</code> <code>nof_instances</code> <code>Union[int, str]</code> <p>maximum number of instances to be used for PDP.</p> <ul> <li>use \"all\", for using all instances.</li> <li>use an <code>int</code>, for using <code>nof_instances</code> instances.</li> </ul> <code>300</code> <code>avg_output</code> <code>Optional[float]</code> <p>The average output of the model.</p> <ul> <li>use a <code>float</code>, to specify it manually</li> <li>use <code>None</code>, to be inferred as <code>np.mean(model(data))</code></li> </ul> <code>None</code> <code>feature_names</code> <code>Optional[List]</code> <p>The names of the features</p> <ul> <li>use a <code>list</code> of <code>str</code>, to specify the name manually. For example: <code>[\"age\", \"weight\", ...]</code></li> <li>use <code>None</code>, to keep the default names: <code>[\"x_0\", \"x_1\", ...]</code></li> </ul> <code>None</code> <code>target_name</code> <code>Optional[str]</code> <p>The name of the target variable</p> <ul> <li>use a <code>str</code>, to specify it name manually. For example: <code>\"price\"</code></li> <li>use <code>None</code>, to keep the default name: <code>\"y\"</code></li> </ul> <code>None</code> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_pdp.py</code> <pre><code>def __init__(\n        self,\n        data: np.ndarray,\n        model: Callable,\n        model_jac: Optional[Callable] = None,\n        axis_limits: Optional[np.ndarray] = None,\n        nof_instances: Union[int, str] = 300,\n        avg_output: Optional[float] = None,\n        feature_names: Optional[List] = None,\n        target_name: Optional[str] = None,\n):\n\"\"\"\n    Constructor of the DerivativePDP class.\n\n    Definition:\n        d-PDP is defined as:\n        $$\n        \\hat{f}^{d-PDP}(x_s) = {1 \\over N} \\sum_{i=1}^N {df \\over d x_s} (x_s, x_C^i)\n        $$\n\n        The d-ICE plots are:\n        $$\n        \\hat{f}^i(x_s) = {df \\over d x_s}(x_s, x_C^i), \\quad i=1, \\dots, N\n        $$\n\n        The heterogeneity is:\n        $$\n        \\mathcal{H}^{d-PDP}(x_s) = \\sqrt {{1 \\over N} \\sum_{i=1}^N ( \\hat{f}^i(x_s) - \\hat{f}^{d-PDP}(x_s) )^2}\n        $$\n\n    Notes:\n        - The required parameters are `data` and `model`. The rest are optional.\n        - The `model_jac` is the Jacobian of the model. If `None`, the Jacobian will be computed numerically.\n\n    Args:\n        data: the design matrix\n\n            - shape: `(N,D)`\n        model: the black-box model. Must be a `Callable` with:\n\n            - input: `ndarray` of shape `(N, D)`\n            - output: `ndarray` of shape `(N, )`\n\n        model_jac: the black-box model Jacobian. Must be a `Callable` with:\n\n            - input: `ndarray` of shape `(N, D)`\n            - output: `ndarray` of shape `(N, D)`\n\n        axis_limits: The limits of the feature effect plot along each axis\n\n            - use a `ndarray` of shape `(2, D)`, to specify them manually\n            - use `None`, to be inferred from the data\n\n        nof_instances: maximum number of instances to be used for PDP.\n\n            - use \"all\", for using all instances.\n            - use an `int`, for using `nof_instances` instances.\n\n        avg_output: The average output of the model.\n\n            - use a `float`, to specify it manually\n            - use `None`, to be inferred as `np.mean(model(data))`\n\n        feature_names: The names of the features\n\n            - use a `list` of `str`, to specify the name manually. For example: `[\"age\", \"weight\", ...]`\n            - use `None`, to keep the default names: `[\"x_0\", \"x_1\", ...]`\n\n        target_name: The name of the target variable\n\n            - use a `str`, to specify it name manually. For example: `\"price\"`\n            - use `None`, to keep the default name: `\"y\"`\n    \"\"\"\n\n    super(DerivativePDP, self).__init__(\n        data, model, model_jac, axis_limits, avg_output, nof_instances, feature_names, target_name, method_name=\"d-PDP\"\n    )\n</code></pre>"},{"location":"api/#effector.global_effect_shap.SHAPDependence","title":"<code>effector.global_effect_shap.SHAPDependence</code>","text":"<p>         Bases: <code>GlobalEffectBase</code></p> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_shap.py</code> <pre><code>class SHAPDependence(GlobalEffectBase):\n    def __init__(\n            self,\n            data: np.ndarray,\n            model: Callable,\n            axis_limits: Optional[np.ndarray] = None,\n            nof_instances: Union[int, str] = 100,\n            avg_output: Optional[float] = None,\n            feature_names: Optional[List[str]] = None,\n            target_name: Optional[str] = None,\n    ):\n\"\"\"\n        Constructor of the SHAPDependence class.\n\n        Definition:\n            The value of a coalition of $S$ features is estimated as:\n            $$\n            \\hat{v}(S) = {1 \\over N} \\sum_{i=1}^N  f(x_S \\cup x_C^i) - f(x^i)\n            $$\n            The value of a coalition $S$ quantifies what the values $\\mathbf{x}_S$ of the features in $S$ contribute to the output of the model. It\n            is the average (over all instances) difference on the output between setting features in $S$ to be $x_S$, i.e., $\\mathbf{x} = (\\mathbf{x}_S, \\mathbf{x}_C^i)$ and leaving the instance as it is, i.e., $\\mathbf{x}^i = (\\mathbf{x}_S^i, \\mathbf{x}_C^i)$.\n\n            The contribution of a feature $j$ added to a coalition $S$ is estimated as:\n            $$\n            \\hat{\\Delta}_{S, j} = \\hat{v}(S \\cup \\{j\\}) - \\hat{v}(S)\n            $$\n\n            The SHAP value of a feature $j$ with value $x_j$ is the average contribution of feature $j$ across all possible coalitions with a weight $w_{S, j}$:\n\n            $$\n            \\hat{\\phi}_j(x_j) = {1 \\over N} \\sum_{S \\subseteq \\{1, \\dots, D\\} \\setminus \\{j\\}} w_{S, j} \\hat{\\Delta}_{S, j}\n            $$\n\n            where $w_{S, j}$ assures that the contribution of feature $j$ is the same for all coalitions of the same size. For example, there are $D-1$ ways for $x_j$ to enter a coalition of $|S| = 1$ feature, so $w_{S, j} = {1 \\over D (D-1)}$ for each of them. In contrast, there is only one way for $x_j$ to enter a coaltion of $|S|=0$ (to be the first specified feature), so $w_{S, j} = {1 \\over D}$.\n\n            The SHAP Dependence Plot (SHAP-DP) is a spline $\\hat{f}^{SDP}_j(x_j)$ fit to the dataset $\\{(x_j^i, \\hat{\\phi}_j(x_j^i))\\}_{i=1}^N$ using the `UnivariateSpline` function from `scipy.interpolate`.\n\n        Notes:\n            * The required parameters are `data` and `model`. The rest are optional.\n            * SHAP values are computed using the `shap` package, using the class `Explainer`.\n            * SHAP values are centered by default, i.e., the average SHAP value is subtracted from the SHAP values.\n            * More details on the SHAP values can be found in the [original paper](https://arxiv.org/abs/1705.07874) and in the book [Interpreting Machine Learning Models with SHAP](https://christophmolnar.com/books/shap/)\n\n        Args:\n            data: the design matrix\n\n                - shape: `(N,D)`\n            model: the black-box model. Must be a `Callable` with:\n\n                - input: `ndarray` of shape `(N, D)`\n                - output: `ndarray` of shape `(N,)`\n\n            axis_limits: The limits of the feature effect plot along each axis\n\n                - use a `ndarray` of shape `(2, D)`, to specify them manually\n                - use `None`, to be inferred from the data\n\n            nof_instances: maximum number of instances to be used for SHAP estimation.\n\n                - use \"all\", for using all instances.\n                - use an `int`, for using `nof_instances` instances.\n\n            avg_output: The average output of the model.\n\n                - use a `float`, to specify it manually\n                - use `None`, to be inferred as `np.mean(model(data))`\n\n            feature_names: The names of the features\n\n                - use a `list` of `str`, to specify the name manually. For example: `                  [\"age\", \"weight\", ...]`\n                - use `None`, to keep the default names: `[\"x_0\", \"x_1\", ...]`\n\n            target_name: The name of the target variable\n\n                - use a `str`, to specify it name manually. For example: `\"price\"`\n                - use `None`, to keep the default name: `\"y\"`\n        \"\"\"\n        self.nof_instances, self.indices = helpers.prep_nof_instances(\n            nof_instances, data.shape[0]\n        )\n        data = data[self.indices, :]\n\n        super(SHAPDependence, self).__init__(\n            \"SHAP DP\", data, model, nof_instances, axis_limits, avg_output, feature_names, target_name\n        )\n\n    def _fit_feature(\n        self,\n        feature: int,\n        centering: typing.Union[bool, str] = False,\n        points_for_centering: int = 100,\n    ) -&gt; typing.Dict:\n\n        # drop points outside of limits\n        self.data = self.data[self.data[:, feature] &gt;= self.axis_limits[0, feature]]\n        self.data = self.data[self.data[:, feature] &lt;= self.axis_limits[1, feature]]\n\n        # compute shap values\n        data = self.data\n        shap_explainer = shap.Explainer(self.model, data)\n        explanation = shap_explainer(data)\n\n        # extract x and y pais\n        yy = explanation.values[:, feature]\n        xx = data[:, feature]\n\n        # make xx monotonic\n        idx = np.argsort(xx)\n        xx = xx[idx]\n        yy = yy[idx]\n\n        # fit spline_mean to xx, yy pairs\n        spline_mean = UnivariateSpline(xx, yy)\n\n        # fit spline_mean to the sqrt of the residuals\n        yy_std = np.abs(yy - spline_mean(xx))\n        spline_std = UnivariateSpline(xx, yy_std)\n\n        # compute norm constant\n        if centering == \"zero_integral\":\n            x_norm = np.linspace(xx[0], xx[-1], points_for_centering)\n            y_norm = spline_mean(x_norm)\n            norm_const = np.trapz(y_norm, x_norm) / (xx[-1] - xx[0])\n        elif centering == \"zero_start\":\n            norm_const = spline_mean(xx[0])\n        else:\n            norm_const = helpers.EMPTY_SYMBOL\n\n        ret_dict = {\n            \"spline_mean\": spline_mean,\n            \"spline_std\": spline_std,\n            \"xx\": xx,\n            \"yy\": yy,\n            \"norm_const\": norm_const,\n        }\n        return ret_dict\n\n    def fit(\n            self,\n            features: Union[int, str, List] = \"all\",\n            centering: Union[bool, str] = False,\n            points_for_centering: Union[int, str] = 100,\n    ) -&gt; None:\n\"\"\"Fit the SHAP Dependence Plot to the data.\n\n        Notes:\n            The SHAP Dependence Plot (SDP) $\\hat{f}^{SDP}_j(x_j)$ is a spline fit to\n            the dataset $\\{(x_j^i, \\hat{\\phi}_j(x_j^i))\\}_{i=1}^N$\n            using the `UnivariateSpline` function from `scipy.interpolate`.\n\n            The SHAP standard deviation, $\\hat{\\sigma}^{SDP}_j(x_j)$, is a spline fit            to the absolute value of the residuals, i.e., to the dataset $\\{(x_j^i, |\\hat{\\phi}_j(x_j^i) - \\hat{f}^{SDP}_j(x_j^i)|)\\}_{i=1}^N$, using the `UnivariateSpline` function from `scipy.interpolate`.\n\n        Args:\n            features: the features to fit.\n                - If set to \"all\", all the features will be fitted.\n            centering:\n                - If set to False, no centering will be applied.\n                - If set to \"zero_integral\" or True, the integral of the feature effect will be set to zero.\n                - If set to \"zero_mean\", the mean of the feature effect will be set to zero.\n\n            points_for_centering: number of linspaced points along the feature axis used for centering.\n\n                - If set to `all`, all the dataset points will be used.\n\n        Notes:\n            SHAP values are by default centered, i.e., $\\sum_{i=1}^N \\hat{\\phi}_j(x_j^i) = 0$. This does not mean that the SHAP _curve_ is centered around zero; this happens only if the $s$-th feature of the dataset instances, i.e., the set $\\{x_s^i\\}_{i=1}^N$ is uniformly distributed along the $s$-th axis. So, use:\n\n            * `centering=False`, to leave the SHAP values as they are.\n            * `centering=True` or `centering=zero_integral`, to center the SHAP curve around the `y` axis.\n            * `centering=zero_start`, to start the SHAP curve from `y=0`.\n\n            SHAP values are expensive to compute.\n            To speed up the computation consider using a subset of the dataset\n            points for computing the SHAP values and for centering the spline.\n            The default values (`points_for_fitting_spline=100`\n            and `points_for_centering=100`) are a moderate choice.\n        \"\"\"\n        centering = helpers.prep_centering(centering)\n        features = helpers.prep_features(features, self.dim)\n\n        # new implementation\n        for s in features:\n            self.feature_effect[\"feature_\" + str(s)] = self._fit_feature(\n                s, centering, points_for_centering\n            )\n            self.is_fitted[s] = True\n            self.method_args[\"feature_\" + str(s)] = {\n                \"centering\": centering,\n                \"points_for_centering\": points_for_centering,\n            }\n\n    def eval(\n        self,\n        feature: int,\n        xs: np.ndarray,\n        heterogeneity: bool = False,\n        centering: typing.Union[bool, str] = False,\n    ) -&gt; typing.Union[np.ndarray, typing.Tuple[np.ndarray, np.ndarray]]:\n\"\"\"Evaluate the effect of the s-th feature at positions `xs`.\n\n        Args:\n            feature: index of feature of interest\n            xs: the points along the s-th axis to evaluate the FE plot\n\n              - `np.ndarray` of shape `(T,)`\n            heterogeneity: whether to return the heterogeneity measures.\n\n                  - if `heterogeneity=False`, the function returns the mean effect at the given `xs`\n                  - If `heterogeneity=True`, the function returns `(y, std)` where `y` is the mean effect and `std` is the standard deviation of the mean effect\n\n            centering: whether to center the plot\n\n                - If `centering` is `False`, the SHAP curve is not centered\n                - If `centering` is `True` or `zero_integral`, the SHAP curve is centered around the `y` axis.\n                - If `centering` is `zero_start`, the SHAP curve starts from `y=0`.\n\n        Returns:\n            the mean effect `y`, if `heterogeneity=False` (default) or a tuple `(y, std, estimator_var)` otherwise\n        \"\"\"\n        centering = helpers.prep_centering(centering)\n\n        if self.refit(feature, centering):\n            self.fit(features=feature, centering=centering)\n\n        # Check if the lower bound is less than the upper bound\n        assert self.axis_limits[0, feature] &lt; self.axis_limits[1, feature]\n\n        yy = self.feature_effect[\"feature_\" + str(feature)][\"spline_mean\"](xs)\n\n        if centering is not False:\n            norm_const = self.feature_effect[\"feature_\" + str(feature)][\"norm_const\"]\n            yy = yy - norm_const\n\n        if heterogeneity:\n            yy_std = self.feature_effect[\"feature_\" + str(feature)][\"spline_std\"](xs)\n            return yy, yy_std\n        else:\n            return yy\n\n    def plot(\n        self,\n        feature: int,\n        heterogeneity: Union[bool, str] = False,\n        centering: Union[bool, str] = False,\n        nof_points: int = 30,\n        scale_x: Optional[dict] = None,\n        scale_y: Optional[dict] = None,\n        nof_shap_values: Union[int, str] = \"all\",\n        show_avg_output: bool = False,\n        y_limits: Optional[List] = None,\n    ) -&gt; None:\n\"\"\"\n        Plot the SHAP Dependence Plot (SDP) of the s-th feature.\n\n        Args:\n            feature: index of the plotted feature\n            heterogeneity: whether to output the heterogeneity of the SHAP values\n\n                - If `heterogeneity` is `False`, no heterogeneity is plotted\n                - If `heterogeneity` is `True` or `\"std\"`, the standard deviation of the shap values is plotted\n                - If `heterogeneity` is `\"shap_values\"`, the shap values are scattered on top of the SHAP curve\n\n            centering: whether to center the SDP\n\n                - If `centering` is `False`, the SHAP curve is not centered\n                - If `centering` is `True` or `zero_integral`, the SHAP curve is centered around the `y` axis.\n                - If `centering` is `zero_start`, the SHAP curve starts from `y=0`.\n\n            nof_points: number of points to evaluate the SDP plot\n            scale_x: dictionary with keys \"mean\" and \"std\" for scaling the x-axis\n            scale_y: dictionary with keys \"mean\" and \"std\" for scaling the y-axis\n            nof_shap_values: number of shap values to show on top of the SHAP curve\n            show_avg_output: whether to show the average output of the model\n            y_limits: limits of the y-axis\n        \"\"\"\n        heterogeneity = helpers.prep_confidence_interval(heterogeneity)\n\n        x = np.linspace(\n            self.axis_limits[0, feature], self.axis_limits[1, feature], nof_points\n        )\n\n        # get the SHAP curve\n        y = self.eval(feature, x, heterogeneity=False, centering=centering)\n        y_std = (\n            self.feature_effect[\"feature_\" + str(feature)][\"spline_std\"](x)\n            if heterogeneity == \"std\" or True\n            else None\n        )\n\n        # get some SHAP values\n        _, ind = helpers.prep_nof_instances(nof_shap_values, self.data.shape[0])\n        yy = (\n            self.feature_effect[\"feature_\" + str(feature)][\"yy\"][ind]\n            if heterogeneity == \"shap_values\"\n            else None\n        )\n        if yy is not None and centering is not False:\n            yy = yy - self.feature_effect[\"feature_\" + str(feature)][\"norm_const\"]\n        xx = (\n            self.feature_effect[\"feature_\" + str(feature)][\"xx\"][ind]\n            if heterogeneity == \"shap_values\"\n            else None\n        )\n\n        if show_avg_output:\n            avg_output = helpers.prep_avg_output(self.data, self.model, self.avg_output, scale_y)\n        else:\n            avg_output = None\n\n        vis.plot_shap(\n            x,\n            y,\n            xx,\n            yy,\n            y_std,\n            feature,\n            heterogeneity=heterogeneity,\n            scale_x=scale_x,\n            scale_y=scale_y,\n            avg_output=avg_output,\n            feature_names=self.feature_names,\n            target_name=self.target_name,\n            y_limits=y_limits\n        )\n</code></pre>"},{"location":"api/#effector.global_effect_shap.SHAPDependence.__init__","title":"<code>__init__(data, model, axis_limits=None, nof_instances=100, avg_output=None, feature_names=None, target_name=None)</code>","text":"<p>Constructor of the SHAPDependence class.</p> Definition <p>The value of a coalition of \\(S\\) features is estimated as: $$ \\hat{v}(S) = {1 \\over N} \\sum_{i=1}^N  f(x_S \\cup x_C^i) - f(x^i) $$ The value of a coalition \\(S\\) quantifies what the values \\(\\mathbf{x}_S\\) of the features in \\(S\\) contribute to the output of the model. It is the average (over all instances) difference on the output between setting features in \\(S\\) to be \\(x_S\\), i.e., \\(\\mathbf{x} = (\\mathbf{x}_S, \\mathbf{x}_C^i)\\) and leaving the instance as it is, i.e., \\(\\mathbf{x}^i = (\\mathbf{x}_S^i, \\mathbf{x}_C^i)\\).</p> <p>The contribution of a feature \\(j\\) added to a coalition \\(S\\) is estimated as: $$ \\hat{\\Delta}_{S, j} = \\hat{v}(S \\cup {j}) - \\hat{v}(S) $$</p> <p>The SHAP value of a feature \\(j\\) with value \\(x_j\\) is the average contribution of feature \\(j\\) across all possible coalitions with a weight \\(w_{S, j}\\):</p> \\[ \\hat{\\phi}_j(x_j) = {1 \\over N} \\sum_{S \\subseteq \\{1, \\dots, D\\} \\setminus \\{j\\}} w_{S, j} \\hat{\\Delta}_{S, j} \\] <p>where \\(w_{S, j}\\) assures that the contribution of feature \\(j\\) is the same for all coalitions of the same size. For example, there are \\(D-1\\) ways for \\(x_j\\) to enter a coalition of \\(|S| = 1\\) feature, so \\(w_{S, j} = {1 \\over D (D-1)}\\) for each of them. In contrast, there is only one way for \\(x_j\\) to enter a coaltion of \\(|S|=0\\) (to be the first specified feature), so \\(w_{S, j} = {1 \\over D}\\).</p> <p>The SHAP Dependence Plot (SHAP-DP) is a spline \\(\\hat{f}^{SDP}_j(x_j)\\) fit to the dataset \\(\\{(x_j^i, \\hat{\\phi}_j(x_j^i))\\}_{i=1}^N\\) using the <code>UnivariateSpline</code> function from <code>scipy.interpolate</code>.</p> Notes <ul> <li>The required parameters are <code>data</code> and <code>model</code>. The rest are optional.</li> <li>SHAP values are computed using the <code>shap</code> package, using the class <code>Explainer</code>.</li> <li>SHAP values are centered by default, i.e., the average SHAP value is subtracted from the SHAP values.</li> <li>More details on the SHAP values can be found in the original paper and in the book Interpreting Machine Learning Models with SHAP</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray</code> <p>the design matrix</p> <ul> <li>shape: <code>(N,D)</code></li> </ul> required <code>model</code> <code>Callable</code> <p>the black-box model. Must be a <code>Callable</code> with:</p> <ul> <li>input: <code>ndarray</code> of shape <code>(N, D)</code></li> <li>output: <code>ndarray</code> of shape <code>(N,)</code></li> </ul> required <code>axis_limits</code> <code>Optional[np.ndarray]</code> <p>The limits of the feature effect plot along each axis</p> <ul> <li>use a <code>ndarray</code> of shape <code>(2, D)</code>, to specify them manually</li> <li>use <code>None</code>, to be inferred from the data</li> </ul> <code>None</code> <code>nof_instances</code> <code>Union[int, str]</code> <p>maximum number of instances to be used for SHAP estimation.</p> <ul> <li>use \"all\", for using all instances.</li> <li>use an <code>int</code>, for using <code>nof_instances</code> instances.</li> </ul> <code>100</code> <code>avg_output</code> <code>Optional[float]</code> <p>The average output of the model.</p> <ul> <li>use a <code>float</code>, to specify it manually</li> <li>use <code>None</code>, to be inferred as <code>np.mean(model(data))</code></li> </ul> <code>None</code> <code>feature_names</code> <code>Optional[List[str]]</code> <p>The names of the features</p> <ul> <li>use a <code>list</code> of <code>str</code>, to specify the name manually. For example: <code>[\"age\", \"weight\", ...]</code></li> <li>use <code>None</code>, to keep the default names: <code>[\"x_0\", \"x_1\", ...]</code></li> </ul> <code>None</code> <code>target_name</code> <code>Optional[str]</code> <p>The name of the target variable</p> <ul> <li>use a <code>str</code>, to specify it name manually. For example: <code>\"price\"</code></li> <li>use <code>None</code>, to keep the default name: <code>\"y\"</code></li> </ul> <code>None</code> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_shap.py</code> <pre><code>def __init__(\n        self,\n        data: np.ndarray,\n        model: Callable,\n        axis_limits: Optional[np.ndarray] = None,\n        nof_instances: Union[int, str] = 100,\n        avg_output: Optional[float] = None,\n        feature_names: Optional[List[str]] = None,\n        target_name: Optional[str] = None,\n):\n\"\"\"\n    Constructor of the SHAPDependence class.\n\n    Definition:\n        The value of a coalition of $S$ features is estimated as:\n        $$\n        \\hat{v}(S) = {1 \\over N} \\sum_{i=1}^N  f(x_S \\cup x_C^i) - f(x^i)\n        $$\n        The value of a coalition $S$ quantifies what the values $\\mathbf{x}_S$ of the features in $S$ contribute to the output of the model. It\n        is the average (over all instances) difference on the output between setting features in $S$ to be $x_S$, i.e., $\\mathbf{x} = (\\mathbf{x}_S, \\mathbf{x}_C^i)$ and leaving the instance as it is, i.e., $\\mathbf{x}^i = (\\mathbf{x}_S^i, \\mathbf{x}_C^i)$.\n\n        The contribution of a feature $j$ added to a coalition $S$ is estimated as:\n        $$\n        \\hat{\\Delta}_{S, j} = \\hat{v}(S \\cup \\{j\\}) - \\hat{v}(S)\n        $$\n\n        The SHAP value of a feature $j$ with value $x_j$ is the average contribution of feature $j$ across all possible coalitions with a weight $w_{S, j}$:\n\n        $$\n        \\hat{\\phi}_j(x_j) = {1 \\over N} \\sum_{S \\subseteq \\{1, \\dots, D\\} \\setminus \\{j\\}} w_{S, j} \\hat{\\Delta}_{S, j}\n        $$\n\n        where $w_{S, j}$ assures that the contribution of feature $j$ is the same for all coalitions of the same size. For example, there are $D-1$ ways for $x_j$ to enter a coalition of $|S| = 1$ feature, so $w_{S, j} = {1 \\over D (D-1)}$ for each of them. In contrast, there is only one way for $x_j$ to enter a coaltion of $|S|=0$ (to be the first specified feature), so $w_{S, j} = {1 \\over D}$.\n\n        The SHAP Dependence Plot (SHAP-DP) is a spline $\\hat{f}^{SDP}_j(x_j)$ fit to the dataset $\\{(x_j^i, \\hat{\\phi}_j(x_j^i))\\}_{i=1}^N$ using the `UnivariateSpline` function from `scipy.interpolate`.\n\n    Notes:\n        * The required parameters are `data` and `model`. The rest are optional.\n        * SHAP values are computed using the `shap` package, using the class `Explainer`.\n        * SHAP values are centered by default, i.e., the average SHAP value is subtracted from the SHAP values.\n        * More details on the SHAP values can be found in the [original paper](https://arxiv.org/abs/1705.07874) and in the book [Interpreting Machine Learning Models with SHAP](https://christophmolnar.com/books/shap/)\n\n    Args:\n        data: the design matrix\n\n            - shape: `(N,D)`\n        model: the black-box model. Must be a `Callable` with:\n\n            - input: `ndarray` of shape `(N, D)`\n            - output: `ndarray` of shape `(N,)`\n\n        axis_limits: The limits of the feature effect plot along each axis\n\n            - use a `ndarray` of shape `(2, D)`, to specify them manually\n            - use `None`, to be inferred from the data\n\n        nof_instances: maximum number of instances to be used for SHAP estimation.\n\n            - use \"all\", for using all instances.\n            - use an `int`, for using `nof_instances` instances.\n\n        avg_output: The average output of the model.\n\n            - use a `float`, to specify it manually\n            - use `None`, to be inferred as `np.mean(model(data))`\n\n        feature_names: The names of the features\n\n            - use a `list` of `str`, to specify the name manually. For example: `                  [\"age\", \"weight\", ...]`\n            - use `None`, to keep the default names: `[\"x_0\", \"x_1\", ...]`\n\n        target_name: The name of the target variable\n\n            - use a `str`, to specify it name manually. For example: `\"price\"`\n            - use `None`, to keep the default name: `\"y\"`\n    \"\"\"\n    self.nof_instances, self.indices = helpers.prep_nof_instances(\n        nof_instances, data.shape[0]\n    )\n    data = data[self.indices, :]\n\n    super(SHAPDependence, self).__init__(\n        \"SHAP DP\", data, model, nof_instances, axis_limits, avg_output, feature_names, target_name\n    )\n</code></pre>"},{"location":"api/#effector.global_effect_shap.SHAPDependence.fit","title":"<code>fit(features='all', centering=False, points_for_centering=100)</code>","text":"<p>Fit the SHAP Dependence Plot to the data.</p> Notes <p>The SHAP Dependence Plot (SDP) \\(\\hat{f}^{SDP}_j(x_j)\\) is a spline fit to the dataset \\(\\{(x_j^i, \\hat{\\phi}_j(x_j^i))\\}_{i=1}^N\\) using the <code>UnivariateSpline</code> function from <code>scipy.interpolate</code>.</p> <p>The SHAP standard deviation, \\(\\hat{\\sigma}^{SDP}_j(x_j)\\), is a spline fit            to the absolute value of the residuals, i.e., to the dataset \\(\\{(x_j^i, |\\hat{\\phi}_j(x_j^i) - \\hat{f}^{SDP}_j(x_j^i)|)\\}_{i=1}^N\\), using the <code>UnivariateSpline</code> function from <code>scipy.interpolate</code>.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Union[int, str, List]</code> <p>the features to fit. - If set to \"all\", all the features will be fitted.</p> <code>'all'</code> <code>centering</code> <code>Union[bool, str]</code> <ul> <li>If set to False, no centering will be applied.</li> <li>If set to \"zero_integral\" or True, the integral of the feature effect will be set to zero.</li> <li>If set to \"zero_mean\", the mean of the feature effect will be set to zero.</li> </ul> <code>False</code> <code>points_for_centering</code> <code>Union[int, str]</code> <p>number of linspaced points along the feature axis used for centering.</p> <ul> <li>If set to <code>all</code>, all the dataset points will be used.</li> </ul> <code>100</code> Notes <p>SHAP values are by default centered, i.e., \\(\\sum_{i=1}^N \\hat{\\phi}_j(x_j^i) = 0\\). This does not mean that the SHAP curve is centered around zero; this happens only if the \\(s\\)-th feature of the dataset instances, i.e., the set \\(\\{x_s^i\\}_{i=1}^N\\) is uniformly distributed along the \\(s\\)-th axis. So, use:</p> <ul> <li><code>centering=False</code>, to leave the SHAP values as they are.</li> <li><code>centering=True</code> or <code>centering=zero_integral</code>, to center the SHAP curve around the <code>y</code> axis.</li> <li><code>centering=zero_start</code>, to start the SHAP curve from <code>y=0</code>.</li> </ul> <p>SHAP values are expensive to compute. To speed up the computation consider using a subset of the dataset points for computing the SHAP values and for centering the spline. The default values (<code>points_for_fitting_spline=100</code> and <code>points_for_centering=100</code>) are a moderate choice.</p> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_shap.py</code> <pre><code>def fit(\n        self,\n        features: Union[int, str, List] = \"all\",\n        centering: Union[bool, str] = False,\n        points_for_centering: Union[int, str] = 100,\n) -&gt; None:\n\"\"\"Fit the SHAP Dependence Plot to the data.\n\n    Notes:\n        The SHAP Dependence Plot (SDP) $\\hat{f}^{SDP}_j(x_j)$ is a spline fit to\n        the dataset $\\{(x_j^i, \\hat{\\phi}_j(x_j^i))\\}_{i=1}^N$\n        using the `UnivariateSpline` function from `scipy.interpolate`.\n\n        The SHAP standard deviation, $\\hat{\\sigma}^{SDP}_j(x_j)$, is a spline fit            to the absolute value of the residuals, i.e., to the dataset $\\{(x_j^i, |\\hat{\\phi}_j(x_j^i) - \\hat{f}^{SDP}_j(x_j^i)|)\\}_{i=1}^N$, using the `UnivariateSpline` function from `scipy.interpolate`.\n\n    Args:\n        features: the features to fit.\n            - If set to \"all\", all the features will be fitted.\n        centering:\n            - If set to False, no centering will be applied.\n            - If set to \"zero_integral\" or True, the integral of the feature effect will be set to zero.\n            - If set to \"zero_mean\", the mean of the feature effect will be set to zero.\n\n        points_for_centering: number of linspaced points along the feature axis used for centering.\n\n            - If set to `all`, all the dataset points will be used.\n\n    Notes:\n        SHAP values are by default centered, i.e., $\\sum_{i=1}^N \\hat{\\phi}_j(x_j^i) = 0$. This does not mean that the SHAP _curve_ is centered around zero; this happens only if the $s$-th feature of the dataset instances, i.e., the set $\\{x_s^i\\}_{i=1}^N$ is uniformly distributed along the $s$-th axis. So, use:\n\n        * `centering=False`, to leave the SHAP values as they are.\n        * `centering=True` or `centering=zero_integral`, to center the SHAP curve around the `y` axis.\n        * `centering=zero_start`, to start the SHAP curve from `y=0`.\n\n        SHAP values are expensive to compute.\n        To speed up the computation consider using a subset of the dataset\n        points for computing the SHAP values and for centering the spline.\n        The default values (`points_for_fitting_spline=100`\n        and `points_for_centering=100`) are a moderate choice.\n    \"\"\"\n    centering = helpers.prep_centering(centering)\n    features = helpers.prep_features(features, self.dim)\n\n    # new implementation\n    for s in features:\n        self.feature_effect[\"feature_\" + str(s)] = self._fit_feature(\n            s, centering, points_for_centering\n        )\n        self.is_fitted[s] = True\n        self.method_args[\"feature_\" + str(s)] = {\n            \"centering\": centering,\n            \"points_for_centering\": points_for_centering,\n        }\n</code></pre>"},{"location":"api/#effector.global_effect_shap.SHAPDependence.eval","title":"<code>eval(feature, xs, heterogeneity=False, centering=False)</code>","text":"<p>Evaluate the effect of the s-th feature at positions <code>xs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>int</code> <p>index of feature of interest</p> required <code>xs</code> <code>np.ndarray</code> <p>the points along the s-th axis to evaluate the FE plot</p> <ul> <li><code>np.ndarray</code> of shape <code>(T,)</code></li> </ul> required <code>heterogeneity</code> <code>bool</code> <p>whether to return the heterogeneity measures.</p> <ul> <li>if <code>heterogeneity=False</code>, the function returns the mean effect at the given <code>xs</code></li> <li>If <code>heterogeneity=True</code>, the function returns <code>(y, std)</code> where <code>y</code> is the mean effect and <code>std</code> is the standard deviation of the mean effect</li> </ul> <code>False</code> <code>centering</code> <code>typing.Union[bool, str]</code> <p>whether to center the plot</p> <ul> <li>If <code>centering</code> is <code>False</code>, the SHAP curve is not centered</li> <li>If <code>centering</code> is <code>True</code> or <code>zero_integral</code>, the SHAP curve is centered around the <code>y</code> axis.</li> <li>If <code>centering</code> is <code>zero_start</code>, the SHAP curve starts from <code>y=0</code>.</li> </ul> <code>False</code> <p>Returns:</p> Type Description <code>typing.Union[np.ndarray, typing.Tuple[np.ndarray, np.ndarray]]</code> <p>the mean effect <code>y</code>, if <code>heterogeneity=False</code> (default) or a tuple <code>(y, std, estimator_var)</code> otherwise</p> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_shap.py</code> <pre><code>def eval(\n    self,\n    feature: int,\n    xs: np.ndarray,\n    heterogeneity: bool = False,\n    centering: typing.Union[bool, str] = False,\n) -&gt; typing.Union[np.ndarray, typing.Tuple[np.ndarray, np.ndarray]]:\n\"\"\"Evaluate the effect of the s-th feature at positions `xs`.\n\n    Args:\n        feature: index of feature of interest\n        xs: the points along the s-th axis to evaluate the FE plot\n\n          - `np.ndarray` of shape `(T,)`\n        heterogeneity: whether to return the heterogeneity measures.\n\n              - if `heterogeneity=False`, the function returns the mean effect at the given `xs`\n              - If `heterogeneity=True`, the function returns `(y, std)` where `y` is the mean effect and `std` is the standard deviation of the mean effect\n\n        centering: whether to center the plot\n\n            - If `centering` is `False`, the SHAP curve is not centered\n            - If `centering` is `True` or `zero_integral`, the SHAP curve is centered around the `y` axis.\n            - If `centering` is `zero_start`, the SHAP curve starts from `y=0`.\n\n    Returns:\n        the mean effect `y`, if `heterogeneity=False` (default) or a tuple `(y, std, estimator_var)` otherwise\n    \"\"\"\n    centering = helpers.prep_centering(centering)\n\n    if self.refit(feature, centering):\n        self.fit(features=feature, centering=centering)\n\n    # Check if the lower bound is less than the upper bound\n    assert self.axis_limits[0, feature] &lt; self.axis_limits[1, feature]\n\n    yy = self.feature_effect[\"feature_\" + str(feature)][\"spline_mean\"](xs)\n\n    if centering is not False:\n        norm_const = self.feature_effect[\"feature_\" + str(feature)][\"norm_const\"]\n        yy = yy - norm_const\n\n    if heterogeneity:\n        yy_std = self.feature_effect[\"feature_\" + str(feature)][\"spline_std\"](xs)\n        return yy, yy_std\n    else:\n        return yy\n</code></pre>"},{"location":"api/#effector.global_effect_shap.SHAPDependence.plot","title":"<code>plot(feature, heterogeneity=False, centering=False, nof_points=30, scale_x=None, scale_y=None, nof_shap_values='all', show_avg_output=False, y_limits=None)</code>","text":"<p>Plot the SHAP Dependence Plot (SDP) of the s-th feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>int</code> <p>index of the plotted feature</p> required <code>heterogeneity</code> <code>Union[bool, str]</code> <p>whether to output the heterogeneity of the SHAP values</p> <ul> <li>If <code>heterogeneity</code> is <code>False</code>, no heterogeneity is plotted</li> <li>If <code>heterogeneity</code> is <code>True</code> or <code>\"std\"</code>, the standard deviation of the shap values is plotted</li> <li>If <code>heterogeneity</code> is <code>\"shap_values\"</code>, the shap values are scattered on top of the SHAP curve</li> </ul> <code>False</code> <code>centering</code> <code>Union[bool, str]</code> <p>whether to center the SDP</p> <ul> <li>If <code>centering</code> is <code>False</code>, the SHAP curve is not centered</li> <li>If <code>centering</code> is <code>True</code> or <code>zero_integral</code>, the SHAP curve is centered around the <code>y</code> axis.</li> <li>If <code>centering</code> is <code>zero_start</code>, the SHAP curve starts from <code>y=0</code>.</li> </ul> <code>False</code> <code>nof_points</code> <code>int</code> <p>number of points to evaluate the SDP plot</p> <code>30</code> <code>scale_x</code> <code>Optional[dict]</code> <p>dictionary with keys \"mean\" and \"std\" for scaling the x-axis</p> <code>None</code> <code>scale_y</code> <code>Optional[dict]</code> <p>dictionary with keys \"mean\" and \"std\" for scaling the y-axis</p> <code>None</code> <code>nof_shap_values</code> <code>Union[int, str]</code> <p>number of shap values to show on top of the SHAP curve</p> <code>'all'</code> <code>show_avg_output</code> <code>bool</code> <p>whether to show the average output of the model</p> <code>False</code> <code>y_limits</code> <code>Optional[List]</code> <p>limits of the y-axis</p> <code>None</code> Source code in <code>/home/runner/work/effector/effector/effector/global_effect_shap.py</code> <pre><code>def plot(\n    self,\n    feature: int,\n    heterogeneity: Union[bool, str] = False,\n    centering: Union[bool, str] = False,\n    nof_points: int = 30,\n    scale_x: Optional[dict] = None,\n    scale_y: Optional[dict] = None,\n    nof_shap_values: Union[int, str] = \"all\",\n    show_avg_output: bool = False,\n    y_limits: Optional[List] = None,\n) -&gt; None:\n\"\"\"\n    Plot the SHAP Dependence Plot (SDP) of the s-th feature.\n\n    Args:\n        feature: index of the plotted feature\n        heterogeneity: whether to output the heterogeneity of the SHAP values\n\n            - If `heterogeneity` is `False`, no heterogeneity is plotted\n            - If `heterogeneity` is `True` or `\"std\"`, the standard deviation of the shap values is plotted\n            - If `heterogeneity` is `\"shap_values\"`, the shap values are scattered on top of the SHAP curve\n\n        centering: whether to center the SDP\n\n            - If `centering` is `False`, the SHAP curve is not centered\n            - If `centering` is `True` or `zero_integral`, the SHAP curve is centered around the `y` axis.\n            - If `centering` is `zero_start`, the SHAP curve starts from `y=0`.\n\n        nof_points: number of points to evaluate the SDP plot\n        scale_x: dictionary with keys \"mean\" and \"std\" for scaling the x-axis\n        scale_y: dictionary with keys \"mean\" and \"std\" for scaling the y-axis\n        nof_shap_values: number of shap values to show on top of the SHAP curve\n        show_avg_output: whether to show the average output of the model\n        y_limits: limits of the y-axis\n    \"\"\"\n    heterogeneity = helpers.prep_confidence_interval(heterogeneity)\n\n    x = np.linspace(\n        self.axis_limits[0, feature], self.axis_limits[1, feature], nof_points\n    )\n\n    # get the SHAP curve\n    y = self.eval(feature, x, heterogeneity=False, centering=centering)\n    y_std = (\n        self.feature_effect[\"feature_\" + str(feature)][\"spline_std\"](x)\n        if heterogeneity == \"std\" or True\n        else None\n    )\n\n    # get some SHAP values\n    _, ind = helpers.prep_nof_instances(nof_shap_values, self.data.shape[0])\n    yy = (\n        self.feature_effect[\"feature_\" + str(feature)][\"yy\"][ind]\n        if heterogeneity == \"shap_values\"\n        else None\n    )\n    if yy is not None and centering is not False:\n        yy = yy - self.feature_effect[\"feature_\" + str(feature)][\"norm_const\"]\n    xx = (\n        self.feature_effect[\"feature_\" + str(feature)][\"xx\"][ind]\n        if heterogeneity == \"shap_values\"\n        else None\n    )\n\n    if show_avg_output:\n        avg_output = helpers.prep_avg_output(self.data, self.model, self.avg_output, scale_y)\n    else:\n        avg_output = None\n\n    vis.plot_shap(\n        x,\n        y,\n        xx,\n        yy,\n        y_std,\n        feature,\n        heterogeneity=heterogeneity,\n        scale_x=scale_x,\n        scale_y=scale_y,\n        avg_output=avg_output,\n        feature_names=self.feature_names,\n        target_name=self.target_name,\n        y_limits=y_limits\n    )\n</code></pre>"},{"location":"api/#regional-effect-methods","title":"Regional Effect Methods","text":""},{"location":"api/#effector.regional_effect.RegionalEffectBase","title":"<code>effector.regional_effect.RegionalEffectBase</code>","text":"Source code in <code>/home/runner/work/effector/effector/effector/regional_effect.py</code> <pre><code>class RegionalEffectBase:\n    empty_symbol = helpers.EMPTY_SYMBOL\n\n    def __init__(\n        self,\n        method_name: str,\n        data: np.ndarray,\n        model: Callable,\n        model_jac: Optional[Callable] = None,\n        data_effect: Optional[np.ndarray] = None,\n        nof_instances: Union[int, str] = 100,\n        axis_limits: Optional[np.ndarray] = None,\n        feature_types: Optional[List] = None,\n        cat_limit: Optional[int] = 10,\n        feature_names: Optional[List] = None,\n        target_name: Optional[str] = None,\n    ) -&gt; None:\n\"\"\"\n        Constructor for the RegionalEffect class.\n        \"\"\"\n        self.method_name = method_name.lower()\n        self.model = model\n        self.model_jac = model_jac\n\n        # select nof_instances from the data\n        self.nof_instances, self.indices = helpers.prep_nof_instances(\n            nof_instances, data.shape[0]\n        )\n        self.data = data[self.indices, :]\n        self.instance_effects = data_effect[self.indices, :] if data_effect is not None else None\n        self.dim = self.data.shape[1]\n\n        # set axis_limits\n        axis_limits = (\n            helpers.axis_limits_from_data(data) if axis_limits is None else axis_limits\n        )\n        self.axis_limits: np.ndarray = axis_limits\n\n        # set feature types\n        self.cat_limit = cat_limit\n        feature_types = (\n            utils.get_feature_types(data, cat_limit)\n            if feature_types is None\n            else feature_types\n        )\n        self.feature_types: list = feature_types\n\n        # set feature names\n        feature_names: list[str] = (\n            helpers.get_feature_names(axis_limits.shape[1])\n            if feature_names is None\n            else feature_names\n        )\n        self.feature_names: list = feature_names\n\n        # set target name\n        self.target_name = \"y\" if target_name is None else target_name\n\n        # state variables\n        self.is_fitted: np.ndarray = np.ones([self.dim]) &lt; 0\n\n        # parameters used when fitting the regional effect\n        self.method_args: typing.Dict = {}\n\n        # dictionary with all the information required for plotting or evaluating the regional effects\n        self.partitioners: typing.Dict[str, Regions] = {}\n        self.tree_full: typing.Dict[str, Tree] = {}\n        self.tree_pruned: typing.Dict[str, Tree] = {}\n        self.tree_full_scaled: typing.Dict[str, Tree] = {}\n        self.tree_pruned_scaled: typing.Dict[str, Tree] = {}\n\n    def _fit_feature(\n        self,\n        feature: int,\n        heter_func: Callable,\n        heter_pcg_drop_thres: float = 0.1,\n        heter_small_enough: float = 0.1,\n        max_split_levels: int = 2,\n        candidate_positions_for_numerical: int = 20,\n        min_points_per_subregion: int = 10,\n        candidate_foc: Union[str, List] = \"all\",\n        split_categorical_features: bool = False,\n    ):\n\"\"\"\n        Find the subregions for a single feature.\n        \"\"\"\n        # init Region Extractor\n        regions = Regions(\n            feature,\n            heter_func,\n            self.data,\n            self.instance_effects,\n            self.feature_types,\n            self.feature_names,\n            self.target_name,\n            self.cat_limit,\n            candidate_foc,\n            min_points_per_subregion,\n            candidate_positions_for_numerical,\n            max_split_levels,\n            heter_pcg_drop_thres,\n            heter_small_enough,\n            split_categorical_features,\n        )\n\n        # apply partitioning\n        regions.search_all_splits()\n        regions.choose_important_splits()\n        self.tree_full[\"feature_{}\".format(feature)] = regions.splits_to_tree()\n        self.tree_pruned[\"feature_{}\".format(feature)] = regions.splits_to_tree(True)\n\n        # store the partitioning object\n        self.partitioners[\"feature_{}\".format(feature)] = regions\n\n        # update state\n        self.is_fitted[feature] = True\n\n    def refit(self, feature):\n        if not self.is_fitted[feature]:\n            self.fit(feature)\n\n    def get_node_info(self, feature, node_idx):\n        assert self.is_fitted[feature], \"Feature {} has not been fitted yet\".format(feature)\n        assert self.tree_pruned[\"feature_{}\".format(feature)] is not None, \"Feature {} has no splits\".format(feature)\n\n        if self.tree_pruned_scaled is not None and \"feature_{}\".format(feature) in self.tree_pruned_scaled.keys():\n            tree = self.tree_pruned_scaled[\"feature_{}\".format(feature)]\n        else:\n            tree = self.tree_pruned[\"feature_{}\".format(feature)]\n\n        # assert node id exists\n        assert node_idx in [node.idx for node in tree.nodes], \"Node {} does not exist\".format(node_idx)\n\n        # find the node\n        node = [node for node in tree.nodes if node.idx == node_idx][0]\n\n        # get data\n        data = node.data[\"data\"]\n        data_effect = node.data[\"data_effect\"]\n        name = node.name\n        return data, data_effect, name\n\n    def _create_fe_object(self, data, data_effect, feature_names):\n        if self.method_name == \"rhale\":\n            return RHALE(data, self.model, self.model_jac, data_effect=data_effect, feature_names=feature_names, target_name=self.target_name)\n        elif self.method_name == \"ale\":\n            return ALE(data, self.model, feature_names=feature_names, target_name=self.target_name)\n        elif self.method_name == \"shap\":\n            return SHAPDependence(data, self.model, feature_names=feature_names, target_name=self.target_name)\n        elif self.method_name == \"pdp\":\n            return PDP(data, self.model, feature_names=feature_names, target_name=self.target_name)\n        elif self.method_name == \"d-pdp\":\n            return DerivativePDP(data, self.model, self.model_jac, feature_names=feature_names, target_name=self.target_name)\n        else:\n            raise NotImplementedError\n\n    def eval(self, feature, node_idx, xs, heterogeneity=False, centering=False):\n\"\"\"\n        Evaluate the regional effect for a given feature and node.\n\n        Args:\n            feature: the feature to evaluate\n            node_idx: the node corresponding to the subregion to evaluate\n            xs: the points at which to evaluate the regional effect\n            heterogeneity: whether to return the heterogeneity.\n\n                  - if `heterogeneity=False`, the function returns the mean effect at the given `xs`\n                  - If `heterogeneity=True`, the function returns `(y, std)` where `y` is the mean effect and `std` is the standard deviation of the mean effect\n\n            centering: whether to center the regional effect. The following options are available:\n\n                - If `centering` is `False`, the regional effect is not centered\n                - If `centering` is `True` or `zero_integral`, the regional effect is centered around the `y` axis.\n                - If `centering` is `zero_start`, the regional effect starts from `y=0`.\n\n        Returns:\n            the mean effect `y`, if `heterogeneity=False` (default) or a tuple `(y, std)` otherwise\n\n        \"\"\"\n        self.refit(feature)\n        centering = helpers.prep_centering(centering)\n        data, data_effect, _ = self.get_node_info(feature, node_idx)\n        fe_method = self._create_fe_object(data, data_effect, None)\n        return fe_method.eval(feature, xs, heterogeneity, centering)\n\n    def fit(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def plot(self,\n             feature,\n             node_idx,\n             heterogeneity=False,\n             centering=False,\n             scale_x_list=None,\n             scale_y=None,\n             y_limits=None):\n\n        self.refit(feature)\n\n        if scale_x_list is not None:\n            self.tree_full_scaled[\"feature_{}\".format(feature)] = self.partitioners[\"feature_{}\".format(feature)].splits_to_tree(False, scale_x_list)\n            self.tree_pruned_scaled[\"feature_{}\".format(feature)] = self.partitioners[\"feature_{}\".format(feature)].splits_to_tree(True, scale_x_list)\n\n        data, data_effect, name = self.get_node_info(feature, node_idx)\n        feature_names = copy.deepcopy(self.feature_names)\n        feature_names[feature] = name\n        fe_method = self._create_fe_object(data, data_effect, feature_names)\n\n        return fe_method.plot(\n            feature=feature,\n            heterogeneity=heterogeneity,\n            centering=centering,\n            scale_x=scale_x_list[feature] if scale_x_list is not None else None,\n            scale_y=scale_y,\n            y_limits=y_limits\n            )\n\n    def show_partitioning(self, features, only_important=True, scale_x_list=None):\n        features = helpers.prep_features(features, self.dim)\n\n        for feat in features:\n            self.refit(feat)\n\n            if scale_x_list is not None:\n                tree_full_scaled = self.partitioners[\"feature_{}\".format(feat)].splits_to_tree(True, scale_x_list)\n                tree_pruned_scaled = self.partitioners[\"feature_{}\".format(feat)].splits_to_tree(False, scale_x_list)\n                tree_dict = tree_full_scaled if only_important else tree_pruned_scaled\n            else:\n                tree_dict = self.tree_pruned[\"feature_{}\".format(feat)] if only_important else self.tree_full[\"feature_{}\".format(feat)]\n\n            print(\"Feature {} - Full partition tree:\".format(feat))\n\n            if tree_dict is None:\n                print(\"No splits found for feature {}\".format(feat))\n            else:\n                tree_dict.show_full_tree()\n\n            print(\"-\" * 50)\n            print(\"Feature {} - Statistics per tree level:\".format(feat))\n\n            if tree_dict is None:\n                print(\"No splits found for feature {}\".format(feat))\n            else:\n                tree_dict.show_level_stats()\n\n    def describe_subregions(\n        self,\n        features,\n        only_important=True,\n        scale_x_list: typing.Union[None, typing.List[dict]] = None,\n    ):\n        features = helpers.prep_features(features, self.dim)\n        for feature in features:\n            self.refit(feature)\n\n            # it means it a categorical feature\n            if self.tree_full[\"feature_{}\".format(feature)] is None:\n                continue\n\n            feature_name = self.feature_names[feature]\n            if only_important:\n                tree = self.tree_pruned[\"feature_{}\".format(feature)]\n                if len(tree.nodes) == 1:\n                    print(\"No important splits found for feature {}\".format(feature))\n                    continue\n                else:\n                    print(\"Important splits for feature {}\".format(feature_name))\n            else:\n                print(\"All splits for feature {}\".format(feature_name))\n                tree = self.tree_full[\"feature_{}\".format(feature)]\n\n            max_level = max([node.level for node in tree.nodes])\n            for level in range(1, max_level+1):\n                previous_level_nodes = tree.get_level_nodes(level-1)\n                level_nodes = tree.get_level_nodes(level)\n                type_of_split_feature = level_nodes[0].data[\"feature_type\"]\n                foc_name = self.feature_names[level_nodes[0].data[\"feature\"]]\n                print(\"- On feature {} ({})\".format(foc_name, type_of_split_feature))\n\n                position_split_formatted = (\n                    \"{:.2f}\".format(level_nodes[0].data[\"position\"])\n                    if scale_x_list is None\n                    else \"{:.2f}\".format(\n                        level_nodes[0].data[\"position\"] * scale_x_list[level_nodes[0].data[\"feature\"]][\"std\"]\n                        + scale_x_list[level_nodes[0].data[\"feature\"]][\"mean\"]\n                    )\n                )\n                print(\"  - Position of split: {}\".format(position_split_formatted))\n\n                weight_heter_before = np.sum([node.data[\"weight\"] * node.data[\"heterogeneity\"] for node in previous_level_nodes])\n                print(\"  - Heterogeneity before split: {:.2f}\".format(weight_heter_before))\n\n                weight_heter = np.sum([node.data[\"weight\"] * node.data[\"heterogeneity\"] for node in level_nodes])\n                print(\"  - Heterogeneity after split: {:.2f}\".format(weight_heter))\n                weight_heter_drop = weight_heter_before - weight_heter\n                print(\"  - Heterogeneity drop: {:.2f} ({:.2f} %)\".format(\n                    weight_heter_drop, weight_heter_drop / weight_heter_before * 100)\n                )\n\n                nof_instances_before = [nod.data[\"nof_instances\"] for nod in previous_level_nodes]\n                print(\"  - Number of instances before split: {}\".format(nof_instances_before))\n                nof_instances = [nod.data[\"nof_instances\"] for nod in level_nodes]\n                print(\"  - Number of instances after split: {}\".format(nof_instances))\n</code></pre>"},{"location":"api/#effector.regional_effect.RegionalEffectBase.eval","title":"<code>eval(feature, node_idx, xs, heterogeneity=False, centering=False)</code>","text":"<p>Evaluate the regional effect for a given feature and node.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <p>the feature to evaluate</p> required <code>node_idx</code> <p>the node corresponding to the subregion to evaluate</p> required <code>xs</code> <p>the points at which to evaluate the regional effect</p> required <code>heterogeneity</code> <p>whether to return the heterogeneity.</p> <ul> <li>if <code>heterogeneity=False</code>, the function returns the mean effect at the given <code>xs</code></li> <li>If <code>heterogeneity=True</code>, the function returns <code>(y, std)</code> where <code>y</code> is the mean effect and <code>std</code> is the standard deviation of the mean effect</li> </ul> <code>False</code> <code>centering</code> <p>whether to center the regional effect. The following options are available:</p> <ul> <li>If <code>centering</code> is <code>False</code>, the regional effect is not centered</li> <li>If <code>centering</code> is <code>True</code> or <code>zero_integral</code>, the regional effect is centered around the <code>y</code> axis.</li> <li>If <code>centering</code> is <code>zero_start</code>, the regional effect starts from <code>y=0</code>.</li> </ul> <code>False</code> <p>Returns:</p> Type Description <p>the mean effect <code>y</code>, if <code>heterogeneity=False</code> (default) or a tuple <code>(y, std)</code> otherwise</p> Source code in <code>/home/runner/work/effector/effector/effector/regional_effect.py</code> <pre><code>def eval(self, feature, node_idx, xs, heterogeneity=False, centering=False):\n\"\"\"\n    Evaluate the regional effect for a given feature and node.\n\n    Args:\n        feature: the feature to evaluate\n        node_idx: the node corresponding to the subregion to evaluate\n        xs: the points at which to evaluate the regional effect\n        heterogeneity: whether to return the heterogeneity.\n\n              - if `heterogeneity=False`, the function returns the mean effect at the given `xs`\n              - If `heterogeneity=True`, the function returns `(y, std)` where `y` is the mean effect and `std` is the standard deviation of the mean effect\n\n        centering: whether to center the regional effect. The following options are available:\n\n            - If `centering` is `False`, the regional effect is not centered\n            - If `centering` is `True` or `zero_integral`, the regional effect is centered around the `y` axis.\n            - If `centering` is `zero_start`, the regional effect starts from `y=0`.\n\n    Returns:\n        the mean effect `y`, if `heterogeneity=False` (default) or a tuple `(y, std)` otherwise\n\n    \"\"\"\n    self.refit(feature)\n    centering = helpers.prep_centering(centering)\n    data, data_effect, _ = self.get_node_info(feature, node_idx)\n    fe_method = self._create_fe_object(data, data_effect, None)\n    return fe_method.eval(feature, xs, heterogeneity, centering)\n</code></pre>"},{"location":"api/#effector.regional_effect.RegionalEffectBase.plot","title":"<code>plot(feature, node_idx, heterogeneity=False, centering=False, scale_x_list=None, scale_y=None, y_limits=None)</code>","text":"Source code in <code>/home/runner/work/effector/effector/effector/regional_effect.py</code> <pre><code>def plot(self,\n         feature,\n         node_idx,\n         heterogeneity=False,\n         centering=False,\n         scale_x_list=None,\n         scale_y=None,\n         y_limits=None):\n\n    self.refit(feature)\n\n    if scale_x_list is not None:\n        self.tree_full_scaled[\"feature_{}\".format(feature)] = self.partitioners[\"feature_{}\".format(feature)].splits_to_tree(False, scale_x_list)\n        self.tree_pruned_scaled[\"feature_{}\".format(feature)] = self.partitioners[\"feature_{}\".format(feature)].splits_to_tree(True, scale_x_list)\n\n    data, data_effect, name = self.get_node_info(feature, node_idx)\n    feature_names = copy.deepcopy(self.feature_names)\n    feature_names[feature] = name\n    fe_method = self._create_fe_object(data, data_effect, feature_names)\n\n    return fe_method.plot(\n        feature=feature,\n        heterogeneity=heterogeneity,\n        centering=centering,\n        scale_x=scale_x_list[feature] if scale_x_list is not None else None,\n        scale_y=scale_y,\n        y_limits=y_limits\n        )\n</code></pre>"},{"location":"api/#effector.regional_effect.RegionalEffectBase.get_node_info","title":"<code>get_node_info(feature, node_idx)</code>","text":"Source code in <code>/home/runner/work/effector/effector/effector/regional_effect.py</code> <pre><code>def get_node_info(self, feature, node_idx):\n    assert self.is_fitted[feature], \"Feature {} has not been fitted yet\".format(feature)\n    assert self.tree_pruned[\"feature_{}\".format(feature)] is not None, \"Feature {} has no splits\".format(feature)\n\n    if self.tree_pruned_scaled is not None and \"feature_{}\".format(feature) in self.tree_pruned_scaled.keys():\n        tree = self.tree_pruned_scaled[\"feature_{}\".format(feature)]\n    else:\n        tree = self.tree_pruned[\"feature_{}\".format(feature)]\n\n    # assert node id exists\n    assert node_idx in [node.idx for node in tree.nodes], \"Node {} does not exist\".format(node_idx)\n\n    # find the node\n    node = [node for node in tree.nodes if node.idx == node_idx][0]\n\n    # get data\n    data = node.data[\"data\"]\n    data_effect = node.data[\"data_effect\"]\n    name = node.name\n    return data, data_effect, name\n</code></pre>"},{"location":"api/#effector.regional_effect_ale.RegionalALE","title":"<code>effector.regional_effect_ale.RegionalALE</code>","text":"<p>         Bases: <code>RegionalEffectBase</code></p> Source code in <code>/home/runner/work/effector/effector/effector/regional_effect_ale.py</code> <pre><code>class RegionalALE(RegionalEffectBase):\n    def __init__(\n        self,\n        data: np.ndarray,\n        model: callable,\n        nof_instances: typing.Union[int, str] = \"all\",\n        axis_limits: typing.Union[None, np.ndarray] = None,\n        feature_types: typing.Union[list, None] = None,\n        cat_limit: typing.Union[int, None] = 10,\n        feature_names: typing.Union[list, None] = None,\n        target_name: typing.Union[str, None] = None,\n    ):\n\"\"\"\n        Regional RHALE constructor.\n\n        Args:\n            data: X matrix (N,D).\n            model: the black-box model (N,D) -&gt; (N, )\n            model_jac: the black-box model Jacobian (N,D) -&gt; (N,D)\n            axis_limits: axis limits for the FE plot [2, D] or None. If None, axis limits are computed from the data.\n            feature_types: list of feature types (categorical or numerical)\n            cat_limit: the minimum number of unique values for a feature to be considered categorical\n            feature_names: list of feature names\n        \"\"\"\n        super(RegionalALE, self).__init__(\n            \"ale\",\n            data,\n            model,\n            None,\n            None,\n            nof_instances,\n            axis_limits,\n            feature_types,\n            cat_limit,\n            feature_names,\n            target_name\n        )\n\n    def _create_heterogeneity_function(self, foi, binning_method, min_points, centering):\n        binning_method = prep_binning_method(binning_method)\n        isinstance(binning_method, binning_methods.Fixed)\n\n        def heter(data, instance_effects=None) -&gt; float:\n            if data.shape[0] &lt; min_points:\n                return BIG_M\n\n            ale = ALE(data, self.model, \"all\", None, instance_effects)\n            try:\n                ale.fit(features=foi, binning_method=binning_method, centering=centering)\n            except:\n                return BIG_M\n\n            # heterogeneity is the accumulated std at the end of the curve\n            axis_limits = helpers.axis_limits_from_data(data)\n            stop = np.array([axis_limits[:, foi][1]])\n            _, z = ale.eval(feature=foi, xs=stop, heterogeneity=True)\n            return z.item()\n\n        return heter\n\n    def fit(\n        self,\n        features: typing.Union[int, str, list],\n        heter_pcg_drop_thres: float = 0.1,\n        heter_small_enough: float = 0.1,\n        max_depth: int = 1,\n        nof_candidate_splits_for_numerical: int = 20,\n        min_points_per_subregion: int = 10,\n        candidate_conditioning_features: typing.Union[\"str\", list] = \"all\",\n        split_categorical_features: bool = False,\n        binning_method: typing.Union[str, binning_methods.Fixed] = binning_methods.Fixed(nof_bins=20, min_points_per_bin=0),\n        centering: typing.Union[bool, str] = False,\n    ):\n\"\"\"\n        Find the Regional RHALE for a list of features.\n\n        Args:\n            features: list of features to fit\n            heter_pcg_drop_thres: heterogeneity drop threshold for a split to be considered important\n            heter_small_enough: heterogeneity threshold for a region to be considered homogeneous (splitting stops)\n            binning_method: binning method to use\n            max_depth: maximum number of splits to perform (depth of the tree)\n            nof_candidate_splits_for_numerical: number of candidate splits to consider for numerical features\n            min_points_per_subregion: minimum allowed number of points in a subregion (otherwise the split is not considered as valid)\n            candidate_conditioning_features: list of features to consider as conditioning features for the candidate splits\n        \"\"\"\n\n        assert min_points_per_subregion &gt;= 2, \"min_points_per_subregion must be &gt;= 2\"\n        features = helpers.prep_features(features, self.dim)\n        for feat in tqdm(features):\n            heter = self._create_heterogeneity_function(\n                feat, binning_method, min_points_per_subregion, centering\n            )\n\n            self._fit_feature(\n                feat,\n                heter,\n                heter_pcg_drop_thres,\n                heter_small_enough,\n                max_depth,\n                nof_candidate_splits_for_numerical,\n                min_points_per_subregion,\n                candidate_conditioning_features,\n                split_categorical_features,\n            )\n\n            self.method_args[\"feature_\" + str(feat)] = {\n                \"heter_pcg_drop_thres\": heter_pcg_drop_thres,\n                \"heter_small_enough\": heter_small_enough,\n                \"max_depth\": max_depth,\n                \"nof_candidate_splits_for_numerical\": nof_candidate_splits_for_numerical,\n                \"min_points_per_subregion\": min_points_per_subregion,\n                \"candidate_conditioning_features\": candidate_conditioning_features,\n                \"split_categorical_features\": split_categorical_features,\n                \"binning_method\": binning_method,\n                \"centering\": centering,\n            }\n\n    def plot(self,\n             feature,\n             node_idx,\n             heterogeneity=False,\n             centering=False,\n             scale_x_list=None,\n             scale_y=None,\n             y_limits=None,\n             dy_limits=None):\n\n        # get data from the node\n        self.refit(feature)\n\n        if scale_x_list is not None:\n            self.tree_full_scaled[\"feature_{}\".format(feature)] = self.partitioners[\"feature_{}\".format(feature)].splits_to_tree(False, scale_x_list)\n            self.tree_pruned_scaled[\"feature_{}\".format(feature)] = self.partitioners[\"feature_{}\".format(feature)].splits_to_tree(True, scale_x_list)\n\n        data, data_effect, name = self.get_node_info(feature, node_idx)\n        feature_names = copy.deepcopy(self.feature_names)\n        feature_names[feature] = name\n\n        # define the method and fit\n        self.method_args[\"feature_\" + str(feature)][\"heterogeneity\"] = heterogeneity\n        rhale = RHALE(data, self.model, self.model_jac, self.nof_instances, None, data_effect, feature_names=feature_names)\n        binning_method = prep_binning_method(self.method_args[\"feature_\" + str(feature)][\"binning_method\"])\n        rhale.fit(features=feature, binning_method=binning_method, centering=centering)\n        scale_x = scale_x_list[feature] if scale_x_list is not None else None\n        rhale.plot(feature=feature, heterogeneity=heterogeneity, centering=centering, scale_x=scale_x, scale_y=scale_y, y_limits=y_limits, dy_limits=dy_limits)\n</code></pre>"},{"location":"api/#effector.regional_effect_ale.RegionalALE.__init__","title":"<code>__init__(data, model, nof_instances='all', axis_limits=None, feature_types=None, cat_limit=10, feature_names=None, target_name=None)</code>","text":"<p>Regional RHALE constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray</code> <p>X matrix (N,D).</p> required <code>model</code> <code>callable</code> <p>the black-box model (N,D) -&gt; (N, )</p> required <code>model_jac</code> <p>the black-box model Jacobian (N,D) -&gt; (N,D)</p> required <code>axis_limits</code> <code>typing.Union[None, np.ndarray]</code> <p>axis limits for the FE plot [2, D] or None. If None, axis limits are computed from the data.</p> <code>None</code> <code>feature_types</code> <code>typing.Union[list, None]</code> <p>list of feature types (categorical or numerical)</p> <code>None</code> <code>cat_limit</code> <code>typing.Union[int, None]</code> <p>the minimum number of unique values for a feature to be considered categorical</p> <code>10</code> <code>feature_names</code> <code>typing.Union[list, None]</code> <p>list of feature names</p> <code>None</code> Source code in <code>/home/runner/work/effector/effector/effector/regional_effect_ale.py</code> <pre><code>def __init__(\n    self,\n    data: np.ndarray,\n    model: callable,\n    nof_instances: typing.Union[int, str] = \"all\",\n    axis_limits: typing.Union[None, np.ndarray] = None,\n    feature_types: typing.Union[list, None] = None,\n    cat_limit: typing.Union[int, None] = 10,\n    feature_names: typing.Union[list, None] = None,\n    target_name: typing.Union[str, None] = None,\n):\n\"\"\"\n    Regional RHALE constructor.\n\n    Args:\n        data: X matrix (N,D).\n        model: the black-box model (N,D) -&gt; (N, )\n        model_jac: the black-box model Jacobian (N,D) -&gt; (N,D)\n        axis_limits: axis limits for the FE plot [2, D] or None. If None, axis limits are computed from the data.\n        feature_types: list of feature types (categorical or numerical)\n        cat_limit: the minimum number of unique values for a feature to be considered categorical\n        feature_names: list of feature names\n    \"\"\"\n    super(RegionalALE, self).__init__(\n        \"ale\",\n        data,\n        model,\n        None,\n        None,\n        nof_instances,\n        axis_limits,\n        feature_types,\n        cat_limit,\n        feature_names,\n        target_name\n    )\n</code></pre>"},{"location":"api/#effector.regional_effect_ale.RegionalALE.fit","title":"<code>fit(features, heter_pcg_drop_thres=0.1, heter_small_enough=0.1, max_depth=1, nof_candidate_splits_for_numerical=20, min_points_per_subregion=10, candidate_conditioning_features='all', split_categorical_features=False, binning_method=binning_methods.Fixed(nof_bins=20, min_points_per_bin=0), centering=False)</code>","text":"<p>Find the Regional RHALE for a list of features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>typing.Union[int, str, list]</code> <p>list of features to fit</p> required <code>heter_pcg_drop_thres</code> <code>float</code> <p>heterogeneity drop threshold for a split to be considered important</p> <code>0.1</code> <code>heter_small_enough</code> <code>float</code> <p>heterogeneity threshold for a region to be considered homogeneous (splitting stops)</p> <code>0.1</code> <code>binning_method</code> <code>typing.Union[str, binning_methods.Fixed]</code> <p>binning method to use</p> <code>binning_methods.Fixed(nof_bins=20, min_points_per_bin=0)</code> <code>max_depth</code> <code>int</code> <p>maximum number of splits to perform (depth of the tree)</p> <code>1</code> <code>nof_candidate_splits_for_numerical</code> <code>int</code> <p>number of candidate splits to consider for numerical features</p> <code>20</code> <code>min_points_per_subregion</code> <code>int</code> <p>minimum allowed number of points in a subregion (otherwise the split is not considered as valid)</p> <code>10</code> <code>candidate_conditioning_features</code> <code>typing.Union[str, list]</code> <p>list of features to consider as conditioning features for the candidate splits</p> <code>'all'</code> Source code in <code>/home/runner/work/effector/effector/effector/regional_effect_ale.py</code> <pre><code>def fit(\n    self,\n    features: typing.Union[int, str, list],\n    heter_pcg_drop_thres: float = 0.1,\n    heter_small_enough: float = 0.1,\n    max_depth: int = 1,\n    nof_candidate_splits_for_numerical: int = 20,\n    min_points_per_subregion: int = 10,\n    candidate_conditioning_features: typing.Union[\"str\", list] = \"all\",\n    split_categorical_features: bool = False,\n    binning_method: typing.Union[str, binning_methods.Fixed] = binning_methods.Fixed(nof_bins=20, min_points_per_bin=0),\n    centering: typing.Union[bool, str] = False,\n):\n\"\"\"\n    Find the Regional RHALE for a list of features.\n\n    Args:\n        features: list of features to fit\n        heter_pcg_drop_thres: heterogeneity drop threshold for a split to be considered important\n        heter_small_enough: heterogeneity threshold for a region to be considered homogeneous (splitting stops)\n        binning_method: binning method to use\n        max_depth: maximum number of splits to perform (depth of the tree)\n        nof_candidate_splits_for_numerical: number of candidate splits to consider for numerical features\n        min_points_per_subregion: minimum allowed number of points in a subregion (otherwise the split is not considered as valid)\n        candidate_conditioning_features: list of features to consider as conditioning features for the candidate splits\n    \"\"\"\n\n    assert min_points_per_subregion &gt;= 2, \"min_points_per_subregion must be &gt;= 2\"\n    features = helpers.prep_features(features, self.dim)\n    for feat in tqdm(features):\n        heter = self._create_heterogeneity_function(\n            feat, binning_method, min_points_per_subregion, centering\n        )\n\n        self._fit_feature(\n            feat,\n            heter,\n            heter_pcg_drop_thres,\n            heter_small_enough,\n            max_depth,\n            nof_candidate_splits_for_numerical,\n            min_points_per_subregion,\n            candidate_conditioning_features,\n            split_categorical_features,\n        )\n\n        self.method_args[\"feature_\" + str(feat)] = {\n            \"heter_pcg_drop_thres\": heter_pcg_drop_thres,\n            \"heter_small_enough\": heter_small_enough,\n            \"max_depth\": max_depth,\n            \"nof_candidate_splits_for_numerical\": nof_candidate_splits_for_numerical,\n            \"min_points_per_subregion\": min_points_per_subregion,\n            \"candidate_conditioning_features\": candidate_conditioning_features,\n            \"split_categorical_features\": split_categorical_features,\n            \"binning_method\": binning_method,\n            \"centering\": centering,\n        }\n</code></pre>"},{"location":"api/#effector.regional_effect_ale.RegionalRHALE","title":"<code>effector.regional_effect_ale.RegionalRHALE</code>","text":"<p>         Bases: <code>RegionalEffectBase</code></p> Source code in <code>/home/runner/work/effector/effector/effector/regional_effect_ale.py</code> <pre><code>class RegionalRHALE(RegionalEffectBase):\n    def __init__(\n        self,\n        data: np.ndarray,\n        model: Callable,\n        model_jac: Optional[Callable] = None,\n        instance_effects: Optional[np.ndarray] = None,\n        nof_instances: Union[int, str] = \"all\",\n        axis_limits: Optional[np.ndarray] = None,\n        feature_types: Optional[List] = None,\n        cat_limit: Optional[int] = 10,\n        feature_names: Optional[List] = None,\n        target_name: Optional[str] = None,\n    ):\n\"\"\"\n        Regional RHALE constructor.\n\n        Args:\n            data: X matrix (N,D).\n            model: the black-box model (N,D) -&gt; (N, )\n            model_jac: the black-box model Jacobian (N,D) -&gt; (N,D)\n            axis_limits: axis limits for the FE plot [2, D] or None. If None, axis limits are computed from the data.\n            feature_types: list of feature types (categorical or numerical)\n            cat_limit: the minimum number of unique values for a feature to be considered categorical\n            feature_names: list of feature names\n        \"\"\"\n\n        if instance_effects is None:\n            if model_jac is not None:\n                instance_effects = model_jac(data)\n            else:\n                instance_effects = utils.compute_jacobian_numerically(model, data)\n\n\n        super(RegionalRHALE, self).__init__(\n            \"rhale\",\n            data,\n            model,\n            model_jac,\n            instance_effects,\n            nof_instances,\n            axis_limits,\n            feature_types,\n            cat_limit,\n            feature_names,\n            target_name\n        )\n\n    def _create_heterogeneity_function(self, foi, binning_method, min_points, centering):\n        binning_method = prep_binning_method(binning_method)\n\n        def heter(data, instance_effects=None) -&gt; float:\n            if data.shape[0] &lt; min_points:\n                return BIG_M\n\n            rhale = RHALE(data, self.model, self.model_jac, \"all\", None, instance_effects)\n            try:\n                rhale.fit(features=foi, binning_method=binning_method, centering=centering)\n            except:\n                return BIG_M\n\n            # heterogeneity is the accumulated std at the end of the curve\n            axis_limits = helpers.axis_limits_from_data(data)\n            stop = np.array([axis_limits[:, foi][1]])\n            _, z = rhale.eval(feature=foi, xs=stop, heterogeneity=True)\n            return z.item()\n\n        return heter\n\n    def fit(\n        self,\n        features: typing.Union[int, str, list] = \"all\",\n        heter_pcg_drop_thres: float = 0.1,\n        heter_small_enough: float = 0.1,\n        max_depth: int = 1,\n        nof_candidate_splits_for_numerical: int = 20,\n        min_points_per_subregion: int = 10,\n        candidate_conditioning_features: typing.Union[\"str\", list] = \"all\",\n        split_categorical_features: bool = False,\n        binning_method: typing.Union[\n                str,\n                binning_methods.Fixed,\n                binning_methods.DynamicProgramming,\n                binning_methods.Greedy,\n        ] = \"greedy\",\n        centering: typing.Union[bool, str] = False,\n    ):\n\"\"\"\n        Find the Regional RHALE for a list of features.\n\n        Args:\n            features: list of features to fit\n            heter_pcg_drop_thres: heterogeneity drop threshold for a split to be considered important\n            heter_small_enough: heterogeneity threshold for a region to be considered homogeneous (splitting stops)\n            binning_method: binning method to use\n            max_depth: maximum number of splits to perform (depth of the tree)\n            nof_candidate_splits_for_numerical: number of candidate splits to consider for numerical features\n            min_points_per_subregion: minimum allowed number of points in a subregion (otherwise the split is not considered as valid)\n            candidate_conditioning_features: list of features to consider as conditioning features for the candidate splits\n        \"\"\"\n\n        assert min_points_per_subregion &gt;= 2, \"min_points_per_subregion must be &gt;= 2\"\n        features = helpers.prep_features(features, self.dim)\n        for feat in tqdm(features):\n            heter = self._create_heterogeneity_function(\n                feat, binning_method, min_points_per_subregion, centering\n            )\n\n            self._fit_feature(\n                feat,\n                heter,\n                heter_pcg_drop_thres,\n                heter_small_enough,\n                max_depth,\n                nof_candidate_splits_for_numerical,\n                min_points_per_subregion,\n                candidate_conditioning_features,\n                split_categorical_features,\n            )\n\n            self.method_args[\"feature_\" + str(feat)] = {\n                \"heter_pcg_drop_thres\": heter_pcg_drop_thres,\n                \"heter_small_enough\": heter_small_enough,\n                \"max_depth\": max_depth,\n                \"nof_candidate_splits_for_numerical\": nof_candidate_splits_for_numerical,\n                \"min_points_per_subregion\": min_points_per_subregion,\n                \"candidate_conditioning_features\": candidate_conditioning_features,\n                \"split_categorical_features\": split_categorical_features,\n                \"binning_method\": binning_method,\n            }\n\n    def plot(self,\n             feature,\n             node_idx,\n             heterogeneity=False,\n             centering=False,\n             scale_x_list=None,\n             scale_y=None,\n             y_limits=None,\n             dy_limits=None):\n\n        # get data from the node\n        self.refit(feature)\n\n        if scale_x_list is not None:\n            self.tree_full_scaled[\"feature_{}\".format(feature)] = self.partitioners[\"feature_{}\".format(feature)].splits_to_tree(False, scale_x_list)\n            self.tree_pruned_scaled[\"feature_{}\".format(feature)] = self.partitioners[\"feature_{}\".format(feature)].splits_to_tree(True, scale_x_list)\n\n        data, data_effect, name = self.get_node_info(feature, node_idx)\n        feature_names = copy.deepcopy(self.feature_names)\n        feature_names[feature] = name\n\n        # define the method and fit\n        self.method_args[\"feature_\" + str(feature)][\"heterogeneity\"] = heterogeneity\n        rhale = RHALE(data, self.model, self.model_jac, self.nof_instances, None, data_effect, feature_names=feature_names)\n        binning_method = prep_binning_method(self.method_args[\"feature_\" + str(feature)][\"binning_method\"])\n        rhale.fit(features=feature, binning_method=binning_method, centering=centering)\n        scale_x = scale_x_list[feature] if scale_x_list is not None else None\n        rhale.plot(feature=feature, heterogeneity=heterogeneity, centering=centering, scale_x=scale_x, scale_y=scale_y, y_limits=y_limits, dy_limits=dy_limits)\n</code></pre>"},{"location":"api/#effector.regional_effect_ale.RegionalRHALE.__init__","title":"<code>__init__(data, model, model_jac=None, instance_effects=None, nof_instances='all', axis_limits=None, feature_types=None, cat_limit=10, feature_names=None, target_name=None)</code>","text":"<p>Regional RHALE constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray</code> <p>X matrix (N,D).</p> required <code>model</code> <code>Callable</code> <p>the black-box model (N,D) -&gt; (N, )</p> required <code>model_jac</code> <code>Optional[Callable]</code> <p>the black-box model Jacobian (N,D) -&gt; (N,D)</p> <code>None</code> <code>axis_limits</code> <code>Optional[np.ndarray]</code> <p>axis limits for the FE plot [2, D] or None. If None, axis limits are computed from the data.</p> <code>None</code> <code>feature_types</code> <code>Optional[List]</code> <p>list of feature types (categorical or numerical)</p> <code>None</code> <code>cat_limit</code> <code>Optional[int]</code> <p>the minimum number of unique values for a feature to be considered categorical</p> <code>10</code> <code>feature_names</code> <code>Optional[List]</code> <p>list of feature names</p> <code>None</code> Source code in <code>/home/runner/work/effector/effector/effector/regional_effect_ale.py</code> <pre><code>def __init__(\n    self,\n    data: np.ndarray,\n    model: Callable,\n    model_jac: Optional[Callable] = None,\n    instance_effects: Optional[np.ndarray] = None,\n    nof_instances: Union[int, str] = \"all\",\n    axis_limits: Optional[np.ndarray] = None,\n    feature_types: Optional[List] = None,\n    cat_limit: Optional[int] = 10,\n    feature_names: Optional[List] = None,\n    target_name: Optional[str] = None,\n):\n\"\"\"\n    Regional RHALE constructor.\n\n    Args:\n        data: X matrix (N,D).\n        model: the black-box model (N,D) -&gt; (N, )\n        model_jac: the black-box model Jacobian (N,D) -&gt; (N,D)\n        axis_limits: axis limits for the FE plot [2, D] or None. If None, axis limits are computed from the data.\n        feature_types: list of feature types (categorical or numerical)\n        cat_limit: the minimum number of unique values for a feature to be considered categorical\n        feature_names: list of feature names\n    \"\"\"\n\n    if instance_effects is None:\n        if model_jac is not None:\n            instance_effects = model_jac(data)\n        else:\n            instance_effects = utils.compute_jacobian_numerically(model, data)\n\n\n    super(RegionalRHALE, self).__init__(\n        \"rhale\",\n        data,\n        model,\n        model_jac,\n        instance_effects,\n        nof_instances,\n        axis_limits,\n        feature_types,\n        cat_limit,\n        feature_names,\n        target_name\n    )\n</code></pre>"},{"location":"api/#effector.regional_effect_ale.RegionalRHALE.fit","title":"<code>fit(features='all', heter_pcg_drop_thres=0.1, heter_small_enough=0.1, max_depth=1, nof_candidate_splits_for_numerical=20, min_points_per_subregion=10, candidate_conditioning_features='all', split_categorical_features=False, binning_method='greedy', centering=False)</code>","text":"<p>Find the Regional RHALE for a list of features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>typing.Union[int, str, list]</code> <p>list of features to fit</p> <code>'all'</code> <code>heter_pcg_drop_thres</code> <code>float</code> <p>heterogeneity drop threshold for a split to be considered important</p> <code>0.1</code> <code>heter_small_enough</code> <code>float</code> <p>heterogeneity threshold for a region to be considered homogeneous (splitting stops)</p> <code>0.1</code> <code>binning_method</code> <code>typing.Union[str, binning_methods.Fixed, binning_methods.DynamicProgramming, binning_methods.Greedy]</code> <p>binning method to use</p> <code>'greedy'</code> <code>max_depth</code> <code>int</code> <p>maximum number of splits to perform (depth of the tree)</p> <code>1</code> <code>nof_candidate_splits_for_numerical</code> <code>int</code> <p>number of candidate splits to consider for numerical features</p> <code>20</code> <code>min_points_per_subregion</code> <code>int</code> <p>minimum allowed number of points in a subregion (otherwise the split is not considered as valid)</p> <code>10</code> <code>candidate_conditioning_features</code> <code>typing.Union[str, list]</code> <p>list of features to consider as conditioning features for the candidate splits</p> <code>'all'</code> Source code in <code>/home/runner/work/effector/effector/effector/regional_effect_ale.py</code> <pre><code>def fit(\n    self,\n    features: typing.Union[int, str, list] = \"all\",\n    heter_pcg_drop_thres: float = 0.1,\n    heter_small_enough: float = 0.1,\n    max_depth: int = 1,\n    nof_candidate_splits_for_numerical: int = 20,\n    min_points_per_subregion: int = 10,\n    candidate_conditioning_features: typing.Union[\"str\", list] = \"all\",\n    split_categorical_features: bool = False,\n    binning_method: typing.Union[\n            str,\n            binning_methods.Fixed,\n            binning_methods.DynamicProgramming,\n            binning_methods.Greedy,\n    ] = \"greedy\",\n    centering: typing.Union[bool, str] = False,\n):\n\"\"\"\n    Find the Regional RHALE for a list of features.\n\n    Args:\n        features: list of features to fit\n        heter_pcg_drop_thres: heterogeneity drop threshold for a split to be considered important\n        heter_small_enough: heterogeneity threshold for a region to be considered homogeneous (splitting stops)\n        binning_method: binning method to use\n        max_depth: maximum number of splits to perform (depth of the tree)\n        nof_candidate_splits_for_numerical: number of candidate splits to consider for numerical features\n        min_points_per_subregion: minimum allowed number of points in a subregion (otherwise the split is not considered as valid)\n        candidate_conditioning_features: list of features to consider as conditioning features for the candidate splits\n    \"\"\"\n\n    assert min_points_per_subregion &gt;= 2, \"min_points_per_subregion must be &gt;= 2\"\n    features = helpers.prep_features(features, self.dim)\n    for feat in tqdm(features):\n        heter = self._create_heterogeneity_function(\n            feat, binning_method, min_points_per_subregion, centering\n        )\n\n        self._fit_feature(\n            feat,\n            heter,\n            heter_pcg_drop_thres,\n            heter_small_enough,\n            max_depth,\n            nof_candidate_splits_for_numerical,\n            min_points_per_subregion,\n            candidate_conditioning_features,\n            split_categorical_features,\n        )\n\n        self.method_args[\"feature_\" + str(feat)] = {\n            \"heter_pcg_drop_thres\": heter_pcg_drop_thres,\n            \"heter_small_enough\": heter_small_enough,\n            \"max_depth\": max_depth,\n            \"nof_candidate_splits_for_numerical\": nof_candidate_splits_for_numerical,\n            \"min_points_per_subregion\": min_points_per_subregion,\n            \"candidate_conditioning_features\": candidate_conditioning_features,\n            \"split_categorical_features\": split_categorical_features,\n            \"binning_method\": binning_method,\n        }\n</code></pre>"},{"location":"api/#effector.regional_effect_pdp.RegionalPDPBase","title":"<code>effector.regional_effect_pdp.RegionalPDPBase</code>","text":"<p>         Bases: <code>RegionalEffectBase</code></p> Source code in <code>/home/runner/work/effector/effector/effector/regional_effect_pdp.py</code> <pre><code>class RegionalPDPBase(RegionalEffectBase):\n    def __init__(\n        self,\n        method_name: str,\n        data: np.ndarray,\n        model: callable,\n        model_jac: typing.Union[None, callable] = None,\n        nof_instances: typing.Union[int, str] = 100,\n        axis_limits: typing.Union[None, np.ndarray] = None,\n        feature_types: typing.Union[list, None] = None,\n        cat_limit: typing.Union[int, None] = 10,\n        feature_names: typing.Union[list, None] = None,\n        target_name: typing.Union[str, None] = None,\n    ):\n        super(RegionalPDPBase, self).__init__(\n            method_name,\n            data,\n            model,\n            model_jac,\n            None,\n            nof_instances,\n            axis_limits,\n            feature_types,\n            cat_limit,\n            feature_names,\n            target_name)\n\n    def _create_heterogeneity_function(self, foi, min_points, centering, nof_instances, points_for_centering):\n        def heter(data) -&gt; float:\n            if data.shape[0] &lt; min_points:\n                return BIG_M\n\n            if self.method_name == \"pdp\":\n                pdp = PDP(data, self.model, self.axis_limits, nof_instances=nof_instances)\n            else:\n                pdp = DerivativePDP(data, self.model, self.model_jac, self.axis_limits, nof_instances=nof_instances)\n\n            try:\n                pdp.fit(features=foi, centering=centering, points_for_centering=points_for_centering)\n            except:\n                return BIG_M\n\n            # heterogeneity is the mean heterogeneity over the curve\n            axis_limits = helpers.axis_limits_from_data(data)\n\n            xx = np.linspace(axis_limits[:, foi][0], axis_limits[:, foi][1], 10)\n            try:\n                _, z = pdp.eval(feature=foi, xs=xx, heterogeneity=True)\n            except:\n                return BIG_M\n            return np.mean(z)\n\n        return heter\n\n    def fit(\n        self,\n        features: typing.Union[int, str, list] = \"all\",\n        heter_pcg_drop_thres: float = 0.1,\n        heter_small_enough: float = 0.1,\n        max_depth: int = 1,\n        nof_candidate_splits_for_numerical: int = 20,\n        min_points_per_subregion: int = 10,\n        candidate_conditioning_features: typing.Union[\"str\", list] = \"all\",\n        split_categorical_features: bool = False,\n        centering: typing.Union[bool, str] = False,\n        nof_instances: int = \"all\",\n        points_for_centering: int = 100,\n    ):\n\"\"\"\n        Find the Regional PDP for a list of features.\n\n        Args:\n            features: list of features to fit\n            heter_pcg_drop_thres: heterogeneity drop threshold for a split to be considered important\n            heter_small_enough: heterogeneity threshold for a region to be considered homogeneous (splitting stops)\n            max_depth: maximum number of splits to perform (depth of the tree)\n            nof_candidate_splits_for_numerical: number of candidate splits to consider for numerical features\n            min_points_per_subregion: minimum allowed number of points in a subregion (otherwise the split is not considered as valid)\n            candidate_conditioning_features: list of features to consider as conditioning features for the candidate splits\n            split_categorical_features\n        \"\"\"\n\n        assert min_points_per_subregion &gt;= 2, \"min_points_per_subregion must be &gt;= 2\"\n        features = helpers.prep_features(features, self.dim)\n        for feat in tqdm(features):\n            heter = self._create_heterogeneity_function(feat, min_points_per_subregion, centering, nof_instances, points_for_centering)\n\n            self._fit_feature(\n                feat,\n                heter,\n                heter_pcg_drop_thres,\n                heter_small_enough,\n                max_depth,\n                nof_candidate_splits_for_numerical,\n                min_points_per_subregion,\n                candidate_conditioning_features,\n                split_categorical_features,\n            )\n</code></pre>"},{"location":"api/#effector.regional_effect_pdp.RegionalPDPBase.fit","title":"<code>fit(features='all', heter_pcg_drop_thres=0.1, heter_small_enough=0.1, max_depth=1, nof_candidate_splits_for_numerical=20, min_points_per_subregion=10, candidate_conditioning_features='all', split_categorical_features=False, centering=False, nof_instances='all', points_for_centering=100)</code>","text":"<p>Find the Regional PDP for a list of features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>typing.Union[int, str, list]</code> <p>list of features to fit</p> <code>'all'</code> <code>heter_pcg_drop_thres</code> <code>float</code> <p>heterogeneity drop threshold for a split to be considered important</p> <code>0.1</code> <code>heter_small_enough</code> <code>float</code> <p>heterogeneity threshold for a region to be considered homogeneous (splitting stops)</p> <code>0.1</code> <code>max_depth</code> <code>int</code> <p>maximum number of splits to perform (depth of the tree)</p> <code>1</code> <code>nof_candidate_splits_for_numerical</code> <code>int</code> <p>number of candidate splits to consider for numerical features</p> <code>20</code> <code>min_points_per_subregion</code> <code>int</code> <p>minimum allowed number of points in a subregion (otherwise the split is not considered as valid)</p> <code>10</code> <code>candidate_conditioning_features</code> <code>typing.Union[str, list]</code> <p>list of features to consider as conditioning features for the candidate splits</p> <code>'all'</code> Source code in <code>/home/runner/work/effector/effector/effector/regional_effect_pdp.py</code> <pre><code>def fit(\n    self,\n    features: typing.Union[int, str, list] = \"all\",\n    heter_pcg_drop_thres: float = 0.1,\n    heter_small_enough: float = 0.1,\n    max_depth: int = 1,\n    nof_candidate_splits_for_numerical: int = 20,\n    min_points_per_subregion: int = 10,\n    candidate_conditioning_features: typing.Union[\"str\", list] = \"all\",\n    split_categorical_features: bool = False,\n    centering: typing.Union[bool, str] = False,\n    nof_instances: int = \"all\",\n    points_for_centering: int = 100,\n):\n\"\"\"\n    Find the Regional PDP for a list of features.\n\n    Args:\n        features: list of features to fit\n        heter_pcg_drop_thres: heterogeneity drop threshold for a split to be considered important\n        heter_small_enough: heterogeneity threshold for a region to be considered homogeneous (splitting stops)\n        max_depth: maximum number of splits to perform (depth of the tree)\n        nof_candidate_splits_for_numerical: number of candidate splits to consider for numerical features\n        min_points_per_subregion: minimum allowed number of points in a subregion (otherwise the split is not considered as valid)\n        candidate_conditioning_features: list of features to consider as conditioning features for the candidate splits\n        split_categorical_features\n    \"\"\"\n\n    assert min_points_per_subregion &gt;= 2, \"min_points_per_subregion must be &gt;= 2\"\n    features = helpers.prep_features(features, self.dim)\n    for feat in tqdm(features):\n        heter = self._create_heterogeneity_function(feat, min_points_per_subregion, centering, nof_instances, points_for_centering)\n\n        self._fit_feature(\n            feat,\n            heter,\n            heter_pcg_drop_thres,\n            heter_small_enough,\n            max_depth,\n            nof_candidate_splits_for_numerical,\n            min_points_per_subregion,\n            candidate_conditioning_features,\n            split_categorical_features,\n        )\n</code></pre>"},{"location":"api/#effector.regional_effect_pdp.RegionalPDP","title":"<code>effector.regional_effect_pdp.RegionalPDP</code>","text":"<p>         Bases: <code>RegionalPDPBase</code></p> Source code in <code>/home/runner/work/effector/effector/effector/regional_effect_pdp.py</code> <pre><code>class RegionalPDP(RegionalPDPBase):\n    def __init__(\n        self,\n        data: np.ndarray,\n        model: callable,\n        nof_instances: typing.Union[int, str] = 1000,\n        axis_limits: typing.Union[None, np.ndarray] = None,\n        feature_types: typing.Union[list, None] = None,\n        cat_limit: typing.Union[int, None] = 10,\n        feature_names: typing.Union[list, None] = None,\n        target_name: typing.Union[str, None] = None,\n    ):\n        super(RegionalPDP, self).__init__(\n            \"pdp\",\n            data,\n            model,\n            None,\n            nof_instances,\n            axis_limits,\n            feature_types,\n            cat_limit,\n            feature_names,\n            target_name)\n</code></pre>"},{"location":"api/#effector.regional_effect_pdp.RegionalPDP.__init__","title":"<code>__init__(data, model, nof_instances=1000, axis_limits=None, feature_types=None, cat_limit=10, feature_names=None, target_name=None)</code>","text":"Source code in <code>/home/runner/work/effector/effector/effector/regional_effect_pdp.py</code> <pre><code>def __init__(\n    self,\n    data: np.ndarray,\n    model: callable,\n    nof_instances: typing.Union[int, str] = 1000,\n    axis_limits: typing.Union[None, np.ndarray] = None,\n    feature_types: typing.Union[list, None] = None,\n    cat_limit: typing.Union[int, None] = 10,\n    feature_names: typing.Union[list, None] = None,\n    target_name: typing.Union[str, None] = None,\n):\n    super(RegionalPDP, self).__init__(\n        \"pdp\",\n        data,\n        model,\n        None,\n        nof_instances,\n        axis_limits,\n        feature_types,\n        cat_limit,\n        feature_names,\n        target_name)\n</code></pre>"},{"location":"api/#effector.regional_effect_pdp.RegionalDerivativePDP","title":"<code>effector.regional_effect_pdp.RegionalDerivativePDP</code>","text":"<p>         Bases: <code>RegionalPDPBase</code></p> Source code in <code>/home/runner/work/effector/effector/effector/regional_effect_pdp.py</code> <pre><code>class RegionalDerivativePDP(RegionalPDPBase):\n    def __init__(\n        self,\n        data: np.ndarray,\n        model: callable,\n        model_jac: typing.Union[None, callable] = None,\n        nof_instances: typing.Union[int, str] = 1000,\n        axis_limits: typing.Union[None, np.ndarray] = None,\n        feature_types: typing.Union[list, None] = None,\n        cat_limit: typing.Union[int, None] = 10,\n        feature_names: typing.Union[list, None] = None,\n        target_name: typing.Union[str, None] = None,\n    ):\n        super(RegionalDerivativePDP, self).__init__(\n            \"d-pdp\",\n            data,\n            model,\n            model_jac,\n            nof_instances,\n            axis_limits,\n            feature_types,\n            cat_limit,\n            feature_names,\n            target_name)\n</code></pre>"},{"location":"api/#effector.regional_effect_pdp.RegionalDerivativePDP.__init__","title":"<code>__init__(data, model, model_jac=None, nof_instances=1000, axis_limits=None, feature_types=None, cat_limit=10, feature_names=None, target_name=None)</code>","text":"Source code in <code>/home/runner/work/effector/effector/effector/regional_effect_pdp.py</code> <pre><code>def __init__(\n    self,\n    data: np.ndarray,\n    model: callable,\n    model_jac: typing.Union[None, callable] = None,\n    nof_instances: typing.Union[int, str] = 1000,\n    axis_limits: typing.Union[None, np.ndarray] = None,\n    feature_types: typing.Union[list, None] = None,\n    cat_limit: typing.Union[int, None] = 10,\n    feature_names: typing.Union[list, None] = None,\n    target_name: typing.Union[str, None] = None,\n):\n    super(RegionalDerivativePDP, self).__init__(\n        \"d-pdp\",\n        data,\n        model,\n        model_jac,\n        nof_instances,\n        axis_limits,\n        feature_types,\n        cat_limit,\n        feature_names,\n        target_name)\n</code></pre>"},{"location":"api/#effector.regional_effect_shap.RegionalSHAP","title":"<code>effector.regional_effect_shap.RegionalSHAP</code>","text":"<p>         Bases: <code>RegionalEffectBase</code></p> Source code in <code>/home/runner/work/effector/effector/effector/regional_effect_shap.py</code> <pre><code>class RegionalSHAP(RegionalEffectBase):\n    big_m = helpers.BIG_M\n\n    def __init__(\n        self,\n        data: np.ndarray,\n        model: Callable,\n        axis_limits: Optional[np.ndarray] = None,\n        nof_instances: Union[int, str] = 100,\n        feature_types: Optional[List[str]] = None,\n        cat_limit: Optional[int] = 10,\n        feature_names: Optional[List[str]] = None,\n        target_name: Optional[str] = None,\n    ):\n\"\"\"\n        Regional SHAP constructor.\n\n        Args:\n            data: the design matrix\n\n                - shape: `(N,D)`\n            model: the black-box model. Must be a `Callable` with:\n\n                - input: `ndarray` of shape `(N, D)`\n                - output: `ndarray` of shape `(N, )`\n\n            axis_limits: The limits of the feature effect plot along each axis\n\n                - use a `ndarray` of shape `(2, D)`, to specify them manually\n                - use `None`, to be inferred from the data\n\n            nof_instances: maximum number of instances to be used for PDP.\n\n                - use \"all\", for using all instances.\n                - use an `int`, for using `nof_instances` instances.\n\n            feature_types: The feature types.\n\n                - use `None`, to infer them from the data; whether a feature is categorical or numerical is inferred\n                from whether it exceeds the `cat_limit` unique values.\n                - use a list with elements `\"cat\"` or `\"numerical\"`, to specify them manually.\n\n            cat_limit: the minimum number of unique values for a feature to be considered categorical\n\n            feature_names: The names of the features\n\n                - use a `list` of `str`, to specify the name manually. For example: `                  [\"age\", \"weight\", ...]`\n                - use `None`, to keep the default names: `[\"x_0\", \"x_1\", ...]`\n\n            target_name: The name of the target variable\n\n                - use a `str`, to specify it name manually. For example: `\"price\"`\n                - use `None`, to keep the default name: `\"y\"`\n        \"\"\"\n        super(RegionalSHAP, self).__init__(\n            \"shap\",\n            data,\n            model,\n            None,\n            None,\n            nof_instances,\n            axis_limits,\n            feature_types,\n            cat_limit,\n            feature_names,\n            target_name\n        )\n\n    def _create_heterogeneity_function(self, foi, min_points, centering, points_for_centering):\n\n        def heterogeneity_function(data) -&gt; float:\n            if data.shape[0] &lt; min_points:\n                return self.big_m\n\n            axis_limits = helpers.axis_limits_from_data(data)\n            xx = np.linspace(axis_limits[:, foi][0], axis_limits[:, foi][1], 10)\n\n            shap = SHAPDependence(data, self.model, None, self.nof_instances)\n            shap.fit(foi, centering, points_for_centering)\n            _, z = shap.eval(foi, xx, heterogeneity=True)\n            return np.mean(z)\n\n        return heterogeneity_function\n\n    def fit(\n            self,\n            features: typing.Union[int, str, list],\n            heter_pcg_drop_thres: float = 0.1,\n            heter_small_enough: float = 0.1,\n            max_depth: int = 1,\n            nof_candidate_splits_for_numerical: int = 20,\n            min_points_per_subregion: int = 10,\n            candidate_conditioning_features: typing.Union[\"str\", list] = \"all\",\n            split_categorical_features: bool = False,\n            centering: typing.Union[bool, str] = False,\n            points_for_centering: int = 100,\n    ):\n\"\"\"\n        Fit the regional SHAP.\n\n        Args:\n            features: the features to fit.\n                - If set to \"all\", all the features will be fitted.\n\n            heter_pcg_drop_thres: threshold for the percentage drop in heterogeneity to consider a split valid\n            heter_small_enough: heterogeneity threshold for a region to be considered homogeneous (splitting stops)\n            max_depth: maximum number of splits to perform (depth of the tree)\n            nof_candidate_splits_for_numerical: number of candidate splits to consider for numerical features\n            min_points_per_subregion: minimum allowed number of points in a subregion (otherwise the split is not considered as valid)\n            candidate_conditioning_features: list of features to consider as conditioning features for the candidate splits\n            split_categorical_features: whether to search for subregions in categorical features\n            centering: whether to center the SHAP dependence plots before estimating the heterogeneity\n            points_for_centering: number of points to use for centering\n        \"\"\"\n        assert min_points_per_subregion &gt;= 2, \"min_points_per_subregion must be &gt;= 2\"\n        features = helpers.prep_features(features, self.dim)\n        for feat in tqdm(features):\n            heter = self._create_heterogeneity_function(\n                feat, min_points_per_subregion, centering, points_for_centering\n            )\n\n            self._fit_feature(\n                feat,\n                heter,\n                heter_pcg_drop_thres,\n                heter_small_enough,\n                max_depth,\n                nof_candidate_splits_for_numerical,\n                min_points_per_subregion,\n                candidate_conditioning_features,\n                split_categorical_features,\n            )\n\n            self.method_args[\"feature_\" + str(feat)] = {\n                \"heter_pcg_drop_thres\": heter_pcg_drop_thres,\n                \"heter_small_enough\": heter_small_enough,\n                \"max_depth\": max_depth,\n                \"nof_candidate_splits_for_numerical\": nof_candidate_splits_for_numerical,\n                \"min_points_per_subregion\": min_points_per_subregion,\n                \"candidate_conditioning_features\": candidate_conditioning_features,\n                \"split_categorical_features\": split_categorical_features,\n                \"centering\": centering,\n                \"points_for_centering\": points_for_centering,\n            }\n</code></pre>"},{"location":"api/#effector.regional_effect_shap.RegionalSHAP.__init__","title":"<code>__init__(data, model, axis_limits=None, nof_instances=100, feature_types=None, cat_limit=10, feature_names=None, target_name=None)</code>","text":"<p>Regional SHAP constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray</code> <p>the design matrix</p> <ul> <li>shape: <code>(N,D)</code></li> </ul> required <code>model</code> <code>Callable</code> <p>the black-box model. Must be a <code>Callable</code> with:</p> <ul> <li>input: <code>ndarray</code> of shape <code>(N, D)</code></li> <li>output: <code>ndarray</code> of shape <code>(N, )</code></li> </ul> required <code>axis_limits</code> <code>Optional[np.ndarray]</code> <p>The limits of the feature effect plot along each axis</p> <ul> <li>use a <code>ndarray</code> of shape <code>(2, D)</code>, to specify them manually</li> <li>use <code>None</code>, to be inferred from the data</li> </ul> <code>None</code> <code>nof_instances</code> <code>Union[int, str]</code> <p>maximum number of instances to be used for PDP.</p> <ul> <li>use \"all\", for using all instances.</li> <li>use an <code>int</code>, for using <code>nof_instances</code> instances.</li> </ul> <code>100</code> <code>feature_types</code> <code>Optional[List[str]]</code> <p>The feature types.</p> <ul> <li>use <code>None</code>, to infer them from the data; whether a feature is categorical or numerical is inferred from whether it exceeds the <code>cat_limit</code> unique values.</li> <li>use a list with elements <code>\"cat\"</code> or <code>\"numerical\"</code>, to specify them manually.</li> </ul> <code>None</code> <code>cat_limit</code> <code>Optional[int]</code> <p>the minimum number of unique values for a feature to be considered categorical</p> <code>10</code> <code>feature_names</code> <code>Optional[List[str]]</code> <p>The names of the features</p> <ul> <li>use a <code>list</code> of <code>str</code>, to specify the name manually. For example: <code>[\"age\", \"weight\", ...]</code></li> <li>use <code>None</code>, to keep the default names: <code>[\"x_0\", \"x_1\", ...]</code></li> </ul> <code>None</code> <code>target_name</code> <code>Optional[str]</code> <p>The name of the target variable</p> <ul> <li>use a <code>str</code>, to specify it name manually. For example: <code>\"price\"</code></li> <li>use <code>None</code>, to keep the default name: <code>\"y\"</code></li> </ul> <code>None</code> Source code in <code>/home/runner/work/effector/effector/effector/regional_effect_shap.py</code> <pre><code>def __init__(\n    self,\n    data: np.ndarray,\n    model: Callable,\n    axis_limits: Optional[np.ndarray] = None,\n    nof_instances: Union[int, str] = 100,\n    feature_types: Optional[List[str]] = None,\n    cat_limit: Optional[int] = 10,\n    feature_names: Optional[List[str]] = None,\n    target_name: Optional[str] = None,\n):\n\"\"\"\n    Regional SHAP constructor.\n\n    Args:\n        data: the design matrix\n\n            - shape: `(N,D)`\n        model: the black-box model. Must be a `Callable` with:\n\n            - input: `ndarray` of shape `(N, D)`\n            - output: `ndarray` of shape `(N, )`\n\n        axis_limits: The limits of the feature effect plot along each axis\n\n            - use a `ndarray` of shape `(2, D)`, to specify them manually\n            - use `None`, to be inferred from the data\n\n        nof_instances: maximum number of instances to be used for PDP.\n\n            - use \"all\", for using all instances.\n            - use an `int`, for using `nof_instances` instances.\n\n        feature_types: The feature types.\n\n            - use `None`, to infer them from the data; whether a feature is categorical or numerical is inferred\n            from whether it exceeds the `cat_limit` unique values.\n            - use a list with elements `\"cat\"` or `\"numerical\"`, to specify them manually.\n\n        cat_limit: the minimum number of unique values for a feature to be considered categorical\n\n        feature_names: The names of the features\n\n            - use a `list` of `str`, to specify the name manually. For example: `                  [\"age\", \"weight\", ...]`\n            - use `None`, to keep the default names: `[\"x_0\", \"x_1\", ...]`\n\n        target_name: The name of the target variable\n\n            - use a `str`, to specify it name manually. For example: `\"price\"`\n            - use `None`, to keep the default name: `\"y\"`\n    \"\"\"\n    super(RegionalSHAP, self).__init__(\n        \"shap\",\n        data,\n        model,\n        None,\n        None,\n        nof_instances,\n        axis_limits,\n        feature_types,\n        cat_limit,\n        feature_names,\n        target_name\n    )\n</code></pre>"},{"location":"api/#effector.regional_effect_shap.RegionalSHAP.fit","title":"<code>fit(features, heter_pcg_drop_thres=0.1, heter_small_enough=0.1, max_depth=1, nof_candidate_splits_for_numerical=20, min_points_per_subregion=10, candidate_conditioning_features='all', split_categorical_features=False, centering=False, points_for_centering=100)</code>","text":"<p>Fit the regional SHAP.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>typing.Union[int, str, list]</code> <p>the features to fit. - If set to \"all\", all the features will be fitted.</p> required <code>heter_pcg_drop_thres</code> <code>float</code> <p>threshold for the percentage drop in heterogeneity to consider a split valid</p> <code>0.1</code> <code>heter_small_enough</code> <code>float</code> <p>heterogeneity threshold for a region to be considered homogeneous (splitting stops)</p> <code>0.1</code> <code>max_depth</code> <code>int</code> <p>maximum number of splits to perform (depth of the tree)</p> <code>1</code> <code>nof_candidate_splits_for_numerical</code> <code>int</code> <p>number of candidate splits to consider for numerical features</p> <code>20</code> <code>min_points_per_subregion</code> <code>int</code> <p>minimum allowed number of points in a subregion (otherwise the split is not considered as valid)</p> <code>10</code> <code>candidate_conditioning_features</code> <code>typing.Union[str, list]</code> <p>list of features to consider as conditioning features for the candidate splits</p> <code>'all'</code> <code>split_categorical_features</code> <code>bool</code> <p>whether to search for subregions in categorical features</p> <code>False</code> <code>centering</code> <code>typing.Union[bool, str]</code> <p>whether to center the SHAP dependence plots before estimating the heterogeneity</p> <code>False</code> <code>points_for_centering</code> <code>int</code> <p>number of points to use for centering</p> <code>100</code> Source code in <code>/home/runner/work/effector/effector/effector/regional_effect_shap.py</code> <pre><code>def fit(\n        self,\n        features: typing.Union[int, str, list],\n        heter_pcg_drop_thres: float = 0.1,\n        heter_small_enough: float = 0.1,\n        max_depth: int = 1,\n        nof_candidate_splits_for_numerical: int = 20,\n        min_points_per_subregion: int = 10,\n        candidate_conditioning_features: typing.Union[\"str\", list] = \"all\",\n        split_categorical_features: bool = False,\n        centering: typing.Union[bool, str] = False,\n        points_for_centering: int = 100,\n):\n\"\"\"\n    Fit the regional SHAP.\n\n    Args:\n        features: the features to fit.\n            - If set to \"all\", all the features will be fitted.\n\n        heter_pcg_drop_thres: threshold for the percentage drop in heterogeneity to consider a split valid\n        heter_small_enough: heterogeneity threshold for a region to be considered homogeneous (splitting stops)\n        max_depth: maximum number of splits to perform (depth of the tree)\n        nof_candidate_splits_for_numerical: number of candidate splits to consider for numerical features\n        min_points_per_subregion: minimum allowed number of points in a subregion (otherwise the split is not considered as valid)\n        candidate_conditioning_features: list of features to consider as conditioning features for the candidate splits\n        split_categorical_features: whether to search for subregions in categorical features\n        centering: whether to center the SHAP dependence plots before estimating the heterogeneity\n        points_for_centering: number of points to use for centering\n    \"\"\"\n    assert min_points_per_subregion &gt;= 2, \"min_points_per_subregion must be &gt;= 2\"\n    features = helpers.prep_features(features, self.dim)\n    for feat in tqdm(features):\n        heter = self._create_heterogeneity_function(\n            feat, min_points_per_subregion, centering, points_for_centering\n        )\n\n        self._fit_feature(\n            feat,\n            heter,\n            heter_pcg_drop_thres,\n            heter_small_enough,\n            max_depth,\n            nof_candidate_splits_for_numerical,\n            min_points_per_subregion,\n            candidate_conditioning_features,\n            split_categorical_features,\n        )\n\n        self.method_args[\"feature_\" + str(feat)] = {\n            \"heter_pcg_drop_thres\": heter_pcg_drop_thres,\n            \"heter_small_enough\": heter_small_enough,\n            \"max_depth\": max_depth,\n            \"nof_candidate_splits_for_numerical\": nof_candidate_splits_for_numerical,\n            \"min_points_per_subregion\": min_points_per_subregion,\n            \"candidate_conditioning_features\": candidate_conditioning_features,\n            \"split_categorical_features\": split_categorical_features,\n            \"centering\": centering,\n            \"points_for_centering\": points_for_centering,\n        }\n</code></pre>"},{"location":"api/#binning-methods","title":"Binning Methods","text":""},{"location":"api/#effector.binning_methods.Fixed","title":"<code>effector.binning_methods.Fixed</code>","text":"Source code in <code>/home/runner/work/effector/effector/effector/binning_methods.py</code> <pre><code>class Fixed:\n    def __init__(self,\n                 nof_bins: int = 100,\n                 min_points_per_bin=10,\n                 cat_limit: int = 15\n                 ):\n        self.nof_bins = nof_bins\n        self.min_points_per_bin = min_points_per_bin\n        self.cat_limit = cat_limit\n</code></pre>"},{"location":"api/#effector.binning_methods.Fixed.__init__","title":"<code>__init__(nof_bins=100, min_points_per_bin=10, cat_limit=15)</code>","text":"Source code in <code>/home/runner/work/effector/effector/effector/binning_methods.py</code> <pre><code>def __init__(self,\n             nof_bins: int = 100,\n             min_points_per_bin=10,\n             cat_limit: int = 15\n             ):\n    self.nof_bins = nof_bins\n    self.min_points_per_bin = min_points_per_bin\n    self.cat_limit = cat_limit\n</code></pre>"},{"location":"api/#effector.binning_methods.Greedy","title":"<code>effector.binning_methods.Greedy</code>","text":"Source code in <code>/home/runner/work/effector/effector/effector/binning_methods.py</code> <pre><code>class Greedy:\n    def __init__(self,\n                 init_nof_bins: int = 100,\n                 min_points_per_bin: int = 10,\n                 discount: float = 0.3,\n                 cat_limit: int = 15\n                 ):\n        self.max_nof_bins = init_nof_bins\n        self.min_points_per_bin = min_points_per_bin\n        self.discount = discount\n        self.cat_limit = cat_limit\n</code></pre>"},{"location":"api/#effector.binning_methods.Greedy.__init__","title":"<code>__init__(init_nof_bins=100, min_points_per_bin=10, discount=0.3, cat_limit=15)</code>","text":"Source code in <code>/home/runner/work/effector/effector/effector/binning_methods.py</code> <pre><code>def __init__(self,\n             init_nof_bins: int = 100,\n             min_points_per_bin: int = 10,\n             discount: float = 0.3,\n             cat_limit: int = 15\n             ):\n    self.max_nof_bins = init_nof_bins\n    self.min_points_per_bin = min_points_per_bin\n    self.discount = discount\n    self.cat_limit = cat_limit\n</code></pre>"},{"location":"api/#effector.binning_methods.DynamicProgramming","title":"<code>effector.binning_methods.DynamicProgramming</code>","text":"Source code in <code>/home/runner/work/effector/effector/effector/binning_methods.py</code> <pre><code>class DynamicProgramming:\n    def __init__(self,\n                 max_nof_bins: int = 20,\n                 min_points_per_bin: int = 10,\n                 discount: float = 0.3,\n                 cat_limit: int = 15):\n        self.max_nof_bins = max_nof_bins\n        self.min_points_per_bin = min_points_per_bin\n        self.discount = discount\n        self.cat_limit = cat_limit\n</code></pre>"},{"location":"api/#effector.binning_methods.DynamicProgramming.__init__","title":"<code>__init__(max_nof_bins=20, min_points_per_bin=10, discount=0.3, cat_limit=15)</code>","text":"Source code in <code>/home/runner/work/effector/effector/effector/binning_methods.py</code> <pre><code>def __init__(self,\n             max_nof_bins: int = 20,\n             min_points_per_bin: int = 10,\n             discount: float = 0.3,\n             cat_limit: int = 15):\n    self.max_nof_bins = max_nof_bins\n    self.min_points_per_bin = min_points_per_bin\n    self.discount = discount\n    self.cat_limit = cat_limit\n</code></pre>"},{"location":"api/#utils","title":"Utils","text":""},{"location":"api/#effector.utils.compute_accumulated_effect","title":"<code>compute_accumulated_effect(x, limits, bin_effect, dx, square=False)</code>","text":"<p>Compute the accumulated effect at each point <code>x</code>.</p> Notes <p>The function implements the following formula:</p> \\[ \\mathtt{dx}[i] = \\mathtt{limits}[i+1] - \\mathtt{limits}[i] \\] \\[ \\mathtt{full\\_bin\\_acc} = \\sum_{i=0}^{k_x - 1} \\mathtt{dx}[i] * \\mathtt{bin\\_effect}[i] \\] \\[ \\mathtt{remainder} = (x - \\mathtt{limits}[k_x-1])* \\mathtt{bin\\_effect}[k_x] \\] \\[ f(x) =  \\mathtt{full\\_bin\\_acc} + \\mathtt{remainder} \\] Notes <p>if <code>square=True</code>, then the formula is: $$ \\mathtt{full_bin_acc} = \\sum_{i=0}^{k_x - 1} \\mathtt{dx}^2[i] * \\mathtt{bin_effect}[i] $$</p> \\[ \\mathtt{remainder} = (x - \\mathtt{limits}[k_x-1])^2* \\mathtt{bin\\_effect}[k_x] \\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; x = np.array([-1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0])\n&gt;&gt;&gt; limits = np.array([0, 1.5, 2.0])\n&gt;&gt;&gt; bin_effect = np.array([1.0, -1.0])\n&gt;&gt;&gt; dx = np.array([1.5, 0.5])\n&gt;&gt;&gt; compute_accumulated_effect(x, limits, bin_effect, dx)\narray([0. , 0. , 0. , 0.5, 1. , 1.5, 1. , 1. , 1. ])\n</code></pre> <pre><code>&gt;&gt;&gt; x = np.array([-1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0])\n&gt;&gt;&gt; limits = np.array([0, 1.5, 2.0])\n&gt;&gt;&gt; bin_effect = np.array([1.0, 1.0])\n&gt;&gt;&gt; dx = np.array([1.5, 0.5])\n&gt;&gt;&gt; compute_accumulated_effect(x, limits, bin_effect, dx)\narray([0. , 0. , 0. , 0.5, 1. , 1.5, 2. , 2. , 2. ])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray</code> <p>The points we want to evaluate at, (T)</p> required <code>limits</code> <code>np.ndarray</code> <p>The bin limits, (K+1)</p> required <code>bin_effect</code> <code>np.ndarray</code> <p>The effect in each bin, (K)</p> required <code>dx</code> <code>np.ndarray</code> <p>The bin-widths, (K)</p> required <code>square</code> <code>bool</code> <p>Whether to square the width. If true, the effect is bin_effect * dx^2, otherwise bin_effect * dx</p> <code>False</code> <p>Returns:</p> Name Type Description <code>y</code> <code>np.ndarray</code> <p>The accumulated effect at each point, (T)</p> Source code in <code>/home/runner/work/effector/effector/effector/utils.py</code> <pre><code>def compute_accumulated_effect(\n    x: np.ndarray,\n    limits: np.ndarray,\n    bin_effect: np.ndarray,\n    dx: np.ndarray,\n    square: bool = False,\n) -&gt; np.ndarray:\n\"\"\"Compute the accumulated effect at each point `x`.\n\n    Notes:\n        The function implements the following formula:\n\n        $$\n        \\mathtt{dx}[i] = \\mathtt{limits}[i+1] - \\mathtt{limits}[i]\n        $$\n\n        $$\n        \\mathtt{full\\_bin\\_acc} = \\sum_{i=0}^{k_x - 1} \\mathtt{dx}[i] * \\mathtt{bin\\_effect}[i]\n        $$\n\n        $$\n        \\mathtt{remainder} = (x - \\mathtt{limits}[k_x-1])* \\mathtt{bin\\_effect}[k_x]\n        $$\n\n        $$\n        f(x) =  \\mathtt{full\\_bin\\_acc} + \\mathtt{remainder}\n        $$\n\n    Notes:\n        if `square=True`, then the formula is:\n        $$\n        \\mathtt{full\\_bin\\_acc} = \\sum_{i=0}^{k_x - 1} \\mathtt{dx}^2[i] * \\mathtt{bin\\_effect}[i]\n        $$\n\n        $$\n        \\mathtt{remainder} = (x - \\mathtt{limits}[k_x-1])^2* \\mathtt{bin\\_effect}[k_x]\n        $$\n\n    Examples:\n        &gt;&gt;&gt; x = np.array([-1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0])\n        &gt;&gt;&gt; limits = np.array([0, 1.5, 2.0])\n        &gt;&gt;&gt; bin_effect = np.array([1.0, -1.0])\n        &gt;&gt;&gt; dx = np.array([1.5, 0.5])\n        &gt;&gt;&gt; compute_accumulated_effect(x, limits, bin_effect, dx)\n        array([0. , 0. , 0. , 0.5, 1. , 1.5, 1. , 1. , 1. ])\n\n        &gt;&gt;&gt; x = np.array([-1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0])\n        &gt;&gt;&gt; limits = np.array([0, 1.5, 2.0])\n        &gt;&gt;&gt; bin_effect = np.array([1.0, 1.0])\n        &gt;&gt;&gt; dx = np.array([1.5, 0.5])\n        &gt;&gt;&gt; compute_accumulated_effect(x, limits, bin_effect, dx)\n        array([0. , 0. , 0. , 0.5, 1. , 1.5, 2. , 2. , 2. ])\n\n\n\n    Parameters:\n        x: The points we want to evaluate at, (T)\n        limits: The bin limits, (K+1)\n        bin_effect: The effect in each bin, (K)\n        dx: The bin-widths, (K)\n        square: Whether to square the width. If true, the effect is bin_effect * dx^2, otherwise bin_effect * dx\n\n    Returns:\n        y: The accumulated effect at each point, (T)\n\n\n    \"\"\"\n    # find where each point belongs to\n    ind = np.digitize(x, limits)\n\n    # for each point, find the accumulated full-bin effect\n    x_cumsum = (bin_effect * dx**2).cumsum() if square else (bin_effect * dx).cumsum()\n    tmp = np.concatenate([[0, 0], x_cumsum])\n    full_bin_effect = tmp[ind]\n\n    # for each point, find the remaining effect\n    tmp = np.concatenate([[limits[0]], limits[:-1], [BIG_M]])\n    deltas = x - tmp[ind]\n    deltas[deltas &lt; 0] = 0  # if xs &lt; left_limit =&gt; delta = 0\n    deltas = deltas**2 if square else deltas\n    tmp = np.concatenate([[0.0], bin_effect, [bin_effect[-1]]])\n    remaining_effect = deltas * tmp[ind]\n\n    # final effect\n    y = full_bin_effect + remaining_effect\n    return y\n</code></pre>"},{"location":"api/#effector.utils.compute_ale_params","title":"<code>compute_ale_params(xs, df_dxs, limits)</code>","text":"<p>Compute all important parameters for the ALE plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Example without interpolation\n&gt;&gt;&gt; xs = np.array([0.5, 1.2, 2, 2.3])\n&gt;&gt;&gt; df_dxs = np.array([30, 34, 15, 17])\n&gt;&gt;&gt; limits = np.array([0, 1.5, 3.])\n&gt;&gt;&gt; compute_ale_params(xs, df_dxs, limits)\n{'limits': array([0. , 1.5, 3. ]), 'dx': array([1.5, 1.5]), 'points_per_bin': array([2, 2]), 'bin_effect': array([32., 16.]), 'bin_variance': array([4., 1.]), 'bin_estimator_variance': array([2. , 0.5])}\n</code></pre> <pre><code>&gt;&gt;&gt; # Example with interpolation\n&gt;&gt;&gt; xs = np.array([1, 2, 2.8, 4])\n&gt;&gt;&gt; df_dxs = np.array([31, 34, 37, 40])\n&gt;&gt;&gt; limits = np.array([1, 3, 4])\n&gt;&gt;&gt; compute_ale_params(xs, df_dxs, limits)\n{'limits': array([1, 3, 4]), 'dx': array([2, 1]), 'points_per_bin': array([3, 1]), 'bin_effect': array([34., 40.]), 'bin_variance': array([6., 6.]), 'bin_estimator_variance': array([2., 2.])}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>xs</code> <code>np.ndarray</code> <p>The values of s-th feature, (N)</p> required <code>df_dxs</code> <code>np.ndarray</code> <p>The effect wrt the s-th feature, (N)</p> required <code>limits</code> <code>np.ndarray</code> <p>The bin limits, (K+1)</p> required <p>Returns:</p> Name Type Description <code>parameters</code> <code>dict</code> <p>dict</p> Source code in <code>/home/runner/work/effector/effector/effector/utils.py</code> <pre><code>def compute_ale_params(xs: np.ndarray, df_dxs: np.ndarray, limits: np.ndarray) -&gt; dict:\n\"\"\"\n    Compute all important parameters for the ALE plot.\n\n    Examples:\n        &gt;&gt;&gt; # Example without interpolation\n        &gt;&gt;&gt; xs = np.array([0.5, 1.2, 2, 2.3])\n        &gt;&gt;&gt; df_dxs = np.array([30, 34, 15, 17])\n        &gt;&gt;&gt; limits = np.array([0, 1.5, 3.])\n        &gt;&gt;&gt; compute_ale_params(xs, df_dxs, limits)\n        {'limits': array([0. , 1.5, 3. ]), 'dx': array([1.5, 1.5]), 'points_per_bin': array([2, 2]), 'bin_effect': array([32., 16.]), 'bin_variance': array([4., 1.]), 'bin_estimator_variance': array([2. , 0.5])}\n\n        &gt;&gt;&gt; # Example with interpolation\n        &gt;&gt;&gt; xs = np.array([1, 2, 2.8, 4])\n        &gt;&gt;&gt; df_dxs = np.array([31, 34, 37, 40])\n        &gt;&gt;&gt; limits = np.array([1, 3, 4])\n        &gt;&gt;&gt; compute_ale_params(xs, df_dxs, limits)\n        {'limits': array([1, 3, 4]), 'dx': array([2, 1]), 'points_per_bin': array([3, 1]), 'bin_effect': array([34., 40.]), 'bin_variance': array([6., 6.]), 'bin_estimator_variance': array([2., 2.])}\n\n    Args:\n        xs: The values of s-th feature, (N)\n        df_dxs: The effect wrt the s-th feature, (N)\n        limits: The bin limits, (K+1)\n\n    Returns:\n        parameters: dict\n\n    \"\"\"\n    # compute bin-widths\n    dx = np.array([limits[i + 1] - limits[i] for i in range(len(limits) - 1)])\n\n    # compute mean effect on each bin\n    bin_effect_nans, points_per_bin = compute_bin_effect(xs, df_dxs, limits)\n\n    # compute effect variance in each bin\n    bin_variance_nans, bin_estimator_variance_nans = compute_bin_variance(\n        xs, df_dxs, limits, bin_effect_nans\n    )\n\n    # interpolate NaNs\n    bin_effect = fill_nans(bin_effect_nans)\n    bin_variance = fill_nans(bin_variance_nans)\n    bin_estimator_variance = fill_nans(bin_estimator_variance_nans)\n\n    parameters = {\n        \"limits\": limits,\n        \"dx\": dx,\n        \"points_per_bin\": points_per_bin,\n        \"bin_effect\": bin_effect,\n        \"bin_variance\": bin_variance,\n        \"bin_estimator_variance\": bin_estimator_variance,\n    }\n    return parameters\n</code></pre>"},{"location":"api/#effector.utils.compute_bin_effect","title":"<code>compute_bin_effect(xs, df_dxs, limits)</code>","text":"<p>Compute the mean effect in each bin.</p> Notes <p>The function (a) allocates the instances in the bins and (b) aggregates the instance-level effects to compute the average bin-effect. If no instances lie in a bin, then the bin effect is NaN.</p> \\[ \\mathtt{bin\\_effect}_k = {1 \\over |i \\in bin_k|} \\sum_{i \\in bin_k} \\mathtt{effect}_i \\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; n = 100\n&gt;&gt;&gt; xs = np.ones([n]) - 0.5\n&gt;&gt;&gt; df_dxs = np.ones_like(xs) * 10\n&gt;&gt;&gt; limits = np.array([0., 1., 2.0])\n&gt;&gt;&gt; bin_effects, ppb = compute_bin_effect(xs, df_dxs, limits)\n&gt;&gt;&gt; bin_effects\narray([10., nan])\n&gt;&gt;&gt; ppb\narray([100,   0])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>xs</code> <code>np.ndarray</code> <p>The s-th feature of the instances, (N)</p> required <code>df_dxs</code> <code>np.ndarray</code> <p>The effect wrt the s-th feature for each instance, (N)</p> required <code>limits</code> <code>np.ndarray</code> <p>The bin limits, (K+1)</p> required <p>Returns:</p> Name Type Description <code>bin_effects</code> <code>np.ndarray</code> <p>The average effect per bin, (K)</p> <code>points_per_bin</code> <code>np.ndarray</code> <p>The number of points per bin, (K)</p> Source code in <code>/home/runner/work/effector/effector/effector/utils.py</code> <pre><code>def compute_bin_effect(\n    xs: np.ndarray, df_dxs: np.ndarray, limits: np.ndarray\n) -&gt; typing.Tuple[np.ndarray, np.ndarray]:\n\"\"\"Compute the mean effect in each bin.\n\n    Notes:\n        The function (a) allocates the instances in the bins and (b) aggregates the instance-level effects to compute\n        the average bin-effect. If no instances lie in a bin, then the bin effect is NaN.\n\n        $$\n        \\mathtt{bin\\_effect}_k = {1 \\over |i \\in bin_k|} \\sum_{i \\in bin_k} \\mathtt{effect}_i\n        $$\n\n    Examples:\n        &gt;&gt;&gt; n = 100\n        &gt;&gt;&gt; xs = np.ones([n]) - 0.5\n        &gt;&gt;&gt; df_dxs = np.ones_like(xs) * 10\n        &gt;&gt;&gt; limits = np.array([0., 1., 2.0])\n        &gt;&gt;&gt; bin_effects, ppb = compute_bin_effect(xs, df_dxs, limits)\n        &gt;&gt;&gt; bin_effects\n        array([10., nan])\n        &gt;&gt;&gt; ppb\n        array([100,   0])\n\n    Parameters:\n        xs: The s-th feature of the instances, (N)\n        df_dxs: The effect wrt the s-th feature for each instance, (N)\n        limits: The bin limits, (K+1)\n\n    Returns:\n        bin_effects: The average effect per bin, (K)\n        points_per_bin: The number of points per bin, (K)\n    \"\"\"\n    empty_symbol = np.NaN\n\n    # find bin-index of points\n    limits_enh = copy.deepcopy(limits).astype(float)\n    limits_enh[-1] += EPS\n    ind = np.digitize(xs, limits_enh)\n    # assert np.alltrue(ind &gt; 0)\n\n    # bin effect is the mean of all points that lie in the bin\n    nof_bins = limits.shape[0] - 1\n    aggregated_effect = np.bincount(ind - 1, df_dxs, minlength=nof_bins)\n    points_per_bin = np.bincount(ind - 1, minlength=nof_bins)\n\n    # if no point lies in a bin, store Nan\n    bin_effect_mean = np.divide(\n        aggregated_effect,\n        points_per_bin,\n        out=np.ones(aggregated_effect.shape, dtype=float) * empty_symbol,\n        where=points_per_bin != 0,\n    )\n    return bin_effect_mean, points_per_bin\n</code></pre>"},{"location":"api/#effector.utils.compute_bin_variance","title":"<code>compute_bin_variance(xs, df_dxs, limits, bin_effect_mean)</code>","text":"<p>Compute the variance of the effect in each bin.</p> Notes <p>The function (a) allocates the points in the bins and (b) computes the variance and the variance/nof points. If less than two points in a bin, NaN is passed.</p> \\[ \\mathtt{bin\\_variance}_k = {1 \\over |i \\in bin_k|} \\sum_{i \\in bin_k} (\\mathtt{effect}_i - \\mathtt{bin\\_effect}_k)^2 \\] \\[ \\mathtt{bin\\_estimator\\_variance_k} = {\\mathtt{bin\\_variance}_k \\over |i \\in bin_k|} \\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; n = 100\n&gt;&gt;&gt; xs = np.ones([n]) - 0.5\n&gt;&gt;&gt; df_dxs = np.ones_like(xs) * 10\n&gt;&gt;&gt; limits = np.array([0., 1., 2.0])\n&gt;&gt;&gt; bin_effect_mean, ppb = compute_bin_effect(xs, df_dxs, limits)\n&gt;&gt;&gt; bin_variance, bin_estimator_variance = compute_bin_variance(xs, df_dxs, limits, bin_effect_mean)\n&gt;&gt;&gt; bin_variance\narray([ 0., nan])\n&gt;&gt;&gt; bin_estimator_variance\narray([ 0., nan])\n</code></pre> <pre><code>&gt;&gt;&gt; xs = np.ones(4) * 0.5\n&gt;&gt;&gt; df_dxs = np.array([1.0, 3.0, 3.0, 5.0])\n&gt;&gt;&gt; limits = np.array([0, 1, 2.0])\n&gt;&gt;&gt; bin_effect_mean = np.array([np.mean(df_dxs), np.NaN])\n&gt;&gt;&gt; compute_bin_variance(xs, df_dxs, limits, bin_effect_mean)\n(array([ 2., nan]), array([0.5, nan]))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>xs</code> <code>np.ndarray</code> <p>The points we evaluate, (N)</p> required <code>df_dxs</code> <code>np.ndarray</code> <p>The effect of each point, (N, )</p> required <code>limits</code> <code>np.ndarray</code> <p>The bin limits (K+1)</p> required <code>bin_effect_mean</code> <code>np.ndarray</code> <p>Mean effect in each bin, (K)</p> required <p>Returns:</p> Name Type Description <code>bin_variance</code> <code>np.ndarray</code> <p>The variance in each bin, (K, )</p> <code>bin_estimator_variance</code> <code>np.ndarray</code> <p>The variance of the estimator in each bin, (K, )</p> Source code in <code>/home/runner/work/effector/effector/effector/utils.py</code> <pre><code>def compute_bin_variance(\n    xs: np.ndarray, df_dxs: np.ndarray, limits: np.ndarray, bin_effect_mean: np.ndarray\n) -&gt; typing.Tuple[np.ndarray, np.ndarray]:\n\"\"\"\n    Compute the variance of the effect in each bin.\n\n    Notes:\n        The function (a) allocates the points in the bins and (b) computes the variance and the variance/nof points.\n        If less than two points in a bin, NaN is passed.\n\n        $$\n        \\mathtt{bin\\_variance}_k = {1 \\over |i \\in bin_k|} \\sum_{i \\in bin_k}\n        (\\mathtt{effect}_i - \\mathtt{bin\\_effect}_k)^2\n        $$\n\n        $$\n        \\mathtt{bin\\_estimator\\_variance_k} = {\\mathtt{bin\\_variance}_k \\over |i \\in bin_k|}\n        $$\n\n    Examples:\n        &gt;&gt;&gt; n = 100\n        &gt;&gt;&gt; xs = np.ones([n]) - 0.5\n        &gt;&gt;&gt; df_dxs = np.ones_like(xs) * 10\n        &gt;&gt;&gt; limits = np.array([0., 1., 2.0])\n        &gt;&gt;&gt; bin_effect_mean, ppb = compute_bin_effect(xs, df_dxs, limits)\n        &gt;&gt;&gt; bin_variance, bin_estimator_variance = compute_bin_variance(xs, df_dxs, limits, bin_effect_mean)\n        &gt;&gt;&gt; bin_variance\n        array([ 0., nan])\n        &gt;&gt;&gt; bin_estimator_variance\n        array([ 0., nan])\n\n        &gt;&gt;&gt; xs = np.ones(4) * 0.5\n        &gt;&gt;&gt; df_dxs = np.array([1.0, 3.0, 3.0, 5.0])\n        &gt;&gt;&gt; limits = np.array([0, 1, 2.0])\n        &gt;&gt;&gt; bin_effect_mean = np.array([np.mean(df_dxs), np.NaN])\n        &gt;&gt;&gt; compute_bin_variance(xs, df_dxs, limits, bin_effect_mean)\n        (array([ 2., nan]), array([0.5, nan]))\n\n    Parameters:\n        xs: The points we evaluate, (N)\n        df_dxs: The effect of each point, (N, )\n        limits: The bin limits (K+1)\n        bin_effect_mean: Mean effect in each bin, (K)\n\n    Returns:\n        bin_variance: The variance in each bin, (K, )\n        bin_estimator_variance: The variance of the estimator in each bin, (K, )\n\n    \"\"\"\n    empty_symbol = np.NaN\n\n    # find bin-index of points\n    eps = 1e-8\n    limits_enh = copy.deepcopy(limits).astype(float)\n    limits_enh[-1] += eps\n    ind = np.digitize(xs, limits_enh)\n    # assert np.alltrue(ind &gt; 0)\n\n    # variance of the effect in each bin\n    variance_per_point = (df_dxs - bin_effect_mean[ind - 1]) ** 2\n    nof_bins = limits.shape[0] - 1\n    aggregated_variance_per_bin = np.bincount(\n        ind - 1, variance_per_point, minlength=nof_bins\n    )\n    points_per_bin = np.bincount(ind - 1, minlength=nof_bins)\n\n    # if less than two points in a bin, store Nan\n    bin_variance = np.divide(\n        aggregated_variance_per_bin,\n        points_per_bin,\n        out=np.ones(aggregated_variance_per_bin.shape, dtype=float) * empty_symbol,\n        where=points_per_bin &gt; 1,\n    )\n\n    # the variance of the estimator\n    bin_estimator_variance = np.divide(\n        bin_variance,\n        points_per_bin,\n        out=np.ones(aggregated_variance_per_bin.shape, dtype=float) * empty_symbol,\n        where=points_per_bin &gt; 1,\n    )\n    return bin_variance, bin_estimator_variance\n</code></pre>"},{"location":"api/#effector.utils.compute_jacobian_numerically","title":"<code>compute_jacobian_numerically(model, data, eps=1e-08)</code>","text":"<p>Compute the Jacobian of the model using finite differences.</p> Notes <p>The function computes the Jacobian of the model using finite differences. The formula is:</p> \\[ \\mathtt{J} = {\\mathtt{model}(x + \\mathtt{eps}) - \\mathtt{model}(x) \\over \\mathtt{eps}} \\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = np.array([[1, 2], [2, 3.0]])\n&gt;&gt;&gt; model = lambda x: np.sum(x, axis=1)\n&gt;&gt;&gt; compute_jacobian_numerically(model, data)\narray([[1., 1.],\n       [1., 1.]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray</code> <p>The dataset, (N, D)</p> required <code>model</code> <code>typing.Callable</code> <p>The black-box model ((N, D) -&gt; (N))</p> required <code>eps</code> <code>float</code> <p>The finite difference step</p> <code>1e-08</code> <p>Returns:</p> Name Type Description <code>jacobian</code> <code>np.ndarray</code> <p>The Jacobian of the model, (N, D)</p> Source code in <code>/home/runner/work/effector/effector/effector/utils.py</code> <pre><code>def compute_jacobian_numerically(\n    model: typing.Callable, data: np.ndarray, eps: float = 1e-8\n) -&gt; np.ndarray:\n\"\"\"Compute the Jacobian of the model using finite differences.\n\n    Notes:\n        The function computes the Jacobian of the model using finite differences. The formula is:\n\n        $$\n        \\mathtt{J} = {\\mathtt{model}(x + \\mathtt{eps}) - \\mathtt{model}(x) \\over \\mathtt{eps}}\n        $$\n\n    Examples:\n        &gt;&gt;&gt; data = np.array([[1, 2], [2, 3.0]])\n        &gt;&gt;&gt; model = lambda x: np.sum(x, axis=1)\n        &gt;&gt;&gt; compute_jacobian_numerically(model, data)\n        array([[1., 1.],\n               [1., 1.]])\n\n    Args:\n        data: The dataset, (N, D)\n        model: The black-box model ((N, D) -&gt; (N))\n        eps: The finite difference step\n\n    Returns:\n        jacobian: The Jacobian of the model, (N, D)\n\n    \"\"\"\n    assert data.ndim == 2\n    jacobian = np.zeros_like(data)\n    for f in range(data.shape[1]):\n        data_plus = copy.deepcopy(data)\n        data_plus[:, f] += eps\n        jacobian[:, f] = (model(data_plus) - model(data)) / eps\n    return jacobian\n</code></pre>"},{"location":"api/#effector.utils.compute_local_effects","title":"<code>compute_local_effects(data, model, limits, feature)</code>","text":"<p>Compute the local effects, permuting the feature of interest using the bin limits.</p> Notes <p>The function (a) allocates the points in the bins based on the feature of interest (foi) and (b) computes the effect as the difference when evaluating the output setting the foi at the right and the left limit of the bin.</p> <p>Given that the bins are defined as a list [l_0, l_1, ..., l_k], and x_s of the i-th point belongs to the k-th bin:</p> \\[ {df \\over dx_s}(x^i) = {f(x_0^i, ... ,x_s=l_k, ..., x_D^i) - f(x_0^i, ... ,x_s=l_{k-1}, ..., x_D^i)  \\over l_k - l_{k-1}} \\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = np.array([[1, 2], [2, 3.0]])\n&gt;&gt;&gt; model = lambda x: np.sum(x, axis=1)\n&gt;&gt;&gt; limits = np.array([1.0, 2.0])\n&gt;&gt;&gt; data_effect = compute_local_effects(data, model, limits, feature=0)\n&gt;&gt;&gt; data_effect\narray([1., 1.])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray</code> <p>The training set, (N, D)</p> required <code>model</code> <code>typing.Callable</code> <p>The black-box model ((N, D) -&gt; (N))</p> required <code>limits</code> <code>np.ndarray</code> <p>The bin limits, (K+1)</p> required <code>feature</code> <code>int</code> <p>Index of the feature-of-interest</p> required <p>Returns:</p> Name Type Description <code>data_effect</code> <code>np.ndarray</code> <p>The local effect of each data point, (N)</p> Source code in <code>/home/runner/work/effector/effector/effector/utils.py</code> <pre><code>def compute_local_effects(\n    data: np.ndarray, model: typing.Callable, limits: np.ndarray, feature: int\n) -&gt; np.ndarray:\n\"\"\"Compute the local effects, permuting the feature of interest using the bin limits.\n\n    Notes:\n        The function (a) allocates the points in the bins based on the feature of interest (foi)\n        and (b) computes the effect as the difference when evaluating the output setting the foi at the right and the\n        left limit of the bin.\n\n        Given that the bins are defined as a list [l_0, l_1, ..., l_k], and x_s of the i-th point belongs to the k-th bin:\n\n        $$\n        {df \\over dx_s}(x^i) = {f(x_0^i, ... ,x_s=l_k, ..., x_D^i) - f(x_0^i, ... ,x_s=l_{k-1}, ..., x_D^i)\n         \\over l_k - l_{k-1}}\n        $$\n\n\n    Examples:\n        &gt;&gt;&gt; data = np.array([[1, 2], [2, 3.0]])\n        &gt;&gt;&gt; model = lambda x: np.sum(x, axis=1)\n        &gt;&gt;&gt; limits = np.array([1.0, 2.0])\n        &gt;&gt;&gt; data_effect = compute_local_effects(data, model, limits, feature=0)\n        &gt;&gt;&gt; data_effect\n        array([1., 1.])\n\n    Args:\n        data: The training set, (N, D)\n        model: The black-box model ((N, D) -&gt; (N))\n        limits: The bin limits, (K+1)\n        feature: Index of the feature-of-interest\n\n    Returns:\n        data_effect: The local effect of each data point, (N)\n\n    \"\"\"\n    assert data.ndim == 2\n\n    # check that limits cover all data points\n    assert limits[0] &lt;= np.min(data[:, feature])\n    assert limits[-1] &gt;= np.max(data[:, feature])\n\n    # for each point, find the bin-index it belongs to\n    limits[-1] += EPS\n    ind = np.digitize(data[:, feature], limits)\n    assert np.alltrue(ind &gt; 0)\n\n    # compute effect\n    right_lim = copy.deepcopy(data)\n    left_lim = copy.deepcopy(data)\n    right_lim[:, feature] = limits[ind]\n    left_lim[:, feature] = limits[ind - 1]\n    dx = limits[1] - limits[0]\n    data_effect = model(right_lim) - model(left_lim)\n    return np.squeeze(data_effect) / dx\n</code></pre>"},{"location":"api/#effector.utils.fill_nans","title":"<code>fill_nans(x)</code>","text":"<p>Replace NaNs with interpolated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; x = np.array([1.0, np.NaN, 2.0])\n&gt;&gt;&gt; fill_nans(x)\narray([1. , 1.5, 2. ])\n</code></pre> <pre><code>&gt;&gt;&gt; x = np.array([1.0, np.NaN, np.NaN, np.NaN, 2.0])\n&gt;&gt;&gt; fill_nans(x)\narray([1.  , 1.25, 1.5 , 1.75, 2.  ])\n</code></pre> <pre><code>&gt;&gt;&gt; x = np.array([0.5, 1.0, np.NaN, np.NaN, np.NaN])\n&gt;&gt;&gt; fill_nans(x)\narray([0.5, 1. , 1. , 1. , 1. ])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray</code> <p>Time-series with NaNs, (T)</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>np.ndarray</code> <p>Time-series values without NaNs, (T)</p> Source code in <code>/home/runner/work/effector/effector/effector/utils.py</code> <pre><code>def fill_nans(x: np.ndarray) -&gt; np.ndarray:\n\"\"\"Replace NaNs with interpolated values.\n\n    Examples:\n        &gt;&gt;&gt; x = np.array([1.0, np.NaN, 2.0])\n        &gt;&gt;&gt; fill_nans(x)\n        array([1. , 1.5, 2. ])\n\n        &gt;&gt;&gt; x = np.array([1.0, np.NaN, np.NaN, np.NaN, 2.0])\n        &gt;&gt;&gt; fill_nans(x)\n        array([1.  , 1.25, 1.5 , 1.75, 2.  ])\n\n        &gt;&gt;&gt; x = np.array([0.5, 1.0, np.NaN, np.NaN, np.NaN])\n        &gt;&gt;&gt; fill_nans(x)\n        array([0.5, 1. , 1. , 1. , 1. ])\n\n    Parameters:\n        x: Time-series with NaNs, (T)\n\n    Returns:\n        x: Time-series values without NaNs, (T)\n    \"\"\"\n    bin_effect_1 = copy.deepcopy(x)\n\n    def nan_helper(y):\n        return np.isnan(y), lambda z: z.nonzero()[0]\n\n    nans, x = nan_helper(bin_effect_1)\n    bin_effect_1[nans] = np.interp(x(nans), x(~nans), bin_effect_1[~nans])\n    return bin_effect_1\n</code></pre>"},{"location":"api/#effector.utils.filter_points_in_bin","title":"<code>filter_points_in_bin(xs, df_dxs, limits)</code>","text":"<p>Filter the points inside the bin defined by the <code>limits</code>.</p> Notes <p>Filtering depends on whether <code>xs</code> lies in the interval [limits[0], limits[1]], not <code>df_dxs</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; xs = np.array([1, 2, 3])\n&gt;&gt;&gt; df_dxs = np.array([32, 34, 36])\n&gt;&gt;&gt; limits = np.array([1, 2])\n&gt;&gt;&gt; xs, df_dxs = filter_points_in_bin(xs, df_dxs, limits)\n&gt;&gt;&gt; xs\narray([1, 2])\n&gt;&gt;&gt; df_dxs\narray([32, 34])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>xs</code> <code>np.ndarray</code> <p>The instances, (N)</p> required <code>df_dxs</code> <code>typing.Union[np.ndarray, None]</code> <p>The instance-effects (N) or None</p> required <code>limits</code> <code>np.ndarray</code> <p>[Start, Stop] of the bin</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>np.ndarray</code> <p>The instances in the bin, (nof_points_in_bin, D)</p> <code>data_effect</code> <code>typing.Union[np.ndarray, None]</code> <p>The instance-effects in the bin, (nof_points_in_bin, D) or None</p> Source code in <code>/home/runner/work/effector/effector/effector/utils.py</code> <pre><code>def filter_points_in_bin(\n    xs: np.ndarray, df_dxs: typing.Union[np.ndarray, None], limits: np.ndarray\n) -&gt; typing.Tuple[np.ndarray, typing.Union[np.ndarray, None]]:\n\"\"\"\n    Filter the points inside the bin defined by the `limits`.\n\n    Notes:\n        Filtering depends on whether `xs` lies in the interval [limits[0], limits[1]], not `df_dxs`.\n\n    Examples:\n        &gt;&gt;&gt; xs = np.array([1, 2, 3])\n        &gt;&gt;&gt; df_dxs = np.array([32, 34, 36])\n        &gt;&gt;&gt; limits = np.array([1, 2])\n        &gt;&gt;&gt; xs, df_dxs = filter_points_in_bin(xs, df_dxs, limits)\n        &gt;&gt;&gt; xs\n        array([1, 2])\n        &gt;&gt;&gt; df_dxs\n        array([32, 34])\n\n    Args:\n        xs: The instances, (N)\n        df_dxs: The instance-effects (N) or None\n        limits: [Start, Stop] of the bin\n\n    Returns:\n        data: The instances in the bin, (nof_points_in_bin, D)\n        data_effect: The instance-effects in the bin, (nof_points_in_bin, D) or None\n\n    \"\"\"\n    filt = np.logical_and(limits[0] &lt;= xs, xs &lt;= limits[1])\n\n    # return data\n    xs = xs[filt]\n\n    # return data effect if not None\n    if df_dxs is not None:\n        df_dxs = df_dxs[filt]\n    return xs, df_dxs\n</code></pre>"},{"location":"api/#effector.utils.get_feature_types","title":"<code>get_feature_types(data, categorical_limit=10)</code>","text":"<p>Determine the type of each feature.</p> Notes <p>A feature is considered as categorical if it has less than <code>cat_limit</code> unique values.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray</code> <p>The dataset, (N, D)</p> required <code>categorical_limit</code> <code>int</code> <p>Maximum unique values for a feature to be considered as categorical</p> <code>10</code> <p>Returns:</p> Name Type Description <code>types</code> <code>typing.List[str]</code> <p>A list of strings, where each string is either <code>\"cat\"</code> or <code>\"cont\"</code></p> Source code in <code>/home/runner/work/effector/effector/effector/utils.py</code> <pre><code>def get_feature_types(data: np.ndarray, categorical_limit: int = 10) -&gt; typing.List[str]:\n\"\"\"Determine the type of each feature.\n\n    Notes:\n        A feature is considered as categorical if it has less than `cat_limit` unique values.\n\n    Args:\n        data: The dataset, (N, D)\n        categorical_limit: Maximum unique values for a feature to be considered as categorical\n\n\n    Returns:\n        types: A list of strings, where each string is either `\"cat\"` or `\"cont\"`\n\n    \"\"\"\n\n    types = [\n        \"cat\" if len(np.unique(data[:, f])) &lt; categorical_limit else \"cont\"\n        for f in range(data.shape[1])\n    ]\n    return types\n</code></pre>"},{"location":"Feature%20Effect/01_global_effect_intro/","title":"Global Feature Effect","text":"Why do we care about (global) feature effect plots? <p>Because they are one of the simplest ways to globally interpret a black-box model.</p> <p>Imagine you have trained a neural network to predict the expected daily bike rentals, like in this tutorial. The model is delivering satisfactory results,  exhibiting an average prediction error of approximately \\(43\\) bike rentals per day.</p> <p>You want to interpret how the model works.  Using feature effect plots, you can immediately get a graphical representation that illustrates  how individual features impact the model's predictions:</p> <pre><code>effector.RHALE(X, model, model_jac).plot(feature=3)\n</code></pre> <p></p> <pre><code>effector.PDP(X, model).plot(feature=3)\n</code></pre> <p></p> <p>Both plots show the effect of feature \\(\\mathtt{hour}\\) on the daily \\(\\mathtt{bike-rentals}\\); there is an abrupt increase in the number of bike rentals at about 8:00 AM (beginning of the workday) and at about 5:00 PM (end of the workday).  The following table provides a more detailed interpretation of the plot: </p> Interpretation: Move along the axis and interpret Interval Description 0-6 Bike rentals are almost constant and much lower than the average, which is \\(\\approx 189\\) 6-8.30 Rapid increase; at about 7.00 we are at the average rentals and then even more. 8.30-9.30 Sudden drop; rentals move back to the average. 9.30-15 Small increase. 15-17 High increase; at 17 bike rentals reach the maximum. 17-24 A constant drop; at 19.00 rentals reach the average and keep decreasing. <p>Global feature effect plots provide an immediate interpretation of the model's inner workings, which can raise some criticism and lead to appropriate actions.</p> Criticism 1: Does this makes sense? <p>The quick answer is yes. In a typical workday scenario, it's common for people to commute to   work between 6:00 AM and 8:30 AM and return home between 3:00 PM and 5:00 PM.  However, an expert in city transportation might point out that the model appears to have a slight time shift.   They could argue that the actual peak in bike rentals occurs at 8:30 AM, rather than the model's prediction   of 7:30 AM.</p> Action 1: Check if the data confirm criticism 1 <p>We will investigate whether this issue stems from the model or the data.  If the claim is not supported by the data, then it may be the case that the recording system   provokes the time shift or that the expert is wrong.  If the claim is supported by the data, then we should improve the model to increase its accuracy.</p> Criticism 2: Is the explanation align with all potential scenarios? <p>Another expert notices that this pattern is meaningful only for the working days. During weekends and holidays, it doesn't logically justify having a peak in rentals as early as 7:30 AM.</p> Action 2: Check the fidelity of the explanation <p>To take meaningful action, it is crucial to determine whether this issue originates from the model or the explanation. Is the model treating the feature \\(\\mathtt{hour}\\) uniformly across all days, despite the intuitive understanding that this feature may vary between weekdays and weekends? Is it the opposite- the model has learned to differentiate between these two cases,  but it is the explanation that obscures them behind the averaging?  Heterogeneity analysis can help us answer these questions.</p>"},{"location":"Feature%20Effect/01_global_effect_intro/#the-heterogeneity-is-the-fidelity-of-the-explanation","title":"The heterogeneity is the fidelity of the explanation","text":"<p>Based on Criticism 2, we want to check whether the explanation is valid for all the instances of the dataset. We can do this by analyzing the heterogeneity, i.e., the deviation of the instance-level effects from the average effect.  In <code>Effector</code>, we can do this by simply setting the <code>confidence_interval</code> parameter an appropriate value:</p> <pre><code>effector.RHALE(X, model, model_jac).plot(feature=3, heterogeneity=True)\n</code></pre> <p></p> <pre><code>effector.PDP(X, model).plot(feature=3, heterogeneity=\"ice\")\n</code></pre> <p></p> <p>Both methods show that there is high-variance in the instance-level effects; this means that the effect of feature \\(\\mathtt{hour}\\) varies significantly across different instances.</p> <p>Moreover, PDP-ICE analysis provides precise insights into the distinct patterns:</p> <ul> <li>There is one cluster, that behaves as described above. </li> <li>There is a second cluster that behaves differently, with a rise starting at 9:00 AM, a peak at 12:00 AM and a decline at 6:00 PM.</li> </ul> We have an answer <p>Great! We have a clear answer on the question raised by Action 2.   The issue does not lie with the model itself; instead, it's the global   explanation that has concealed the two distinct patterns by averaging them out.</p> Don't rush to conclusions <p>There is a small piece of the puzzle missing. Although we have identified the two distinct patterns, we still don't know what causes them. Of course, we can guess that the first pattern is related to the working days, and the second pattern is related to the weekends and holidays. But this is simply our intuition, and we need to confirm it with the data.  We need to find the features that are responsible for the two distinct patterns. Regional effect plots are the answer to this question.</p>"},{"location":"Feature%20Effect/01_global_effect_intro/#resources-for-further-reading","title":"Resources for further reading","text":"<p>Below we provide some resources for further reading.</p> <p>Papers:</p> <ul> <li>Model-Agnostic Effects Plots for Interpreting Machine Learning Models </li> </ul> <p>Books:</p> <ul> <li>https://christophm.github.io/interpretable-ml-book/</li> <li>https://ema.drwhy.ai/preface.html</li> <li>Limitations of Interpretable Machine Learning Methods</li> </ul>"},{"location":"Feature%20Effect/02_regional_effect_intro/","title":"Regional Feature Effect","text":"Why do we care about regional effect plots? <p>Because they are one of the simplest ways to regionally interpret a black-box model.</p> <p>In the previous tutorial, we explained the effect of feature \\(\\mathtt{hour}\\) on the daily \\(\\mathtt{bike-rentals}\\),  using global feature effect plots.  The analysis, however, showed that there is high heterogeneity; there are many instances that behave differently from the average pattern.</p>"},{"location":"Feature%20Effect/02_regional_effect_intro/#the-heterogeneity-is-the-fidelity-of-the-explanation","title":"The heterogeneity is the fidelity of the explanation","text":"Recap on heterogeneity <p>Because Global Effect methods average the instance-level effects,  they may obscure that many instances behave differently from the average pattern. Heterogeneity analysis helps us to identify these instances.</p> <p>Below, we show the global effect plot for the \\(\\mathtt{hour}\\) feature on the daily \\(\\mathtt{bike-rentals}\\):</p> <p> </p> When Global Effect is a weak explanation? <p>In cases where the global effect plot shows high heterogeneity, it is useful to analyze the regional effect. Why is this the case? Because when many instances behave differently from the average pattern,  it means that the effect of feature \\(x_s\\) on the output \\(y\\), depends on the values of other features \\(x_{\\setminus s}\\). In these cases, the average effect of feature \\(x_s\\) on the output \\(y\\) is a weak explanation.</p> When Regional Effect can provide a good solution <p>In cases where the global effect plot shows high heterogeneity,  it may be the case that there are subregions where the instances behave similarly. Regional Effect Plots search for subregions where  the effect of feature \\(x_s\\) on the output \\(y\\), has smaller dependence on the values of other features \\(x_{\\setminus s}\\).</p>"},{"location":"Feature%20Effect/02_regional_effect_intro/#regional-rhale","title":"Regional RHALE","text":"<p>So let's apply regional effect analysis to the \\(\\mathtt{hour}\\) feature. <code>Effector</code> provides a simple API for that, similar to the global effect API:</p> <pre><code>regional_rhale = effector.RegionalRHALE(X, model, model_jac).plot(feature=3, heterogeneity=True)\n</code></pre> <p> </p>"},{"location":"Feature%20Effect/02_regional_effect_intro/#regional-pdp","title":"Regional PDP","text":"<pre><code>regional_pdp = effector.RegionalPDP(X, model).plot(feature=3, heterogeneity=\"ice\")\n</code></pre>"},{"location":"Feature%20Effect/02_regional_effect_intro/#conclusion","title":"Conclusion","text":"Don't rush to conclusions <p>In the Global Effect Tutorial, we said that there is a small piece of the puzzle missing. We copy-paste the text:</p> <p>Although we have identified the two distinct patterns, we still don't know what causes them. Of course, we can guess that the first pattern is related to the working days, and the second pattern is related to the weekends and holidays. But this is simply our intuition, and we need to confirm it with the data.  We need to find the features that are responsible for the two distinct patterns.</p> Let's rush to conclusions <p>The regional effect analysis confirms our intution. There are two distinct patterns in the data: one pattern is related to the working days, and the second pattern is related to the weekends and holidays. The first pattern is characterized by a rise in the number of rentals at 8:30 AM andn  at 17:00 AM, when people go to and from work. The second pattern is characterized by a rise in the number of rentals at 9:00 AM, a peak at 12:00 AM and a decline at  4:00 PM, a typical non-working day pattern.</p>"},{"location":"Tutorials/Guides/wrap_models/","title":"Wrap ML models into callables","text":"<p>In this tutorial, we provide brief snippets for wrapping ML models into callables.</p> scikit-learnpytorchtensorflowxgboost <pre><code>from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor()\nmodel.fit(X, y)\n\ndef model_callable(X):\n  return model.predict(X)\n\n# Attention: scikit-learn does not provide a method for computing the Jacobian using automatic differentiation.\n</code></pre> <pre><code>import torch\nimport torch.nn as nn\n\nclass Net(nn.Module):\n  def __init__(self):\n    super(Net, self).__init__()\n    self.fc1 = nn.Linear(D, 10)\n    self.fc2 = nn.Linear(10, 1)\n\n  def forward(self, x):\n    x = self.fc1(x)\n    x = self.fc2(x)\n    return x\n\nmodel = Net()\n\n# train model\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\nfor epoch in range(10):\n  optimizer.zero_grad()\n  y_pred = model(X)\n  loss = criterion(y_pred, y)\n  loss.backward()\n  optimizer.step()\n\ndef model_callable(X):\n    return model(torch.tensor(X, dtype=torch.float32)).detach().numpy()\n\ndef model_jac_callable(X):\n    X = torch.tensor(X, dtype=torch.float32)\n    X.requires_grad = True\n    y = model(X)\n    return torch.autograd.grad(y, X)[0].numpy()\n</code></pre> <pre><code>import tensorflow as tf\n\n# define model\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(10, activation='relu'),\n  tf.keras.layers.Dense(1)\n])\n\n# train model\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X, y, epochs=10)\n\ndef model_callable(X):\n  return model.predict(X)\n\ndef model_jac_callable(X):\n    X = tf.convert_to_tensor(X, dtype=tf.float32)\n    with tf.GradientTape() as tape:\n        tape.watch(X)\n        y = model(X)\n    return tape.gradient(y, X).numpy()\n</code></pre> <pre><code>import xgboost as xgb\n\nmodel = xgb.XGBRegressor()\nmodel.fit(X, y)\n\ndef model_callable(X):\n  return model.predict(X)\n\nAttention: Tree based models are not differentiable, so there is no way to compute the Jacobian.\n</code></pre>"},{"location":"Tutorials/real-examples/01_bike_sharing_dataset/","title":"Bike-Sharing Dataset","text":"<p>The Bike-Sharing Dataset This dataset contains the hourly and daily count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information. The dataset contains 14 features with information about the day-type, e.g., month, hour, which day of the week, whether it is working-day, and the weather conditions, e.g., temperature, humidity, wind speed, etc. The target variable is the number of bike rentals per hour. The dataset contains 17,379 instances. </p> <pre><code>import effector\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport random\n\nnp.random.seed(42)\ntf.random.set_seed(42)\nrandom.seed(42)\n</code></pre>"},{"location":"Tutorials/real-examples/01_bike_sharing_dataset/#preprocess-the-data","title":"Preprocess the data","text":"<pre><code>from ucimlrepo import fetch_ucirepo \n\n# fetch dataset \nbike_sharing_dataset = fetch_ucirepo(id=275) \n\n# data (as pandas dataframes) \nX = bike_sharing_dataset.data.features \ny = bike_sharing_dataset.data.targets \n\n# metadata \nprint(bike_sharing_dataset.metadata) \n\n# variable information \nprint(bike_sharing_dataset.variables) \n</code></pre> <pre><code>{'uci_id': 275, 'name': 'Bike Sharing Dataset', 'repository_url': 'https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset', 'data_url': 'https://archive.ics.uci.edu/static/public/275/data.csv', 'abstract': 'This dataset contains the hourly and daily count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information.', 'area': 'Social Science', 'tasks': ['Regression'], 'characteristics': ['Multivariate'], 'num_instances': 17389, 'num_features': 13, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['cnt'], 'index_col': ['instant'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2013, 'last_updated': 'Wed Dec 20 2023', 'dataset_doi': '10.24432/C5W894', 'creators': ['Hadi Fanaee-T'], 'intro_paper': None, 'additional_info': {'summary': 'Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues. \\r\\n\\r\\nApart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Both hour.csv and day.csv have the following fields, except hr which is not available in day.csv\\r\\n\\t\\r\\n\\t- instant: record index\\r\\n\\t- dteday : date\\r\\n\\t- season : season (1:winter, 2:spring, 3:summer, 4:fall)\\r\\n\\t- yr : year (0: 2011, 1:2012)\\r\\n\\t- mnth : month ( 1 to 12)\\r\\n\\t- hr : hour (0 to 23)\\r\\n\\t- holiday : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\\r\\n\\t- weekday : day of the week\\r\\n\\t- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\\r\\n\\t+ weathersit : \\r\\n\\t\\t- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\\r\\n\\t\\t- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\\r\\n\\t\\t- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\\r\\n\\t\\t- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\\r\\n\\t- temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\\r\\n\\t- atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\\r\\n\\t- hum: Normalized humidity. The values are divided to 100 (max)\\r\\n\\t- windspeed: Normalized wind speed. The values are divided to 67 (max)\\r\\n\\t- casual: count of casual users\\r\\n\\t- registered: count of registered users\\r\\n\\t- cnt: count of total rental bikes including both casual and registered\\r\\n', 'citation': None}}\n          name     role         type demographic  \\\n0      instant       ID      Integer        None   \n1       dteday  Feature         Date        None   \n2       season  Feature  Categorical        None   \n3           yr  Feature  Categorical        None   \n4         mnth  Feature  Categorical        None   \n5           hr  Feature  Categorical        None   \n6      holiday  Feature       Binary        None   \n7      weekday  Feature  Categorical        None   \n8   workingday  Feature       Binary        None   \n9   weathersit  Feature  Categorical        None   \n10        temp  Feature   Continuous        None   \n11       atemp  Feature   Continuous        None   \n12         hum  Feature   Continuous        None   \n13   windspeed  Feature   Continuous        None   \n14      casual    Other      Integer        None   \n15  registered    Other      Integer        None   \n16         cnt   Target      Integer        None\n\n                                          description units missing_values  \n0                                        record index  None             no  \n1                                                date  None             no  \n2                1:winter, 2:spring, 3:summer, 4:fall  None             no  \n3                             year (0: 2011, 1: 2012)  None             no  \n4                                     month (1 to 12)  None             no  \n5                                      hour (0 to 23)  None             no  \n6   weather day is holiday or not (extracted from ...  None             no  \n7                                     day of the week  None             no  \n8   if day is neither weekend nor holiday is 1, ot...  None             no  \n9   - 1: Clear, Few clouds, Partly cloudy, Partly ...  None             no  \n10  Normalized temperature in Celsius. The values ...     C             no  \n11  Normalized feeling temperature in Celsius. The...     C             no  \n12  Normalized humidity. The values are divided to...  None             no  \n13  Normalized wind speed. The values are divided ...  None             no  \n14                              count of casual users  None             no  \n15                          count of registered users  None             no  \n16  count of total rental bikes including both cas...  None             no\n</code></pre> <pre><code>X = X.drop([\"dteday\", \"atemp\"], axis=1)\n</code></pre> <pre><code>X.head()\n</code></pre> season yr mnth hr holiday weekday workingday weathersit temp hum windspeed 0 1 0 1 0 0 6 0 1 0.24 0.81 0.0 1 1 0 1 1 0 6 0 1 0.22 0.80 0.0 2 1 0 1 2 0 6 0 1 0.22 0.80 0.0 3 1 0 1 3 0 6 0 1 0.24 0.75 0.0 4 1 0 1 4 0 6 0 1 0.24 0.75 0.0 <pre><code># load dataset\n# df = pd.read_csv(\"./../data/Bike-Sharing-Dataset/hour.csv\")\n\n# drop columns\n# df = df.drop([\"instant\", \"dteday\", \"casual\", \"registered\", \"atemp\"], axis=1)\n</code></pre> <pre><code>print(\"Design matrix shape: {}\".format(X.shape))\nprint(\"---------------------------------\")\nfor col_name in X.columns:\n    print(\"Feature: {:15}, unique: {:4d}, Mean: {:6.2f}, Std: {:6.2f}, Min: {:6.2f}, Max: {:6.2f}\".format(col_name, len(X[col_name].unique()), X[col_name].mean(), X[col_name].std(), X[col_name].min(), X[col_name].max()))\n\nprint(\"\\nTarget shape: {}\".format(y.shape))\nprint(\"---------------------------------\")\nfor col_name in y.columns:\n    print(\"Target: {:15}, unique: {:4d}, Mean: {:6.2f}, Std: {:6.2f}, Min: {:6.2f}, Max: {:6.2f}\".format(col_name, len(y[col_name].unique()), y[col_name].mean(), y[col_name].std(), y[col_name].min(), y[col_name].max()))\n</code></pre> <pre><code>Design matrix shape: (17379, 11)\n---------------------------------\nFeature: season         , unique:    4, Mean:   2.50, Std:   1.11, Min:   1.00, Max:   4.00\nFeature: yr             , unique:    2, Mean:   0.50, Std:   0.50, Min:   0.00, Max:   1.00\nFeature: mnth           , unique:   12, Mean:   6.54, Std:   3.44, Min:   1.00, Max:  12.00\nFeature: hr             , unique:   24, Mean:  11.55, Std:   6.91, Min:   0.00, Max:  23.00\nFeature: holiday        , unique:    2, Mean:   0.03, Std:   0.17, Min:   0.00, Max:   1.00\nFeature: weekday        , unique:    7, Mean:   3.00, Std:   2.01, Min:   0.00, Max:   6.00\nFeature: workingday     , unique:    2, Mean:   0.68, Std:   0.47, Min:   0.00, Max:   1.00\nFeature: weathersit     , unique:    4, Mean:   1.43, Std:   0.64, Min:   1.00, Max:   4.00\nFeature: temp           , unique:   50, Mean:   0.50, Std:   0.19, Min:   0.02, Max:   1.00\nFeature: hum            , unique:   89, Mean:   0.63, Std:   0.19, Min:   0.00, Max:   1.00\nFeature: windspeed      , unique:   30, Mean:   0.19, Std:   0.12, Min:   0.00, Max:   0.85\n\nTarget shape: (17379, 1)\n---------------------------------\nTarget: cnt            , unique:  869, Mean: 189.46, Std: 181.39, Min:   1.00, Max: 977.00\n</code></pre> <p>Feature analysis:</p> Feature Description Value Range season season 1: winter, 2: spring, 3: summer, 4: fall yr year 0: 2011, 1: 2012 mnth month 1 to 12 hr hour 0 to 23 holiday whether the day is a holiday or not 0: no, 1: yes weekday day of the week 0: Sunday, 1: Monday, \u2026, 6: Saturday workingday whether the day is a working day or not 0: no, 1: yes weathersit weather situation 1: clear, 2: mist, 3: light rain, 4: heavy rain temp temperature values in [0.02, 1.00], with mean: 0.50 and std: 0.19 hum humidity values in [0.00, 1.00], with mean: 0.63 and std: 0.19 windspeed wind speed values in [0.00, 0.85], with mean: 0.19 and std: 0.12 <p>Target variable:</p> Target Description Value Range cnt bike rentals per hour values in [1, 977] with mean: 189.46 and std: 181.39 <pre><code>def preprocess(X, y):\n    # Standarize X\n    X_df = X\n    x_mean = X_df.mean()\n    x_std = X_df.std()\n    X_df = (X_df - X_df.mean()) / X_df.std()\n\n    # Standarize Y\n    Y_df = y\n    y_mean = Y_df.mean()\n    y_std = Y_df.std()\n    Y_df = (Y_df - Y_df.mean()) / Y_df.std()\n    return X_df, Y_df, x_mean, x_std, y_mean, y_std\n\n# shuffle and standarize all features\nX_df, Y_df, x_mean, x_std, y_mean, y_std = preprocess(X, y)\n</code></pre> <pre><code>def split(X_df, Y_df):\n    # shuffle indices\n    indices = X_df.index.tolist()\n    np.random.shuffle(indices)\n\n    # data split\n    train_size = int(0.8 * len(X_df))\n\n    X_train = X_df.iloc[indices[:train_size]]\n    Y_train = Y_df.iloc[indices[:train_size]]\n    X_test = X_df.iloc[indices[train_size:]]\n    Y_test = Y_df.iloc[indices[train_size:]]\n\n    return X_train, Y_train, X_test, Y_test\n\n# train/test split\nX_train, Y_train, X_test, Y_test = split(X_df, Y_df)\n</code></pre>"},{"location":"Tutorials/real-examples/01_bike_sharing_dataset/#fit-a-neural-network","title":"Fit a Neural Network","text":"<pre><code># Train - Evaluate - Explain a neural network\nmodel = keras.Sequential([\n    keras.layers.Dense(1024, activation=\"relu\"),\n    keras.layers.Dense(512, activation=\"relu\"),\n    keras.layers.Dense(256, activation=\"relu\"),\n    keras.layers.Dense(1)\n])\n\noptimizer = keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\", keras.metrics.RootMeanSquaredError()])\nmodel.fit(X_train, Y_train, batch_size=512, epochs=20, verbose=1)\nmodel.evaluate(X_train, Y_train, verbose=1)\nmodel.evaluate(X_test, Y_test, verbose=1)\n</code></pre> <pre><code>Epoch 1/20\n28/28 [==============================] - 1s 12ms/step - loss: 0.5114 - mae: 0.5164 - root_mean_squared_error: 0.7151\nEpoch 2/20\n28/28 [==============================] - 0s 12ms/step - loss: 0.3652 - mae: 0.4330 - root_mean_squared_error: 0.6043\nEpoch 3/20\n28/28 [==============================] - 0s 13ms/step - loss: 0.2814 - mae: 0.3725 - root_mean_squared_error: 0.5304\nEpoch 4/20\n28/28 [==============================] - 0s 14ms/step - loss: 0.2116 - mae: 0.3225 - root_mean_squared_error: 0.4600\nEpoch 5/20\n28/28 [==============================] - 0s 12ms/step - loss: 0.1513 - mae: 0.2685 - root_mean_squared_error: 0.3889\nEpoch 6/20\n28/28 [==============================] - 0s 12ms/step - loss: 0.1239 - mae: 0.2445 - root_mean_squared_error: 0.3520\nEpoch 7/20\n28/28 [==============================] - 0s 13ms/step - loss: 0.0962 - mae: 0.2186 - root_mean_squared_error: 0.3101\nEpoch 8/20\n28/28 [==============================] - 0s 12ms/step - loss: 0.0762 - mae: 0.1906 - root_mean_squared_error: 0.2761\nEpoch 9/20\n28/28 [==============================] - 0s 13ms/step - loss: 0.0604 - mae: 0.1683 - root_mean_squared_error: 0.2457\nEpoch 10/20\n28/28 [==============================] - 0s 13ms/step - loss: 0.0605 - mae: 0.1700 - root_mean_squared_error: 0.2460\nEpoch 11/20\n28/28 [==============================] - 0s 13ms/step - loss: 0.0621 - mae: 0.1764 - root_mean_squared_error: 0.2492\nEpoch 12/20\n28/28 [==============================] - 0s 13ms/step - loss: 0.0588 - mae: 0.1714 - root_mean_squared_error: 0.2425\nEpoch 13/20\n28/28 [==============================] - 0s 14ms/step - loss: 0.0526 - mae: 0.1581 - root_mean_squared_error: 0.2294\nEpoch 14/20\n28/28 [==============================] - 0s 14ms/step - loss: 0.0465 - mae: 0.1469 - root_mean_squared_error: 0.2157\nEpoch 15/20\n28/28 [==============================] - 0s 13ms/step - loss: 0.0436 - mae: 0.1409 - root_mean_squared_error: 0.2088\nEpoch 16/20\n28/28 [==============================] - 0s 13ms/step - loss: 0.0408 - mae: 0.1376 - root_mean_squared_error: 0.2021\nEpoch 17/20\n28/28 [==============================] - 0s 13ms/step - loss: 0.0429 - mae: 0.1421 - root_mean_squared_error: 0.2071\nEpoch 18/20\n28/28 [==============================] - 0s 13ms/step - loss: 0.0410 - mae: 0.1367 - root_mean_squared_error: 0.2025\nEpoch 19/20\n28/28 [==============================] - 0s 13ms/step - loss: 0.0400 - mae: 0.1354 - root_mean_squared_error: 0.2001\nEpoch 20/20\n28/28 [==============================] - 0s 14ms/step - loss: 0.0483 - mae: 0.1510 - root_mean_squared_error: 0.2198\n435/435 [==============================] - 1s 1ms/step - loss: 0.0480 - mae: 0.1526 - root_mean_squared_error: 0.2190\n109/109 [==============================] - 0s 1ms/step - loss: 0.0673 - mae: 0.1757 - root_mean_squared_error: 0.2593\n\n\n\n\n\n[0.06725959479808807, 0.17573970556259155, 0.25934454798698425]\n</code></pre> <p>We train a deep fully-connected Neural Network with 3 hidden layers for \\(20\\) epochs.  The model achieves a root mean squared error on the test of about \\(0.24\\) units, that corresponds to approximately \\(0.26 * 181 = 47\\) counts.</p>"},{"location":"Tutorials/real-examples/01_bike_sharing_dataset/#explain","title":"Explain","text":"<p>We will focus on the feature <code>temp</code> (temperature) because its global effect is quite heterogeneous and the heterogeneity can be further explained using regional effects.</p> <pre><code>def model_jac(x):\n    x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n    with tf.GradientTape() as t:\n        t.watch(x_tensor)\n        pred = model(x_tensor)\n        grads = t.gradient(pred, x_tensor)\n    return grads.numpy()\n\ndef model_forward(x):\n    return model(x).numpy().squeeze()\n</code></pre> <pre><code>scale_x = {\"mean\": x_mean.iloc[3], \"std\": x_std.iloc[3]}\nscale_y = {\"mean\": y_mean, \"std\": y_std}\nscale_x_list =[{\"mean\": x_mean.iloc[i], \"std\": x_std.iloc[i]} for i in range(len(x_mean))]\nfeature_names = X_df.columns.to_list()\ntarget_name = \"bike-rentals\"\n</code></pre>"},{"location":"Tutorials/real-examples/01_bike_sharing_dataset/#global-effect","title":"Global Effect","text":"<p>We will first analyze the global effect of the feature <code>hour</code> on the target variable <code>bike-rentals</code>, using the PDP and RHALE methods.</p>"},{"location":"Tutorials/real-examples/01_bike_sharing_dataset/#pdp","title":"PDP","text":"<pre><code>pdp = effector.PDP(data=X_train.to_numpy(), model=model_forward, feature_names=feature_names, target_name=target_name, nof_instances=\"all\")\npdp.plot(feature=3, centering=False, scale_x=scale_x, scale_y=scale_y, show_avg_output=True)\n</code></pre> <pre><code>2024-01-08 14:10:10.948112: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1708400640 exceeds 10% of free system memory.\n2024-01-08 14:10:11.211370: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1708400640 exceeds 10% of free system memory.\n2024-01-08 14:10:11.428720: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1708400640 exceeds 10% of free system memory.\n</code></pre> <pre><code>pdp.plot(feature=3, heterogeneity=\"ice\", centering=True, scale_x=scale_x, scale_y=scale_y, nof_ice=300, show_avg_output=True)\n</code></pre> <pre><code>2024-01-08 14:10:13.477440: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5694668800 exceeds 10% of free system memory.\n2024-01-08 14:10:14.355351: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5694668800 exceeds 10% of free system memory.\n</code></pre>"},{"location":"Tutorials/real-examples/01_bike_sharing_dataset/#rhale","title":"RHALE","text":"<pre><code>rhale = effector.RHALE(data=X_train.to_numpy(), model=model_forward, model_jac=model_jac, feature_names=feature_names, target_name=target_name)\nrhale.fit(features=3, binning_method=effector.binning_methods.Greedy(init_nof_bins=100, min_points_per_bin=100, discount=1., cat_limit=10))\nrhale.plot(feature=3, centering=True, scale_x=scale_x, scale_y=scale_y, show_avg_output=True)\n</code></pre> <pre><code>Degrees of freedom &lt;= 0 for slice\ninvalid value encountered in divide\ninvalid value encountered in divide\n</code></pre> <pre><code>rhale.plot(feature=3, heterogeneity=\"std\", centering=True, scale_x=scale_x, scale_y=scale_y, show_avg_output=True)\n</code></pre>"},{"location":"Tutorials/real-examples/01_bike_sharing_dataset/#conclusion","title":"Conclusion","text":"<p>The global effect of feature <code>hour</code> on the target variable <code>bike-rentals</code> shows two high peaks, one at around 8:00 and another at around 17:00, which probably corresponds to the morning and evening commute hours of the working days. However, the effect is quite heterogeneous. For this reason, we will analyze the regional effects which may explain the underlying heterogeneity.</p>"},{"location":"Tutorials/real-examples/01_bike_sharing_dataset/#regional-effect","title":"Regional Effect","text":""},{"location":"Tutorials/real-examples/01_bike_sharing_dataset/#regionalrhale","title":"RegionalRHALE","text":"<pre><code># Regional RHALE\nregional_rhale = effector.RegionalRHALE(\n    data=X_train.to_numpy(),\n    model=model_forward,\n    model_jac=model_jac,\n    cat_limit=10,\n    feature_names=feature_names,\n    nof_instances=\"all\"\n)\n\nregional_rhale.fit(\n    features=3,\n    heter_small_enough=0.1,\n    heter_pcg_drop_thres=0.2,\n    binning_method=effector.binning_methods.Greedy(init_nof_bins=100, min_points_per_bin=100, discount=1., cat_limit=10),\n    max_depth=2,\n    nof_candidate_splits_for_numerical=10,\n    min_points_per_subregion=10,\n    candidate_conditioning_features=\"all\",\n    split_categorical_features=True,\n)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:16&lt;00:00, 16.81s/it]\n</code></pre> <pre><code>regional_rhale.show_partitioning(features=3, only_important=True, scale_x_list=scale_x_list)\n</code></pre> <pre><code>Feature 3 - Full partition tree:\nNode id: 0, name: hr, heter: 6.02 || nof_instances: 13903 || weight: 1.00\n        Node id: 1, name: hr | workingday == 0.0, heter: 2.33 || nof_instances:  4385 || weight: 0.32\n        Node id: 2, name: hr | workingday != 0.0, heter: 4.54 || nof_instances:  9518 || weight: 0.68\n--------------------------------------------------\nFeature 3 - Statistics per tree level:\nLevel 0, heter: 6.02\n        Level 1, heter: 3.84 || heter drop: 2.18 (36.19%)\n</code></pre> <pre><code>regional_rhale.plot(feature=3, node_idx=1, heterogeneity=True, centering=True, scale_x_list=scale_x_list, scale_y=scale_y)\nregional_rhale.plot(feature=3, node_idx=2, heterogeneity=True, centering=True, scale_x_list=scale_x_list, scale_y=scale_y)\n</code></pre>"},{"location":"Tutorials/real-examples/01_bike_sharing_dataset/#regionalpdp","title":"RegionalPDP","text":"<pre><code>regional_pdp = effector.RegionalPDP(\n    data=X_train.to_numpy(),\n    model=model_forward,\n    cat_limit=10,\n    feature_names=feature_names,\n    nof_instances=\"all\"\n)\n\nregional_pdp.fit(\n    features=3,\n    heter_small_enough=0.1,\n    heter_pcg_drop_thres=0.1,\n    max_depth=2,\n    nof_candidate_splits_for_numerical=5,\n    min_points_per_subregion=10,\n    candidate_conditioning_features=\"all\",\n    split_categorical_features=True,\n    nof_instances=1000\n)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:15&lt;00:00, 15.85s/it]\n</code></pre> <pre><code>regional_pdp.show_partitioning(features=3, only_important=True, scale_x_list=scale_x_list)\n</code></pre> <pre><code>Feature 3 - Full partition tree:\nNode id: 0, name: hr, heter: 0.57 || nof_instances: 13903 || weight: 1.00\n        Node id: 1, name: hr | workingday == 1.0, heter: 0.43 || nof_instances:  9518 || weight: 0.68\n                Node id: 3, name: hr | workingday == 1.0 and season == 1.0, heter: 0.29 || nof_instances:  2245 || weight: 0.16\n                Node id: 4, name: hr | workingday == 1.0 and season != 1.0, heter: 0.38 || nof_instances:  7273 || weight: 0.52\n        Node id: 2, name: hr | workingday != 1.0, heter: 0.46 || nof_instances:  4385 || weight: 0.32\n                Node id: 5, name: hr | workingday != 1.0 and season == 1.0, heter: 0.27 || nof_instances:  1140 || weight: 0.08\n                Node id: 6, name: hr | workingday != 1.0 and season != 1.0, heter: 0.39 || nof_instances:  3245 || weight: 0.23\n--------------------------------------------------\nFeature 3 - Statistics per tree level:\nLevel 0, heter: 0.57\n        Level 1, heter: 0.44 || heter drop: 0.13 (22.76%)\n                Level 2, heter: 0.36 || heter drop: 0.08 (17.64%)\n</code></pre> <pre><code>regional_pdp.plot(feature=3, node_idx=1, heterogeneity=\"ice\", centering=True, scale_x_list=scale_x_list, scale_y=scale_y)\n</code></pre> <pre><code>regional_pdp.plot(feature=3, node_idx=2, heterogeneity=\"ice\", centering=True, scale_x_list=scale_x_list, scale_y=scale_y)\n</code></pre>"},{"location":"Tutorials/real-examples/01_bike_sharing_dataset/#conclusion_1","title":"Conclusion","text":"<p>The both PDP and RHALE regional effect reveal two distinct explanations; one for the working days and another for the non-working days. For the working days, the effect is quite similar to the global effect (unfortunately, working ways dominate our life), with two high peaks at around 8:00 and 17:00. However, for the non-working days, the effect is quite different, with a single high peak at around 13:00 which probably corresponds to sightseeing and leisure activities.</p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/","title":"Intro to Feature Effect methods with a linear model","text":"<p>This tutorial is a (slow and gentle) introduction to the basic global feature effect methods and the <code>Effector</code> package. If you only care about using <code>Effector</code>'s API, you can go directly to the Conclusion.</p> <pre><code>import numpy as np\nimport effector\n</code></pre>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#global-feature-effect-methods","title":"Global Feature Effect methods","text":"<p>Feature effect methods estimate the average effect of a specific feature on the model's output, i.e., they create a 1-1 mapping between the feature of interest \\(x_s\\) and the output of the model \\(y\\). </p> <p>Feature effect are perfect explainers for additive models, i.e., models of the form \\(f(\\mathbf{x}) = \\sum_{i=1}^D f(x_i)\\). Black-box models, however, are not additive; they have complex interaction terms between two \\(f(x_i, x_j)\\), three \\(f(x_i, x_j, x_k)\\) or even all features \\(f(x_1, \\cdots, x_D)\\). In these cases, feature effect methods simplify things by distributing the effect of interaction terms to the individual features. </p> <p>This simplification is acceptable when the interaction terms are weak, i.e., they are not so important for the model's prediction. However, when the interaction terms are very strong then the feature effect methods may provide an over-simplistic explanation. Hopefully, there is a quantity called heterogeneity that can be used to check whether the feature effect methods are a good explanation for the features.</p> <p><code>Effector</code> provides five different feature effect methods, which are summarized in the table below. In all methods, setting <code>heterogeneity=True</code> the methods show the level of heterogeneity, along with the average effect.</p> <p> Method Description API in <code>Effector</code> Paper PDP Partial Dependence Plot PDP Friedman, 2001 d-PDP Derivative PDP DerivativePDP Goldstein et. al, 2013 ALE Accumulated Local Effect ALE Apley et. al, 2016 RHALE Robust and Heterogeneity-aware ALE RHALE Gkolemis et al, 2023 SHAP SHAP Dependence Plot SHAPDependence Lundberg et. al, 2017 <p></p> <p>For the rest of the tutorial, we will use the following notation for the rest of the tutorial:</p> <p> Symbol Description \\(f(\\mathbf{x})\\) The black box model \\(\\mathcal{H}\\) The heterogeneity \\(x_s\\) The feature of interest \\(x_c\\) The remaining features, i.e., \\(\\mathbf{x} = (x_s, x_c)\\) \\(\\mathbf{x} = (x_s, x_c) = (x_1, x_2, ..., x_s, ..., x_D)\\) The input features \\(\\mathbf{x}^{(i)} = (x_s^{(i)}, x_c^{(i)})\\) The \\(i\\)-th instance of the dataset <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#dataset-and-model","title":"Dataset and Model","text":"<p>In this example, we will use as black-box function, a simple linear model \\(y = 7x_1 - 3x_2 + 4x_3\\). Since there are no interactions  terms we expect all methods to provide the following feature effects and zero heterogeneity:</p> <p> Feature Feature Effect Heterogeneity \\(x_1\\) \\(7x_1\\) 0 \\(x_2\\) \\(-3x_2\\) 0 \\(x_3\\) \\(4x_3\\) 0 <p></p> <p>As dataset, we will generate \\(N=1000\\) examples comming from the following distribution:</p> Feature Description Distribution \\(x_1\\) Uniformly distributed between \\(0\\) and \\(1\\) \\(x_1 \\sim \\mathcal{U}(0,1)\\) \\(x_2\\) Follows \\(x_1\\) with some added noise $x_2 = x_1 + \\epsilon $, \\(\\epsilon \\sim \\mathcal{N}(0, 0.1)\\) \\(x_3\\) Uniformly distributed between \\(0\\) and \\(1\\) \\(x_3 \\sim \\mathcal{U}(0,1)\\) <pre><code>def generate_dataset(N, x1_min, x1_max, x2_sigma, x3_sigma):\n    x1 = np.random.uniform(x1_min, x1_max, size=int(N))\n    x2 = np.random.normal(loc=x1, scale=x2_sigma)\n    x3 = np.random.uniform(x1_min, x1_max, size=int(N))\n    return np.stack((x1, x2, x3), axis=-1)\n\n# generate the dataset\nnp.random.seed(21)\n\nN = 1000\nx1_min = 0\nx1_max = 1\nx2_sigma = .1\nx3_sigma = 1.\nX = generate_dataset(N, x1_min, x1_max, x2_sigma, x3_sigma)\n</code></pre> <pre><code>def predict(x):\n    y = 7*x[:, 0] - 3*x[:, 1] + 4*x[:, 2]\n    return y\n\ndef predict_grad(x):\n    df_dx1 = 7 * np.ones([x.shape[0]])\n    df_dx2 = -3 * np.ones([x.shape[0]])\n    df_dx3 = 4 * np.ones([x.shape[0]])\n    return np.stack([df_dx1, df_dx2, df_dx3], axis=-1)\n</code></pre>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#partial-dependence-plot-pdp","title":"Partial Dependence Plot (PDP)","text":"<p>The PDP is defined as the average prediction over the entire dataset when setting the feature of interest at a specific value. For example, the effect of the \\(s\\)-th feature at values \\(x_s\\) is defined as:</p> \\[ \\text{PDP}(x_s) = \\mathbb{E}_{x_c}[f(x_s, x_c)] \\] <p>and is approximated by </p> \\[ \\hat{\\text{PDP}}(x_s) = \\frac{1}{N} \\sum_{j=1}^N f(x_s, x^{(i)}_c) \\] <p>In practice, for all the dataset instances, we set the feature of interest at a specific value \\(x_s\\) and we average the model's predictions. Let's check it out the PDP effect using <code>effector</code>.</p> <pre><code>effector.PDP(data=X, model=predict).plot(feature=0, show_avg_output=True, y_limits=[0,10])\neffector.PDP(data=X, model=predict).plot(feature=1, show_avg_output=True, y_limits=[0,10])\neffector.PDP(data=X, model=predict).plot(feature=2, show_avg_output=True, y_limits=[0,10])\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#feature-effect-interpretation","title":"Feature effect interpretation","text":"<p>As we expected, all feature effects are linear. Looking closer, we can also confirm the gradients of the effects:  \\(7\\) for \\(x_1\\), \\(-3\\) for \\(x_2\\) and \\(4\\) for \\(x_3\\). However, you may question why there are different alignments on the y axis? For example, why \\(x_1\\) starts at \\(y=-1.5\\)? Does this have a natural interpretation?</p> <p>There is no global answer to this question. This is why many people prefer to center PDP plots manually, as we will see below. For linear models, the PDP plot is \\(\\text{PDP}(x_s) = a_sx_s + c\\) where \\(a_s\\) is the gradient of the line and \\(c\\) is the intercept.  For feature \\(x_1\\), the intercept is \\(c \\approx 0.5\\).  With a closer look at the formula we can understand why this happens:</p> \\[PDP(x_s) = \\mathbb{E}_{x_c}[f(x_s, x_c)] = a_sx_s + \\sum_{j \\neq s} a_j \\mathbb{E}_{x_j}[x_j] = a_sx_s - 3 * 0.5 + 4 * 0.5 = a_sx_s + 0.5\\] <p>The most convenient centering of the PDP plot depends on the underlying question. If we compare the effect of two features, then it is better to center the PDP plots around \\(y=0\\) to avoid the distraction of intercepts. If we compare the effect of a specific feature on two different subgroups (check the tutorial about Regional Effect methods), then it is better to to leave the PDP plot uncentered.</p> <p><code>Effector</code> has three <code>cenetering</code> alternatives:</p> <code>centering</code> Description Formula <code>False</code> Don't enforce any additional centering - <code>True</code> or <code>zero_integral</code> Center around the \\(y\\) axis \\(c = \\mathbb{E}_{x_s \\sim \\mathcal{U(x_{s,min},x_{s, max})}}[PDP(x_s)]\\) <code>zero_start</code> Center around \\(y=0\\) \\(c = 0\\) <p>Below, we observe that setting <code>centering=True</code> facilitates the comparisons. </p> <pre><code>effector.PDP(data=X, model=predict).plot(feature=0, centering=True, y_limits=[-4, 4])\neffector.PDP(data=X, model=predict).plot(feature=1, centering=True, y_limits=[-4, 4])\neffector.PDP(data=X, model=predict).plot(feature=2, centering=True, y_limits=[-4, 4])\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#heterogeneity","title":"Heterogeneity","text":"<p>Feature effect methods output a 1-1 plot that visualizes the average effect of a specific feature on the output;  the averaging is performed over the instance-level effects.  It is important, therefore, to know to what extent the underlying local (instance-level) effects deviate from the average effect. In other words, to what extent the average effects are a good explanation for the features.</p> <p>In our example, due to zero interactions between the features, the heterogeneity should be zero.</p> <p>In PDP plots there are two ways to check that, either using the ICE plots or as a \\(\\pm\\) interval around the average plot.</p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#option-a-ice-plots","title":"Option (a): ICE plots","text":"<p>ICE plots show the ouput of instance \\(i\\) if changing the feature of interest \\(x_s\\):</p> \\[\\text{ICE}^{(i)}(x_s, x^{(i)}_c) = f(x_s, x^{(i)}_c)\\] <p>Plotting the ICE plots of many instances \\(i\\) on top of the PDP, we can visually observe the heterogeneity. For example in the plot below, we can see that there is no heterogeneity in the instance-level effects, i.e., all instance-level effects are lines with gradient 7.</p> <pre><code>effector.PDP(data=X, model=predict).plot(feature=0, centering=True, heterogeneity=\"ice\")\n</code></pre> <p></p> <p>Keep in mind that it is important to define correctly the argument <code>centering</code>. Setting <code>centering=True</code> centers the PDP plot around the \\(y\\) axis, which facilitates the comparison of the underlying feature effects (as above).</p> <p>However, there are cases where the intercept maybe useful. Imagine a case where the salary of the employees depends only (a) on the number of working hours and (b) on the gender of a person.  If the salary per working hour does not differ between male and female employees, but male employees in general earn 1000 Euros more per month,  then we won\u2019t see this difference in the centered ICE curves of the feature working hours. In contrast, this difference will be visible under the uncentered ICE curves.</p> <p>In our example, setting <code>centering=False</code> gives the following plot; ICE plots with different intercepts but identical gradient.</p> <pre><code>effector.PDP(data=X, model=predict).plot(feature=0, centering=False, heterogeneity=\"ice\")\n</code></pre> <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#option-b-std-of-the-residuals","title":"Option (b): STD of the residuals","text":"<p>A second way to check for heterogeneity is by plotting the standard deviation of the instance-level effects as \\(\\pm\\) interval around the PDP plot. This is done setting <code>confidence_interval=\"std\"</code> in the <code>plot</code> method.  In practice, this approach simply plots the std of the ICE plots instead of the ICE plots themselves.</p> <pre><code>effector.PDP(data=X, model=predict).plot(feature=0, centering=True, heterogeneity=\"std\")\n</code></pre> <p></p> <p>In this case, if we do not perform centering, it is difficult to differentiate whether the heterogeneity is provoked by the gradient or in the intercept. Therefore, we recommend to try both the centered and the uncentered version of the ICE, before coming to a conclusion.  If the heterogeneity is only present on the latter, then it is due to different intercepts.</p> <pre><code>effector.PDP(data=X, model=predict).plot(feature=0, centering=False, heterogeneity=\"std\")\n</code></pre> <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#derivative-pdp-d-pdp","title":"Derivative-PDP (d-PDP)","text":"<p>A similar analysis can be done using the derivative of the model; the name of this approach is Derivative-PDP (d-PDP) and the equivalent of the ICE plots are the Derivative-ICE (d-ICE) plots. The d-PDP and d-ICE are defined as:</p> \\[ \\text{d-PDP}(x_s) = \\mathbb{E}_{x_c}[\\frac{\\partial f}{\\partial x_s} (x_s, x_c)] \\approx \\frac{1}{N} \\sum_{j=1}^N \\frac{\\partial f}{\\partial x_s} (x_s, x_c^{(i)}) \\] <p>and </p> \\[ \\text{d-ICE}^{(i)}(x_s) = \\frac{\\partial f}{\\partial x_s} (x_s, x^{(i)}_c) \\] <p>We have to mention that:</p> <ul> <li>d-PDP needs the model's gradient, which is not always available.</li> <li>Under normal circumstances, the d-PDP and d-ICE should not be centered because the absolute value of the derivative has a natural meaning for the interpretation. In practice, d-ICE plots show variation that is only due to difference in the shapes of the curves. This is because all terms that are not related (interact) with the feature of interest will become zero when taking the derivative. The same applies for the \\(\\pm\\) interval around the d-PDP plot.</li> <li>The interpretation is given in the gradient-space, so it should be treated differently. In d-PDP the plots show how much the model's prediction changes given a change in the feature of interest. This is different from PDP, where the plots says how much the specific feature contributes to the prediction. </li> <li>d-PDP is the gradient of the PDP, i.e., \\(\\text{d-PDP}(x) = \\frac{\\partial \\text{PDP}}{\\partial x_s} (x)\\)</li> <li>d-ICE is the gradient of the ICE, i.e., \\(\\text{d-ICE}^{(i)}(x) = \\frac{\\partial \\text{ICE}^{(i)}}{\\partial x_s} (x)\\)</li> </ul> <p>As we can see below, the standard deviation of the ICE plots is zero, because they only measure the variation of the shapes of the curves; not the variation of the intercepts.</p> <pre><code>effector.DerivativePDP(data=X, model=predict, model_jac=predict_grad).plot(feature=0, heterogeneity=True)\neffector.DerivativePDP(data=X, model=predict, model_jac=predict_grad).plot(feature=0, heterogeneity=\"ice\")\n</code></pre> <p></p> <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#accumulated-local-effects-ale","title":"Accumulated Local Effects (ALE)","text":"<p>The next major category of feature effect techniques is Accumulated Local Effects (ALE). Before we go into the specifics, let's apply the ALE plot to our example.</p> <pre><code>effector.ALE(data=X, model=predict).plot(feature=0)\neffector.ALE(data=X, model=predict).plot(feature=1)\neffector.ALE(data=X, model=predict).plot(feature=2)\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#fearure-effect-interpretation","title":"Fearure effect interpretation","text":"<p>In each of the figures above, there are two subfigures; the upper subfigure is the average feature effect (the typical ALE plot) and the lower subfigure is the derivative of the effect. The upper subfigure shows how much the feature of interest contributes to the prediction (like PDP) while the bottom subplot shows how much a change in the feature of interest changes the prediction (like d-PDP).  For example, for \\(x_1\\) the upper subplot shows a linear effect and the lower subplot confirms that the gradient is constantly \\(7\\). <code>Effector</code> offers two alternatives for centering the ALE plot.</p> <p> <code>centering</code> Description Formula <code>False</code> or <code>zero_start</code> Don't enforce any additional centering c=0 <code>True</code> or <code>zero_integral</code> Center around the \\(y\\) axis c=\\(\\mathbb{E}_{x_s \\sim \\mathcal{U(x_{s,min},x_{s, max})}}[ALE(x_s)]\\) <p> Let's see how centering works for \\(x_1\\):</p> <pre><code>effector.ALE(data=X, model=predict).plot(feature=0, centering=True)\n</code></pre> <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#heterogeneity_1","title":"Heterogeneity","text":"<p>In ALE plots, the only way to check the heterogeneity of the instance-level effects is by plotting the standard deviation of the instance-level effects as \\(\\pm\\) interval around the ALE plot. In <code>Effector</code> this can be done by setting <code>heterogeneity=True\"</code>. The plot below shows that the heterogeneity is zero, which is correct. However, as we will see below (RHALE section), ALE's fixed size bin-splitting is not the best way to estimate the heterogeneity. In contrast, the automatic bin-splitting introduced by RHALE provides a better estimation of the heterogeneity.</p> <pre><code>effector.ALE(data=X, model=predict).plot(feature=0, centering=True, heterogeneity=True)\n</code></pre> <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#bin-splitting","title":"Bin-Splitting","text":"<p>As you may have noticed at the bottom plots of the figures above, \\(x_1\\) axis has been split in \\(K=20\\) bins (intervals) of equal size. The derivative-effect is provided per bin (bin-effect), which in our example is \\(7\\) for all bins. </p> <p>In fact, bin-splitting is apparent also at the top plot; the top plot is not a line, but a piecewise linear function, where each piece is a line in the are covered by each bin and gradient equal to the bin-effect. However, since the bin-effect is the same for all bins, the top plot looks like a line.</p> <p>To explain the need for bin-splitting we have to go back to the definition of ALE. ALE is defined as: </p> \\[\\text{ALE}(x_s) = \\int_{z=0}^{x_s} \\mathbb{E}_{x_c|x_s=z}\\left [ \\frac{\\partial f}{\\partial x_s} (z, x_c) \\right ] \\partial z\\] <p>Apley et. al proposed approximating the above integral by:</p> \\[\\hat{\\text{ALE}}(x_s) = \\sum_{k=1}^{k_{x_s}} \\frac{1}{| \\mathcal{S}_k |} \\sum_{i: x^{(i)} \\in \\mathcal{S}_k} \\left [ f(z_k, x_c) - f(z_{k-1}, x_c) \\right ]\\] <p>where \\(k_{x_s}\\) the index of the bin such that \\(z_{k_{x\u22121}} \u2264 x_s &lt; z_{k_x}\\), \\(\\mathcal{S}_k\\) is the set of the instances lying at the \\(k\\)-th bin, i.e., \\(\\mathcal{S}_k = \\{ x^{(i)} : z_{k\u22121} \\neq x^{(i)}_s &lt; z_k \\}\\) and \\(\\Delta x = \\frac{x_{s, max} - x_{s, min}}{K}\\).</p> <p>\\(\\hat{\\text{ALE}}(x_s)\\) uses a Riemannian sum to approximate the integral of \\(\\text{ALE}(x_s)\\). The axis of the \\(s\\)-th feature is split in \\(K\\) bins (intervals) of equal size. In each bin, the average effect of the feature of interest is estimated using the instances that fall in the bin. The average effect in each bin is called bin-effect. The default in <code>Effector</code> is to use \\(K=20\\) bins but the user can change it using:</p> <pre><code>ale = effector.ALE(data=X, model=predict)\n\n# using 5 bins\nbm = effector.binning_methods.Fixed(nof_bins=5, min_points_per_bin=0, cat_limit=10)\nale.fit(features=0, binning_method=bm)\nale.plot(feature=0)\n\n# using 100 bins\nbm = effector.binning_methods.Fixed(nof_bins=100, min_points_per_bin=0, cat_limit=10)\nale.fit(features=0, binning_method=bm)\nale.plot(feature=0)\n</code></pre> <p></p> <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#robust-and-heterogeneity-aware-ale-rhale","title":"Robust and Heterogeneity-aware ALE (RHALE)","text":"<p>Robust and Heterogeneity-aware ALE (RHALE) is a variant of ALE, proposed by Gkolemis et. al. In their paper, they showed that RHALE has specific advantages over ALE: (a) it ensures on-distribution sampling (b) an unbiased estimation of the heterogeneity and (c) an optimal trade-off between bias and variance. These are achieved using an automated variable-size binning splitting approach. Let's see how it works in practice.</p> <pre><code>effector.RHALE(data=X, model=predict, model_jac=predict_grad).plot(feature=0, centering=False, show_avg_output=False)\n</code></pre> <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#fearure-effect-interpretation_1","title":"Fearure effect interpretation","text":"<p>The interpretation is exactly the same as with the typical ALE; The top subplot is the average feature effect and the bottom subfigure is the derivative of the effect.  The crucial difference, is that the automatic bin-splitting approach optimally creates a single bin that covers the whole area between \\(x=0\\) and \\(x=1\\). As we saw above, the gradient of the feature effect is constant and equal to \\(7\\) for all \\(x_1\\) values. Therefore, merging all bins into one, reduces the variance of the estimation; the estimation is based on more instances, so the variance is lower. </p> <p>In our example, this advantage is not evident; Since there are no interaction terms (linear model) the effect of all instances is always the same; so the variance of the estimation is zero. However, in more complex models, the variance of the estimation is not zero and the automatic bin-splitting approach reduces the variance of the estimation (check tutorial ALE for more details).</p> <p>As with the ALE, there are two alternatives for centering the ALE plot.</p> <p> <code>centering</code> Description Formula <code>False</code> or <code>zero_start</code> Don't enforce any additional centering c=0 <code>True</code> or <code>zero-integral</code> Center around the \\(y\\) axis c=\\(\\mathbb{E}_{x_s \\sim \\mathcal{U(x_{s,min},x_{s, max})}}[ALE(x_s)]\\) <p></p> <p>Let's see how this works for \\(x_1\\):</p> <pre><code>effector.RHALE(data=X, model=predict, model_jac=predict_grad).plot(feature=0, centering=True, show_avg_output=False)\n</code></pre> <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#heterogeneity_2","title":"Heterogeneity","text":"<p>As before, the heterogeneity is given by the the standard deviation of the instance-level effects as \\(\\pm\\) interval around the ALE plot. It is important to notice, that automatic bin-splitting provides a better estimation of the heterogeneity, compared to the equisized binning method used by ALE. (check tutorial ALE for more details).  The plot below correctly informs shows that the heterogeneity is zero.</p> <pre><code>effector.RHALE(data=X, model=predict, model_jac=predict_grad).plot(feature=0, centering=True, heterogeneity=\"std\", show_avg_output=False)\n</code></pre> <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#bin-splitting_1","title":"Bin-Splitting","text":"<p>So how the automatic bin-splitting works? </p> \\[\\text{ALE}(x_s) = \\int_{z=0}^{x_s} \\mathbb{E}_{x_c|x_s=z}\\left [ \\frac{\\partial f}{\\partial x_s} (z, x_c) \\right ] \\partial z\\] <p>and is approximated by:</p> \\[\\hat{\\text{RHALE}}(x_s) = \\sum_{k=1}^{k_{x_s}} \\frac{1}{ \\left | \\mathcal{S}_k \\right |} \\sum_{i: x^{(i)} \\in \\mathcal{S}_k} \\frac{\\partial f}{\\partial x_s} (x_s^{(i)}, x_c^{(i)})\\] <p>The above approximation uses a Riemannian sum to approximate the integral. The axis of the \\(s\\)-th feature is split in \\(K\\) bins (intervals) of equal size. In each bin, the average effect of the feature of interest is estimated using the instances that fall in the bin. The average effect in each bin is called bin-effect. </p> <p>But what we saw above is different. In the figure above, only one bin has been created and covers the whole area between \\(x=0\\) and \\(x=1\\).  This is because the default behaviour of <code>Effector</code> is to use an automatic bin-splitting method, as it was proposed by Gkolemis et. al. For more details about that, you can check the in-depth ALE tutorial.</p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#shap-dependence-plot","title":"SHAP Dependence Plot","text":"<p>TODO add intro</p> <pre><code>effector.SHAPDependence(data=X, model=predict).plot(feature=0, centering=False, show_avg_output=False)\n</code></pre> <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#fearure-effect-interpretation_2","title":"Fearure effect interpretation","text":"<p>TODO add content </p> <p> <code>centering</code> Description Formula <code>False</code> or <code>zero_start</code> Don't enforce any additional centering c=0 <code>True</code> or <code>zero-integral</code> Center around the \\(y\\) axis c=\\(\\mathbb{E}_{x_s \\sim \\mathcal{U(x_{s,min},x_{s, max})}}[ALE(x_s)]\\) <p></p> <p>Let's see how this works for \\(x_1\\):</p> <pre><code>effector.SHAPDependence(data=X, model=predict).plot(feature=0, centering=True, show_avg_output=False)\n</code></pre> <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#heterogeneity_3","title":"Heterogeneity","text":"<p>As before, the heterogeneity is given by the the standard deviation of the instance-level effects as \\(\\pm\\) interval around the ALE plot. It is important to notice, that automatic bin-splitting provides a better estimation of the heterogeneity, compared to the equisized binning method used by ALE. (check tutorial ALE for more details).  The plot below correctly informs shows that the heterogeneity is zero.</p> <pre><code>effector.SHAPDependence(data=X, model=predict).plot(feature=0, heterogeneity=\"shap_values\")\neffector.SHAPDependence(data=X, model=predict).plot(feature=0, heterogeneity=\"std\")\n</code></pre> <p></p> <p></p>"},{"location":"Tutorials/synthetic-examples/01_linear_model/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we introduced the various feature effect methods of <code>Effector</code> and used them to explain a linear model. </p> <p>In summary, given a dataset <code>X: (N, D)</code> and a black-box model <code>model: (N, D) -&gt; (N)</code>, the feature effect plot of the \\(s\\)-th feature <code>feature=s</code> is given with the table below. The argument <code>confidence_interval=True|False</code> indicates whether to plot the standard deviation of the instance-level effects as \\(\\pm\\) interval around the feature effect plot. Some methods also require the gradient of the model <code>model_jac: (N, D) -&gt; (N, D)</code>.</p> <p> Method How to use PDP <code>effector.PDP(X, model).plot(feature, centering, confidence_interval)</code> d-PDP <code>effector.DerivativePDP(X, model, model_jac).plot(feature, centering, confidence_interval)</code> ALE <code>effector.ALE(X, model).plot(feature, centering, confidence_interval)</code> RHALE <code>effector.RHALE(X, model, model_jac).plot(feature, centering, confidence_interval)</code> <p></p> <pre><code>\n</code></pre>"},{"location":"Tutorials/synthetic-examples/02_rhale_vs_ale_vs_pdp/","title":"Global Effect - Methods Comparison","text":"<p>In this tutorial, we will compare all the global effect methods implemented in <code>Effector</code>, namely: ALE, RHALE, PDP-ICE, d-PDP-ICE and SHAP Dependence Plots.  The synthetic example that we will used, was introduced by (Gkolemis et. al, 2023).</p> <pre><code>import numpy as np\nimport effector\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"Tutorials/synthetic-examples/02_rhale_vs_ale_vs_pdp/#problem-setup","title":"Problem setup","text":"<p>We will generate \\(N=60\\) examples with \\(D=3\\) features, as described in the following table. Observe that \\(x_3\\) is highly-dependent on \\(x_1\\), i.e., \\(x_3 \\approx x_1\\).; this will later help us to compute the ground truth ALE effect.</p> Feature Description Distribution \\(x_1\\) \\(x_1\\) lies in \\([-0.5, 0.5]\\) with most samples in \\([-0.5, 0]\\) \\(x_1 \\sim p(x_1) = \\frac{5}{6} \\mathcal{U}(x_1; -0.5, 0) + \\frac{1}{6} \\mathcal{U}(x_1; 0, 0.5)\\) \\(x_2\\) Normally distributed with \\(\\mu = 0\\), \\(\\sigma = 2\\) \\(x_2 \\sim p(x_2) = \\mathcal{N}(x_2; \\mu=0, \\sigma = 2)\\) \\(x_3\\) \\(x_3 = x_1 + \\delta\\), where \\(\\delta \\sim \\mathcal{N}(0, \\sigma=0.01)\\) \\(x_3 = x_1 + \\delta\\) <pre><code>def generate_samples(N1, N2, sigma_2=1, sigma_3=.01):\n    N = N1 + N2\n    x1 = np.concatenate((np.array([-0.5]),\n                         np.random.uniform(-0.5, 0, size=int(N1 - 2)),\n                         np.array([-0.00001]),\n                         np.array([0.]),\n                         np.random.uniform(0, 0.5, size=int(N2 - 2)),\n                         np.array([0.5])))\n    x2 = np.random.normal(0, sigma_2, N)\n    x3 = x1 + np.random.normal(0, sigma_3, N)\n    x = np.stack([x1, x2, x3], -1)\n    return x\n\n\nnp.random.seed(seed=2121)\naxis_limits = np.array([[-.5, .5], [-5, 5], [-.5, .5]]).T\nsigma_2 = 2\nsigma_3 = .01\nN1 = 150\nN2 = 20\nx = generate_samples(N1, N2, sigma_2=sigma_2, sigma_3=sigma_3)\n</code></pre> <p>The black-box function is:</p> \\[ f(x) = \\sin(2\\pi x_1) (\\mathbb{1}_{x_1&lt;0} - 2 \\mathbb{1}_{x_3&lt;0}) + x_1 x_2 + x_2 \\] <p>For estimating RHALE we also need the jacobian of \\(f\\):</p> \\[ \\frac{\\partial f}{\\partial x} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\frac{\\partial f}{\\partial x_3} \\end{bmatrix} = \\begin{bmatrix} 2\\pi x_1 \\cos(2\\pi x_1) (\\mathbb{1}_{x_1&lt;0} - 2 \\mathbb{1}_{x_3&lt;0}) + x_2 \\\\ x_1 + 1 \\\\ 0 \\end{bmatrix} \\] <pre><code>def f(x):\n\"\"\"Evaluate function:\n    y = sin(2*pi*x1)*(if x1&lt;0) - 2*sin(2*pi*x1)*(if x3&lt;0) + x1*x2 + x2\n\n    \"\"\"\n    y = np.zeros_like(x[:,0])\n\n    ind = np.logical_and(x[:, 0] &gt;= -2, x[:, 0] &lt; 0)\n    y[ind] = np.sin(2 * np.pi * x[ind, 0])\n\n    ind = np.logical_and(x[:, 2] &gt;= -2, x[:, 2] &lt; 0)\n    y[ind] -= 2 * np.sin(2 * np.pi * x[ind, 0])\n\n    y += x[:, 0] * x[:, 1] + x[:, 1]\n    return y\n</code></pre> <pre><code>def dfdx(x):\n\"\"\"Evaluate jacobian of:\n    y = sin(2*pi*x1)*(if x1&lt;0) - 2*sin(2*pi*x1)*(if x3&lt;0) + x1*x2 + x2\n\n    dy/dx1 = 2*pi*x1*cos(2*pi*x1)*(if x1&lt;0) - 4*pi*x1*cos(2*pi*x1)*(if x3&lt;0) + x2\n    dy/dx2 = x1 + 1\n    dy/dx3 = 0\n    \"\"\"\n\n    dydx = np.zeros_like(x)\n\n    ind = np.logical_and(x[:, 0] &gt;= -2, x[:, 0] &lt;= 0)\n    dydx[ind, 0] = 2 * np.pi * np.cos(2*np.pi * x[ind, 0])\n\n    ind = np.logical_and(x[:, 2] &gt;= -2, x[:, 2] &lt;= 0)\n    dydx[ind, 0] += - 2 * 2 * np.pi * np.cos(2*np.pi * x[ind, 0])\n\n    dydx[:, 0] += x[:, 1]\n\n    dydx[:, 1] = x[:, 0] + 1\n    return dydx\n</code></pre>"},{"location":"Tutorials/synthetic-examples/02_rhale_vs_ale_vs_pdp/#ale-based-methods","title":"ALE-based methods","text":""},{"location":"Tutorials/synthetic-examples/02_rhale_vs_ale_vs_pdp/#ale-definition","title":"ALE Definition","text":"<p>ALE defines the feature effect as:</p> \\[\\text{ALE}(x_s) = \\int_{z=0}^{x_s} \\mathbb{E}_{x_c|x_s=z}\\left [ \\frac{\\partial f}{\\partial x_s} (z, x_c) \\right ] \\partial z \\] <p>where \\(x_s\\) is the feature of interest and \\(x_c\\) are the other features. In our case, \\(x_1\\) is the feature of interest and \\(x_2, x_3\\) are the other features. In the example, given that \\(x_3 \\approx x_1\\), it holds that:</p> \\[ \\mathbb{E}_{x_2, x_3|x_1=z} \\left [ \\frac{\\partial f}{\\partial x_1} (z, x_2, x_3) \\right ] \\approx - 2 \\pi z \\cos(2 \\pi z) \\mathbb{1}_{z&lt;0}$,  \\] <p>and therefore the ALE and RHALE effect is defined as:</p> \\[\\text{ALE}(x_1) = \\int_{z=0}^{x_1} \\mathbb{E}_{x_2, x_3|x_1=z} \\left [ \\frac{\\partial f}{\\partial x_1} (z, x_2, x_3) \\right ] \\partial z \\approx - \\sin(2\\pi x_1) \\mathbb{1}_{x_1&lt;0} + c\\] <pre><code>def ale_gt(x):\n    y = np.zeros_like(x)\n    ind = x &lt; 0\n    y[ind] = - np.sin(2 * np.pi * x[ind])\n    c = 0.31\n    return y - c\n\ndef ale_gt_derivative_effect(x):\n    dydx = np.zeros_like(x)\n    ind = x &lt; 0\n    dydx[ind] = - 2 * np.pi * np.cos(2 * np.pi * x[ind])\n    return dydx, sigma_2\n</code></pre> <pre><code>plt.figure()\nplt.ylim(-2, 2)\nxx = np.linspace(-0.5, 0.5, 100)\nplt.plot(xx, ale_gt(xx), \"--\", label=\"ALE effect\")\nplt.title(\"ALE and RHALE definition\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"Tutorials/synthetic-examples/02_rhale_vs_ale_vs_pdp/#rhale-definition","title":"RHALE definition","text":"<p>RHALE extends ALE definition, adding an heterogeneity term. For visualizing the heterogeneity, RHALE proposes two plots one below the other. The top plot is the RHALE effect which is identical to the ALE effect. The bottom plot is the derivative of the ALE effect and the heterogeneity is the standard deviation of the derivative effect.</p> <p>RHALE quantifies the heterogeneity as the standard deviation of the derivative of the model with respect to the feature of interest:</p> \\[ \\sigma(z) = \\sigma_{x_c|x_s=z} \\left [ \\frac{\\partial f}{\\partial x_s} (z, x_c) \\right ] \\] <p>The heterogeneity is visualized on top of the ALE effect in two ways. First, as a shaded \\(\\pm\\) area around the ALE plot where the area is given by \\(\\pm \\sigma(x_s)*x_s\\) (aggregated standard deviation). Second, as a shaded area around the derivative of the ALE plot which is defined as \\(\\pm \\sigma(x_s)\\). In our example, the heterogeneity is:</p> \\[ \\sigma(x_1) = \\sigma_{x_2, x_3|x_1=z} \\left [ \\frac{\\partial f}{\\partial x_1} (z, x_2, x_3) \\right ] = \\sigma_2 \\] <p>The heterogeneity informs that the instance-level effects are deviating from the average effect by \\(\\pm \\sigma_2\\).</p> <pre><code>def rhale_gt(x):\n    y = np.zeros_like(x)\n    ind = x &lt; 0\n    y[ind] = - np.sin(2 * np.pi * x[ind])\n    c = 0.31\n    return y - c, (x + .5) * sigma_2\n\ndef rhale_gt_derivative_effect(x):\n    dydx = np.zeros_like(x)\n    ind = x &lt; 0\n    dydx[ind] = - 2 * np.pi * np.cos(2 * np.pi * x[ind])\n    return dydx, sigma_2\n</code></pre> <pre><code>plt.figure()\nplt.ylim(-2, 2)\nxx = np.linspace(-.5, .5, 100)\nplt.plot(xx, rhale_gt(xx)[0], \"--\", label=\"ground truth\")\nplt.title(\"Ground-truth RHALE and ALE\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"dy_dx1\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure()\nxx = np.linspace(-0.5, 0.5, 100)\nplt.plot(xx, rhale_gt_derivative_effect(xx)[0], \"--\", label=\"ground truth\")\nplt.fill_between(xx, rhale_gt_derivative_effect(xx)[0] - rhale_gt_derivative_effect(xx)[1], rhale_gt_derivative_effect(xx)[0] + rhale_gt_derivative_effect(xx)[1], alpha=0.2, label=\"$\\pm$ std\")\nplt.title(\"Ground-truth RHALE derivative effect\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"Tutorials/synthetic-examples/02_rhale_vs_ale_vs_pdp/#ale-approximation","title":"ALE approximation","text":"<p>Bin splitting is a major limitation of ALE. The user has to choose the number of bins, where different number of bins can lead to very different results. In terms of the main ALE effect, a wrong bin-splitting may lead to unstable results. In terms of the heterogeneity, a wrong bin-splitting may lead to a biased estimation of the heterogeneity. More information can be found at the RHALE paper.</p> <p>In our example, if setting a low number of bins (wide bins), e.g., 5, we get the following approximation:</p> <pre><code>feat = 0\nxx = np.linspace(-.5, .5, 100)\n\n# ale 5 bins\nnof_bins = 5\n# dale = effector.RHALE(data=x, model=f, model_jac=dfdx, axis_limits=axis_limits)\nale = effector.ALE(data=x, model=f, axis_limits=axis_limits)\nbinning = effector.binning_methods.Fixed(nof_bins=nof_bins, min_points_per_bin=0)\nale.fit([feat], binning_method=binning)\nale.plot(feature=feat, heterogeneity=True, centering=True)\n</code></pre> <p></p> <p>The approximation with wide bins has two drawbacks:</p> <ul> <li>the average ale effect is of low resolution; for example the change in the effect at \\(x_1=0\\) is not captured</li> <li>the heterogeneity is biased; for example, the central bin show heterogeneity of \\(\\sigma \\approx 4\\), where the ground-truth heterogeneity is \\(\\approx 2\\).</li> </ul> <p>Let's see what happens with a high number of bins (narrow bins), e.g., 50:</p> <pre><code># ale 50 bins\nnof_bins = 50\n# dale = effector.RHALE(data=x, model=f, model_jac=dfdx, axis_limits=axis_limits)\nale = effector.ALE(data=x, model=f, axis_limits=axis_limits)\nbinning = effector.binning_methods.Fixed(nof_bins=nof_bins, min_points_per_bin=0)\nale.fit([feat], binning_method=binning)\nale.plot(feature=feat, heterogeneity=\"std\", centering=True)\n</code></pre> <p></p> <p>The approximation with narrow bins has two drawbacks:</p> <ul> <li>the average ale effect is noisy; for example, the effect at \\(x_1&gt;0\\) in not zero.</li> <li>the heterogeneity is even more noisy. It has many spikes that are not present in the ground-truth heterogeneity.</li> </ul> <p>In real-world problems, the user has to choose the number of bins without any guidance. This is a major drawback of ALE, because the user does not know which approximation to trust, the one with wide bins or the one with narrow bins. </p>"},{"location":"Tutorials/synthetic-examples/02_rhale_vs_ale_vs_pdp/#rhale-approximation","title":"RHALE approximation","text":"<p>RHALE proposes an automatic bin-splitting approach to resolve the issue. Let's first see it practice:</p> <pre><code># rhale\nfeat = 0\nrhale = effector.RHALE(data=x, model=f, model_jac=dfdx, axis_limits=axis_limits)\nbinning = effector.binning_methods.DynamicProgramming(max_nof_bins=30, min_points_per_bin=10, discount=0.2)\nrhale.fit([feat], binning_method=binning, centering=True)\nrhale.plot(feature=feat, heterogeneity=\"std\")\n</code></pre> <p></p> <p>The approximation is much better, both in terms of the average effect and the heterogeneity. For example, the heterogeneity is almost constant around \\(2\\) and the spikes are gone. This happens due to the automatic bin-splitting, which creates bins of different sizes. In the beginning, area \\([-0.5, 0]\\), the bins are smaller for a good trade-off between bias and variance. In the end, area \\([0, 0.5]\\), a single bin is created to limit the variance without adding bias.</p>"},{"location":"Tutorials/synthetic-examples/02_rhale_vs_ale_vs_pdp/#more-about-the-automatic-bin-splitting","title":"More about the automatic bin-splitting","text":"<p>So, how is the automatic bin-splitting working?  Before we delve into the details, intutivelly, a wide bin does not (a) sacrifice resolution of the ALE plot and (b) does not introduce bias in the heterogeneity, only in case the effect is linear inside the bin (or the derivative-effect is constant). In our example, this happens in the area, \\([0, 0.5]\\). Intuitively, bin splitting algorithms search for such areas (to create a single bin there) and in all other cases they try to minimize the bias-variance trade-off.</p> <p>In <code>Effector</code>, there are two automatic bin-splitting methods:</p> <ul> <li><code>Greedy</code>: the user has to choose the maximum number of bins and the minimum number of samples per bin</li> <li><code>DynamicProgramming</code>: the user has to choose the maximum number of bins and the minimum number of samples per bin</li> </ul>"},{"location":"Tutorials/synthetic-examples/02_rhale_vs_ale_vs_pdp/#greedy-approach","title":"Greedy approach","text":"<p><code>Greedy</code> has three parameters; <code>init_nof_bins</code>, <code>min_points_per_bin</code> and <code>discount</code>. <code>max_nof_bins</code> is the initial (and maximum) number of bins. The algorithm then tries to merge bins in a greedy way, i.e., it moves along the axis (from left to right) and if merging with the next bin leads to a similar variance then it merges the bins. In fact, discount expresses to what extent the algorithm favors the creation of wider bins. The higher the discount the more the algorithm tries to minimize the variance, sacrificing some bias and vice versa. <code>min_points_per_bin</code> is the minimum number of samples each bin should have. </p> <pre><code># Greedy\nfeat = 0\nrhale = effector.RHALE(data=x, model=f, model_jac=dfdx, axis_limits=axis_limits)\nbinning = effector.binning_methods.Greedy(init_nof_bins=100, min_points_per_bin=10, discount=0.2)\nrhale.fit([feat], binning_method=binning)\nrhale.plot(feature=feat, heterogeneity=\"std\", show_avg_output=False)\n</code></pre> <p></p>"},{"location":"Tutorials/synthetic-examples/02_rhale_vs_ale_vs_pdp/#dynamic-programming-approach","title":"Dynamic Programming approach","text":"<p><code>DynamicProgramming</code> has three parameters; <code>max_nof_bins</code>, <code>min_points_per_bin</code> and <code>discount</code>. <code>max_nof_bins</code> is the maximum number of bins. The algorithm then tries to find the optimal binning, i.e., the binning that minimizes the bias-variance trade-off. <code>min_points_per_bin</code> is the minimum number of samples each bin should have. <code>discount</code> is the discount factor of the algorithm. The higher the discount the more the algorithm tries to minimize the variance, sacrificing some bias and vice versa. For more details, we refer the reader to the RHALE paper.</p> <pre><code># DynamicProgramming\nfeat = 0\nrhale = effector.RHALE(data=x, model=f, model_jac=dfdx, axis_limits=axis_limits)\nbinning = effector.binning_methods.DynamicProgramming(max_nof_bins=20, min_points_per_bin=10, discount=0.2)\nrhale.fit([feat], binning_method=binning)\nrhale.plot(feature=feat, heterogeneity=\"std\", show_avg_output=False)\n</code></pre> <p></p>"},{"location":"Tutorials/synthetic-examples/02_rhale_vs_ale_vs_pdp/#pdp-family-of-methods","title":"PDP family of methods","text":"<p>We have invested a significant amount of time in exploring ALE and RHALE, attempting to achieve the most accurate approximation of the ALE definition. Is it worthwhile? For instance, PDP-ICE is a widely used method for both average effect and heterogeneity. So, why not opt for PDP-ICE instead of delving into these sophisticated ALE approximations?</p> <p>In short, yes, especially in scenarios where the features exhibit correlation. Both PDP and ICE operate under the implicit assumption of feature independence. In cases where this assumption is substantially violated, relying on PDP and ICE may result in erroneous conclusions, stemming from extrapolation into unobserved regions.</p> <p>In our example, it is evident that \\(x_3\\) is highly dependent on \\(x_1\\). Therefore, let's examine the implications of employing PDP-ICE.</p>"},{"location":"Tutorials/synthetic-examples/02_rhale_vs_ale_vs_pdp/#pdp-and-ice-definition","title":"PDP and ICE definition","text":"<p>The PDP is defined as the average prediction over the entire dataset when setting the feature of interest at a specific value.:</p> \\[ \\text{PDP}(x_s) = \\mathbb{E}_{x_c}[f(x_s, x_c)] \\] <p>The ICE plots show the ouput of instance \\(i\\) if changing the feature of interest \\(x_s\\):</p> \\[\\text{ICE}^{(i)}(x_s, x^{(i)}_c) = f(x_s, x^{(i)}_c)\\] <p>ICE plots are plotted of the PDP.</p> <p>In our example, the PDP and ICE plots are:</p> \\[\\text{PDP}(x_1) = \\mathbb{E}_{x_2, x_3}[f(x_1, x_2, x_3)] = \\sin(2\\pi x_1) \\mathbb{1}_{x_1&lt;0} - \\frac{5}{3} \\sin(2\\pi x_1)\\] \\[\\text{ICE}^{(i)}(x_1, x^{(i)}_2, x^{(i)}_3) = f(x_1, x^{(i)}_2, x^{(i)}_3) = \\sin(2\\pi x_1) (\\mathbb{1}_{x_1&lt;0} - 2 \\mathbb{1}_{x_3^{(i)}} &lt; 0) + x_1 x_2^{(i)}\\] <pre><code>def gt_pdp(x):\n    y = np.sin(2 * np.pi * x) * (x &lt; 0) - 5/3 * np.sin(2 * np.pi * x)\n    return y\n\ndef gt_ice(x, N):\n    K = x.shape[0]\n    x3 = np.random.uniform(0, 6, N)\n    ind_1 = x3 &lt; 5\n    x_2_i = np.random.normal(0, 2, N)\n\n    y = np.stack([np.sin(2 * np.pi * x) * (x &lt; 0) for _ in range(N)], 0)\n    y[ind_1, :] -= np.stack([2 * np.sin(2 * np.pi * x) for _ in range(N)], 0)[ind_1, :]\n    y += np.stack([x * x_2_i[i] for i in range(N)], 0)\n    return y\n</code></pre> <pre><code>plt.figure()\nplt.ylim(-3, 3)\nxx = np.linspace(-.5, .5, 100)\nplt.plot(xx, gt_pdp(xx), \"b\", label=\"ground truth\")\nplt.plot(xx, gt_ice(xx, 50)[0], color=\"red\", label=\"ground truth\", alpha=0.1)\nplt.plot(xx, gt_ice(xx, 50)[1:].T, color=\"red\", alpha=0.1)\nplt.title(\"Ground-truth PDP\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>effector.PDP(data=x, model=f, nof_instances=100).plot(feature=0, centering=True, heterogeneity=\"ice\")\n</code></pre> <p></p> <p>It is crucial to recognize that heterogeneity arises from extrapolation into regions with low density. For instance, considering the data-generating distribution, it is highly improbable to observe a scenario where x_1&lt;0 and x_3&gt;0. However, PDP-ICE disrupts this correlation, leading some individual conditional expectations (ICEs) to trace \\(-\\sin(2\\pi x_1)\\) within the interval \\([-0.5, 0]\\).</p>"},{"location":"Tutorials/synthetic-examples/02_rhale_vs_ale_vs_pdp/#d-pdp-and-d-ice-definition-and-approximation","title":"d-PDP and d-ICE definition and approximation","text":"<p>The d-PDP and d-ICE are the derivative of the PDP and ICE, respectively. The d-PDP is the effect of the feature of interest on the model output. The d-ICE is the effect of the feature of interest on the model output for a specific instance.</p> <p>In our example, the d-PDP and d-ICE plots are:</p> \\[\\text{d-PDP}(x_1) = \\frac{\\partial}{\\partial x_1} \\mathbb{E}_{x_2, x_3}[f(x_1, x_2, x_3)] = - 2 \\pi x_1 \\cos(2 \\pi x_1) \\mathbb{1}_{x_1&lt;0} - \\frac{10 \\pi}{3} \\cos(2 \\pi x_1)\\] \\[\\text{d-ICE}^{(i)}(x_1, x^{(i)}_2, x^{(i)}_3) = \\frac{\\partial}{\\partial x_1} f(x_1, x^{(i)}_2, x^{(i)}_3) = 2 \\pi x_1 \\cos(2 \\pi x_1) (\\mathbb{1}_{x_1&lt;0} - 2 \\mathbb{1}_{x_3^{(i)}} &lt; 0) + x_2^{(i)}\\] <pre><code>def gt_d_pdp(x):\n    y = 2 * np.pi * np.cos(2 * np.pi * x) * (x &lt; 0) - 10 * np.pi / 3 * np.cos(2 * np.pi * x) + 1\n    return y\n\ndef gt_d_ice(x, N):\n    K = x.shape[0]\n    x3 = np.random.uniform(0, 6, N)\n    ind_1 = x3 &lt; 5\n    x_2_i = np.random.normal(0, 2, N)\n\n    y = np.stack([2 * np.pi * np.cos(2 * np.pi * x) * (x &lt; 0) for _ in range(N)], 0)\n    y[ind_1, :] -= np.stack([4 * np.pi * np.cos(2 * np.pi * x) for _ in range(N)], 0)[ind_1, :]\n    y += np.expand_dims(x_2_i, -1)\n    return y\n</code></pre> <pre><code>plt.figure()\nplt.ylim(-20, 20)\nxx = np.linspace(-.5, .5, 100)\nplt.plot(xx, gt_d_pdp(xx), \"b\", label=\"ground truth\")\nplt.plot(xx, gt_d_ice(xx, 50)[0], color=\"red\", label=\"ground truth\", alpha=0.1)\nplt.plot(xx, gt_d_ice(xx, 50)[1:].T, color=\"red\", alpha=0.1)\nplt.title(\"Ground-truth PDP\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>effector.DerivativePDP(data=x, model=f, model_jac=dfdx, nof_instances=50).plot(feature=0, centering=False, heterogeneity=\"ice\", y_limits=[-20, 20])\n</code></pre> <p></p>"},{"location":"Tutorials/synthetic-examples/02_rhale_vs_ale_vs_pdp/#shap-dependence-plots","title":"SHAP Dependence Plots","text":"<pre><code>effector.SHAPDependence(data=x, model=f, nof_instances=\"all\").plot(feature=0, centering=True, heterogeneity=\"shap_values\", y_limits=[-3, 3])\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/","title":"Regional Effects (known black-box function)","text":"<p>This tutorial provides a gentle overview of Regional Effect methods and introduces the <code>Effector</code> package. Regional Effects serve as a bridge between local and global feature effects. \u0391s shown in REPID, regional effect methods split the feature space in subregions where the feature interactions are minimized.</p> <p>In this tutorial, we show how to use <code>Effector</code> to explain a black box function using regional effect plots. The tutorial is organized as follows:</p> <ul> <li>Introduction of the simulation example, using two datasets, one with uncorrelated and the other with correlated features. </li> <li>Examine how PDP/RHALE/SHAP plots model the feature effect and how their regional counterpart can minimize feature interactions, providing better explanations.</li> <li>Show how each of these methods behaves under correlated and uncorrelated features.</li> </ul> <pre><code>import numpy as np\nimport effector\n</code></pre>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#simulation-example","title":"Simulation example","text":""},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#data-generating-distribution","title":"Data Generating Distribution","text":"<p>We will generate \\(N=1000\\) examples with \\(D=3\\) features, which are uniformly distributed as follows:</p> <p> Feature Description Distribution \\(x_1\\) Uniformly distributed between \\(-1\\) and \\(1\\) \\(x_1 \\sim \\mathcal{U}(-1,1)\\) \\(x_2\\) Uniformly distributed between \\(-1\\) and \\(1\\) \\(x_2 \\sim \\mathcal{U}(-1,1)\\) \\(x_3\\) Uniformly distributed between \\(-1\\) and \\(1\\) \\(x_3 \\sim \\mathcal{U}(-1,1)\\) <p></p> <p>For the correlated setting we keep the distributional assumptions for \\(x_2\\) and \\(x_3\\) but define \\(x_1\\) such that it is identical to \\(x_3\\) by: \\(x_1 = x_3\\).</p> <pre><code>def generate_dataset_uncorrelated(N):\n    x1 = np.random.uniform(-1, 1, size=N)\n    x2 = np.random.uniform(-1, 1, size=N)\n    x3 = np.random.uniform(-1, 1, size=N)\n    return np.stack((x1, x2, x3), axis=-1)\n\ndef generate_dataset_correlated(N):\n    x3 = np.random.uniform(-1, 1, size=N)\n    x2 = np.random.uniform(-1, 1, size=N)\n    x1 = x3\n    return np.stack((x1, x2, x3), axis=-1)\n\n# generate the dataset for the uncorrelated and correlated setting\nN = 1000\nX_uncor_train = generate_dataset_uncorrelated(N)\nX_uncor_test = generate_dataset_uncorrelated(10000)\nX_cor_train = generate_dataset_correlated(N)\nX_cor_test = generate_dataset_correlated(10000)\n</code></pre>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#black-box-function","title":"Black-box function","text":"<p>We will use the following linear model with a subgroup-specific interaction term:  $$ y = 3x_1I_{x_3&gt;0} - 3x_1I_{x_3\\leq0} + x_3$$ </p> <p>On a global level, there is a high heterogeneity for the features \\(x_1\\) and \\(x_3\\) due to their interaction with each other. However, this heterogeneity vanishes to 0 if the feature space is separated into subregions:</p> <p> Feature Region Average Effect Heterogeneity \\(x_1\\) \\(x_3&gt;0\\) \\(3x_1\\) 0 \\(x_1\\) \\(x_3\\leq 0\\) \\(-3x_1\\) 0 \\(x_2\\) all 0 0 \\(x_3\\) \\(x_3&gt;0\\) \\(x_3\\) 0 \\(x_3\\) \\(x_3\\leq 0\\) \\(x_3\\) 0 <p></p> <pre><code>def model(x):\n    f = np.where(x[:,2] &gt; 0, 3*x[:,0] + x[:,2], -3*x[:,0] + x[:,2])\n    return f\n\ndef model_jac(x):\n    dy_dx = np.zeros_like(x)\n\n    ind1 = x[:, 2] &gt; 0\n    ind2 = x[:, 2] &lt;= 0\n\n    dy_dx[ind1, 0] = 3\n    dy_dx[ind2, 0] = -3\n    dy_dx[:, 2] = 1\n    return dy_dx\n</code></pre> <pre><code>Y_uncor_train = model(X_uncor_train)\nY_uncor_test = model(X_uncor_test)\nY_cor_train = model(X_cor_train)\nY_cor_test = model(X_cor_test)      \n</code></pre>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#pdp","title":"PDP","text":"<p>The PDP is defined as the average of the model's output over the entire dataset, while varying the feature of interest.:</p> \\[ \\text{PDP}(x_s) = \\mathbb{E}_{x_c}[f(x_s, x_c)] \\] <p>and is approximated using the training data: </p> \\[ \\hat{\\text{PDP}}(x_s) = \\frac{1}{N} \\sum_{j=1}^N f(x_s, x^{(i)}_c) =  \\frac{1}{N} \\sum_{j=1}^N ICE^i(x_s)\\] <p>The PDP is simply the average over the underlying ICE curves (local effects). The ICE curves show how the feature of interest influences the prediction of the ML model for each single instance. The ICE curves show the heterogeneity of the local effects.</p>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#uncorrelated-setting","title":"Uncorrelated setting","text":""},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#global-pdp","title":"Global PDP","text":"<pre><code>pdp = effector.PDP(data=X_uncor_train, model=model, feature_names=['x1','x2','x3'], target_name=\"Y\")\npdp.plot(feature=0, centering=True, show_avg_output=False, heterogeneity=\"ice\", y_limits=[-5, 5])\npdp.plot(feature=1, centering=True, show_avg_output=False, heterogeneity=\"ice\", y_limits=[-5, 5])\npdp.plot(feature=2, centering=True, show_avg_output=False, heterogeneity=\"ice\", y_limits=[-5, 5])\n</code></pre>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#regional-pdp","title":"Regional PDP","text":"<p>Regional PDP will search for explanations that minimize the interaction-related heterogeneity.</p> <pre><code>regional_pdp = effector.RegionalPDP(data=X_uncor_train, model=model, feature_names=['x1','x2','x3'], axis_limits=np.array([[-1,1],[-1,1],[-1,1]]).T)\nregional_pdp.fit(features=\"all\", heter_pcg_drop_thres=0.3, nof_candidate_splits_for_numerical=11)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 27.85it/s]\n</code></pre> <pre><code>regional_pdp.show_partitioning(features=0)\n</code></pre> <pre><code>Feature 0 - Full partition tree:\nNode id: 0, name: x1, heter: 1.74 || nof_instances:  1000 || weight: 1.00\n        Node id: 1, name: x1 | x3 &lt;= -0.0, heter: 0.28 || nof_instances:   496 || weight: 0.50\n        Node id: 2, name: x1 | x3  &gt; -0.0, heter: 0.29 || nof_instances:   504 || weight: 0.50\n--------------------------------------------------\nFeature 0 - Statistics per tree level:\nLevel 0, heter: 1.74\n        Level 1, heter: 0.29 || heter drop: 1.45 (83.59%)\n</code></pre> <pre><code>regional_pdp.plot(feature=0, node_idx=1, heterogeneity=\"ice\", y_limits=[-5, 5])\nregional_pdp.plot(feature=0, node_idx=2, heterogeneity=\"ice\", y_limits=[-5, 5])\n</code></pre> <p></p> <p></p> <pre><code>regional_pdp.show_partitioning(features=1)\n</code></pre> <pre><code>Feature 1 - Full partition tree:\nNode id: 0, name: x2, heter: 1.84 || nof_instances:  1000 || weight: 1.00\n--------------------------------------------------\nFeature 1 - Statistics per tree level:\nLevel 0, heter: 1.84\n</code></pre> <pre><code>regional_pdp.show_partitioning(features=2)\n</code></pre> <pre><code>Feature 2 - Full partition tree:\nNode id: 0, name: x3, heter: 1.75 || nof_instances:  1000 || weight: 1.00\n        Node id: 1, name: x3 | x1 &lt;= -0.0, heter: 0.86 || nof_instances:   489 || weight: 0.49\n        Node id: 2, name: x3 | x1  &gt; -0.0, heter: 0.86 || nof_instances:   511 || weight: 0.51\n--------------------------------------------------\nFeature 2 - Statistics per tree level:\nLevel 0, heter: 1.75\n        Level 1, heter: 0.86 || heter drop: 0.89 (51.06%)\n</code></pre> <pre><code>regional_pdp.plot(feature=2, node_idx=1, heterogeneity=\"ice\", centering=True, y_limits=[-5, 5])\nregional_pdp.plot(feature=2, node_idx=2, heterogeneity=\"ice\", centering=True, y_limits=[-5, 5])\n</code></pre> <p></p> <p></p>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#conclusion","title":"Conclusion","text":"<p>For the Global PDP:</p> <ul> <li>the average effect of \\(x_1\\) is \\(0\\) with some heterogeneity implied by the interaction with \\(x_1\\). The heterogeneity is expressed with two opposite lines; \\(-3x_1\\) when \\(x_1 \\leq 0\\) and \\(3x_1\\) when \\(x_1 &gt;0\\)</li> <li>the average effect of \\(x_2\\) to be \\(0\\) without heterogeneity</li> <li>the average effect of \\(x_3\\) to be \\(x_3\\) with some heterogeneity due to the interaction with \\(x_1\\). The heterogeneity is expressed with a discontinuity around \\(x_3=0\\), with either a positive or a negative offset depending on the value of \\(x_1^i\\)</li> </ul> <p>For the Regional PDP:</p> <ul> <li>For \\(x_1\\), the algorithm finds two regions, one for \\(x_3 \\leq 0\\) and one for \\(x_3 &gt; 0\\)</li> <li>when \\(x_3&gt;0\\) the effect is \\(3x_1\\)</li> <li>when \\(x_3 \\leq 0\\), the effect is \\(-3x_1\\)</li> <li>For \\(x_2\\) the algorithm does not find any subregion </li> <li>For \\(x_3\\), there is a change in the offset:</li> <li>when \\(x_1&gt;0\\) the line is \\(x_3 - 3x_1^i\\) in the first half and \\(x_3 + 3x_1^i\\) later</li> <li>when \\(x_1&lt;0\\) the line is \\(x_3 + 3x_1^i\\) in the first half and \\(x_3 - 3x_1^i\\) later</li> </ul>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#correlated-setting","title":"Correlated setting","text":"<p>PDP assumes feature independence, therefore, it is not a good explanation method for the correlated case. Due to this face, we expect the explanations to be identical with the uncorrelated case, which is not correct as we will see later in (RH)ALE plots.</p>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#global-pdp_1","title":"Global PDP","text":"<pre><code>pdp = effector.PDP(data=X_cor_train, model=model, feature_names=['x1','x2','x3'], target_name=\"Y\")\npdp.plot(feature=0, centering=True, show_avg_output=False, heterogeneity=\"ice\", y_limits=[-5, 5])\npdp.plot(feature=1, centering=True, show_avg_output=False, heterogeneity=\"ice\", y_limits=[-5, 5])\npdp.plot(feature=2, centering=True, show_avg_output=False, heterogeneity=\"ice\", y_limits=[-5, 5])\n</code></pre>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#regional-pdp_1","title":"Regional-PDP","text":"<pre><code>regional_pdp = effector.RegionalPDP(data=X_cor_train, model=model, feature_names=['x1','x2','x3'], axis_limits=np.array([[-1,1],[-1,1],[-1,1]]).T)\nregional_pdp.fit(features=\"all\", heter_pcg_drop_thres=0.4, nof_candidate_splits_for_numerical=11)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 23.85it/s]\n</code></pre> <pre><code>regional_pdp.show_partitioning(features=0)\n</code></pre> <pre><code>Feature 0 - Full partition tree:\nNode id: 0, name: x1, heter: 1.74 || nof_instances:  1000 || weight: 1.00\n        Node id: 1, name: x1 | x3 &lt;= 0.0, heter: 0.29 || nof_instances:   514 || weight: 0.51\n        Node id: 2, name: x1 | x3  &gt; 0.0, heter: 0.30 || nof_instances:   486 || weight: 0.49\n--------------------------------------------------\nFeature 0 - Statistics per tree level:\nLevel 0, heter: 1.74\n        Level 1, heter: 0.29 || heter drop: 1.45 (83.23%)\n</code></pre> <pre><code>regional_pdp.plot(feature=0, node_idx=1, heterogeneity=\"ice\", centering=True, y_limits=[-5, 5])\nregional_pdp.plot(feature=0, node_idx=2, heterogeneity=\"ice\", centering=True, y_limits=[-5, 5])\n</code></pre> <pre><code>regional_pdp.show_partitioning(features=1)\n</code></pre> <pre><code>Feature 1 - Full partition tree:\nNode id: 0, name: x2, heter: 1.04 || nof_instances:  1000 || weight: 1.00\n        Node id: 1, name: x2 | x1 &lt;= 0.54, heter: 0.59 || nof_instances:   783 || weight: 0.78\n        Node id: 2, name: x2 | x1  &gt; 0.54, heter: 0.54 || nof_instances:   217 || weight: 0.22\n--------------------------------------------------\nFeature 1 - Statistics per tree level:\nLevel 0, heter: 1.04\n        Level 1, heter: 0.58 || heter drop: 0.46 (44.15%)\n</code></pre> <pre><code>regional_pdp.show_partitioning(features=2)\n</code></pre> <pre><code>Feature 2 - Full partition tree:\nNode id: 0, name: x3, heter: 1.75 || nof_instances:  1000 || weight: 1.00\n        Node id: 1, name: x3 | x1 &lt;= 0.0, heter: 0.86 || nof_instances:   514 || weight: 0.51\n        Node id: 2, name: x3 | x1  &gt; 0.0, heter: 0.89 || nof_instances:   486 || weight: 0.49\n--------------------------------------------------\nFeature 2 - Statistics per tree level:\nLevel 0, heter: 1.75\n        Level 1, heter: 0.88 || heter drop: 0.87 (49.77%)\n</code></pre> <pre><code>regional_pdp.plot(feature=2, node_idx=1, heterogeneity=\"ice\", centering=True, y_limits=[-5, 5])\nregional_pdp.plot(feature=2, node_idx=2, heterogeneity=\"ice\", centering=True, y_limits=[-5, 5])\n</code></pre>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#conclusion_1","title":"Conclusion","text":"<p>As expected, the global and the regional PDP explanations are identical with the uncorrelated case.</p>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#rhale","title":"(RH)ALE","text":"<p>(RH)ALE defines the feature effect as the integral of the partial derivative of the model's output with respect to the feature of interest:</p> \\[\\text{ALE}(x_s) = \\int_{z=0}^{x_s} \\mathbb{E}_{x_c|x_s=z}\\left [ \\frac{\\partial f}{\\partial x_s} (z, x_c) \\right ] \\partial z\\] <p>The approximation is defined as:</p> \\[\\hat{\\text{ALE}}(x_s) = \\sum_{k=1}^{k_{x_s}} \\frac{1}{| \\mathcal{S}_k |} \\sum_{i: x^{(i)} \\in \\mathcal{S}_k} \\left [ f(z_k, x_c) - f(z_{k-1}, x_c) \\right ]\\] <p>\\(\\hat{\\text{ALE}}(x_s)\\) uses a Riemannian sum to approximate the integral of \\(\\text{ALE}(x_s)\\). The axis of the \\(s\\)-th feature is split in \\(K\\) bins (intervals) of equal size. In each bin, the average effect of the feature of interest is estimated using the instances that fall in the bin. The average effect in each bin is called bin-effect. </p> <p>Robust and Heterogeneity-aware ALE (RHALE) is a variant of ALE, proposed by Gkolemis et. al, where the local effects are computed using automatic differentiation:</p> \\[\\hat{\\text{RHALE}}(x_s) = \\sum_{k=1}^{k_{x_s}} \\frac{1}{ \\left | \\mathcal{S}_k \\right |} \\sum_{i: x^{(i)} \\in \\mathcal{S}_k} \\frac{\\partial f}{\\partial x_s} (x_s^{(i)}, x_c^{(i)})\\] <p>In their paper, Gkolemis et. al showed that RHALE has specific advantages over ALE: (a) it ensures on-distribution sampling (b) an unbiased estimation of the heterogeneity and (c) an optimal trade-off between bias and variance. In our example, we will use the RHALE approximation.</p>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#uncorrelated-setting_1","title":"Uncorrelated setting","text":""},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#global-rhale","title":"Global RHALE","text":"<pre><code>rhale = effector.RHALE(data=X_uncor_train, model=model, model_jac=model_jac, feature_names=['x1','x2','x3'], target_name=\"Y\")\n\nbinning_method = effector.binning_methods.Fixed(10, min_points_per_bin=0)\nrhale.fit(features=\"all\", binning_method=binning_method, centering=True)\n\nrhale.plot(feature=0, centering=True, heterogeneity=\"std\", show_avg_output=False, y_limits=[-5, 5], dy_limits=[-5, 5])\nrhale.plot(feature=1, centering=True, heterogeneity=\"std\", show_avg_output=False, y_limits=[-5, 5], dy_limits=[-5, 5])\nrhale.plot(feature=2, centering=True, heterogeneity=\"std\", show_avg_output=False, y_limits=[-5, 5], dy_limits=[-5, 5])\n</code></pre>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#regional-rhale","title":"Regional RHALE","text":"<p>The disadvantage of RHALE plot is that it does not reveal the type of heterogeneity. Therefore, Regional (RH)ALE plots are very helpful to identify the type of heterogeneity. Let's see that in practice:</p> <pre><code>regional_rhale = effector.RegionalRHALE(\n    data=X_uncor_train, \n    model=model, \n    model_jac= model_jac, \n    feature_names=['x1', 'x2', 'x3'],\n    axis_limits=np.array([[-1, 1], [-1, 1], [-1, 1]]).T) \n\nbinning_method = effector.binning_methods.Fixed(11, min_points_per_bin=0)\nregional_rhale.fit(\n    features=\"all\",\n    heter_pcg_drop_thres=0.6,\n    binning_method=binning_method,\n    nof_candidate_splits_for_numerical=11\n)\n</code></pre> <pre><code>  0%|          | 0/3 [00:00&lt;?, ?it/s]invalid value encountered in divide\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 14.58it/s]\n</code></pre> <pre><code>regional_rhale.show_partitioning(features=0)\n</code></pre> <pre><code>Feature 0 - Full partition tree:\nNode id: 0, name: x1, heter: 5.94 || nof_instances:  1000 || weight: 1.00\n        Node id: 1, name: x1 | x3 &lt;= -0.0, heter: 0.00 || nof_instances:   496 || weight: 0.50\n        Node id: 2, name: x1 | x3  &gt; -0.0, heter: 0.00 || nof_instances:   504 || weight: 0.50\n--------------------------------------------------\nFeature 0 - Statistics per tree level:\nLevel 0, heter: 5.94\n        Level 1, heter: 0.00 || heter drop: 5.94 (100.00%)\n</code></pre> <pre><code>regional_rhale.plot(feature=0, node_idx=1, heterogeneity=\"std\", centering=True, y_limits=[-5, 5])\nregional_rhale.plot(feature=0, node_idx=2, heterogeneity=\"std\", centering=True, y_limits=[-5, 5])\n</code></pre> <p></p> <p></p> <pre><code>regional_rhale.show_partitioning(features=1)\n</code></pre> <pre><code>Feature 1 - Full partition tree:\nNode id: 0, name: x2, heter: 0.00 || nof_instances:  1000 || weight: 1.00\n--------------------------------------------------\nFeature 1 - Statistics per tree level:\nLevel 0, heter: 0.00\n</code></pre> <pre><code>regional_rhale.show_partitioning(features=2)\n</code></pre> <pre><code>Feature 2 - Full partition tree:\nNode id: 0, name: x3, heter: 0.00 || nof_instances:  1000 || weight: 1.00\n--------------------------------------------------\nFeature 2 - Statistics per tree level:\nLevel 0, heter: 0.00\n</code></pre>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#conclusion_2","title":"Conclusion","text":"<p>The explanations are similar to the ones obtained with the PDP plots. The average effect of \\(x_1\\) is \\(0\\) with some heterogeneity due to the interaction with \\(x_1\\). The heterogeneity is shown with the red vertical bars. The average effect of \\(x_2\\) is \\(0\\) without heterogeneity. The average effect of \\(x_3\\) is \\(x_3\\), but in contrast with the PDP plots, there is no heterogeneity. The regional RHALE plots explain the type of the heterogeneity for \\(x_1\\).</p>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#correlated-setting_1","title":"Correlated setting","text":"<p>In the correlated setting \\(x_3=x_1\\), therefore the model's formula becomes:</p> <p>$$ y = 3x_1I_{x_1&gt;0} - 3x_1I_{x_1\\leq0} + x_3$$ </p>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#global-rhale_1","title":"Global RHALE","text":"<p>RHALE plots respect feature correlations, therefore we expect the explanations to follow the formula above.</p> <pre><code>rhale = effector.RHALE(data=X_cor_train, model=model, model_jac=model_jac, \n                       feature_names=['x1','x2','x3'], \n                       target_name=\"Y\", \n                       axis_limits=np.array([[-1, 1], [-1, 1], [-1, 1]]).T)\nbinning_method = effector.binning_methods.Fixed(10, min_points_per_bin=0)\nrhale.fit(features=\"all\", binning_method=binning_method, centering=True)\n</code></pre> <pre><code>rhale.plot(feature=0, centering=True, heterogeneity=\"std\", show_avg_output=False, y_limits=[-5, 5], dy_limits=[-5, 5])\nrhale.plot(feature=1, centering=True, heterogeneity=\"std\", show_avg_output=False, y_limits=[-5, 5], dy_limits=[-5, 5])\nrhale.plot(feature=2, centering=True, heterogeneity=\"std\", show_avg_output=False, y_limits=[-5, 5], dy_limits=[-5, 5])\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#regional-rhale_1","title":"Regional RHALE","text":"<pre><code>regional_rhale = effector.RegionalRHALE(\n    data=X_cor_train, \n    model=model, \n    model_jac= model_jac, \n    feature_names=['x1', 'x2', 'x3'],\n    axis_limits=np.array([[-1, 1], [-1, 1], [-1, 1]]).T) \n\nbinning_method = effector.binning_methods.Fixed(10, min_points_per_bin=0)\nregional_rhale.fit(\n    features=\"all\",\n    heter_pcg_drop_thres=0.6,\n    binning_method=binning_method,\n    nof_candidate_splits_for_numerical=10\n)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 16.78it/s]\n</code></pre> <pre><code>regional_rhale.show_partitioning(features=0)\n</code></pre> <pre><code>Feature 0 - Full partition tree:\nNode id: 0, name: x1, heter: 0.00 || nof_instances:  1000 || weight: 1.00\n--------------------------------------------------\nFeature 0 - Statistics per tree level:\nLevel 0, heter: 0.00\n</code></pre> <pre><code>regional_rhale.show_partitioning(features=1)\n</code></pre> <pre><code>Feature 1 - Full partition tree:\nNode id: 0, name: x2, heter: 0.00 || nof_instances:  1000 || weight: 1.00\n--------------------------------------------------\nFeature 1 - Statistics per tree level:\nLevel 0, heter: 0.00\n</code></pre> <pre><code>regional_rhale.show_partitioning(features=2)\n</code></pre> <pre><code>Feature 2 - Full partition tree:\nNode id: 0, name: x3, heter: 0.00 || nof_instances:  1000 || weight: 1.00\n--------------------------------------------------\nFeature 2 - Statistics per tree level:\nLevel 0, heter: 0.00\n</code></pre>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#conclusion_3","title":"Conclusion","text":"<p>The global RHALE plots follow the formula obtained after setting \\(x_1=x_3\\) while the Regional (RH)ALE plot do not find any subregions in the correlated case.</p>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#shap-dp","title":"SHAP DP","text":""},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#uncorrelated-setting_2","title":"Uncorrelated setting","text":""},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#global-shap-dp","title":"Global SHAP DP","text":"<pre><code>shap = effector.SHAPDependence(data=X_uncor_train, model=model, feature_names=['x1','x2','x3'], target_name=\"Y\")\n\nshap.plot(feature=0, centering=True, heterogeneity=\"shap_values\", show_avg_output=False, y_limits=[-3, 3])\nshap.plot(feature=1, centering=True, heterogeneity=\"shap_values\", show_avg_output=False, y_limits=[-3, 3])\nshap.plot(feature=2, centering=True, heterogeneity=\"shap_values\", show_avg_output=False, y_limits=[-3, 3])\n</code></pre>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#regional-shap-dp","title":"Regional SHAP-DP","text":"<pre><code>regional_shap = effector.RegionalSHAP(\n    data=X_uncor_train, \n    model=model, \n    feature_names=['x1', 'x2', 'x3'],\n    axis_limits=np.array([[-1, 1], [-1, 1], [-1, 1]]).T) \n\nregional_shap.fit(\n    features=\"all\",\n    heter_pcg_drop_thres=0.6,\n    nof_candidate_splits_for_numerical=11\n)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:05&lt;00:00,  1.85s/it]\n</code></pre> <pre><code>regional_shap.show_partitioning(0)\n</code></pre> <pre><code>Feature 0 - Full partition tree:\nNode id: 0, name: x1, heter: 0.75 || nof_instances:   100 || weight: 1.00\n        Node id: 1, name: x1 | x3 &lt;= 0.02, heter: 0.10 || nof_instances:    51 || weight: 0.51\n        Node id: 2, name: x1 | x3  &gt; 0.02, heter: 0.00 || nof_instances:    49 || weight: 0.49\n--------------------------------------------------\nFeature 0 - Statistics per tree level:\nLevel 0, heter: 0.75\n        Level 1, heter: 0.05 || heter drop: 0.70 (92.93%)\n</code></pre> <pre><code>regional_shap.plot(feature=0, node_idx=1, heterogeneity=\"std\", centering=True, y_limits=[-5, 5])\nregional_shap.plot(feature=0, node_idx=2, heterogeneity=\"std\", centering=True, y_limits=[-5, 5])\n</code></pre> <pre><code>regional_shap.show_partitioning(features=1)\n</code></pre> <pre><code>Feature 1 - Full partition tree:\nNode id: 0, name: x2, heter: 0.00 || nof_instances:   100 || weight: 1.00\n--------------------------------------------------\nFeature 1 - Statistics per tree level:\nLevel 0, heter: 0.00\n</code></pre> <pre><code>regional_shap.show_partitioning(features=2)\n</code></pre> <pre><code>Feature 2 - Full partition tree:\nNode id: 0, name: x3, heter: 0.75 || nof_instances:   100 || weight: 1.00\n--------------------------------------------------\nFeature 2 - Statistics per tree level:\nLevel 0, heter: 0.75\n</code></pre>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#conclusion_4","title":"Conclusion","text":"<p>Global SHAP-DP:</p> <ul> <li>the average effect of \\(x_1\\) is \\(0\\) with some heterogeneity implied by the interaction with \\(x_1\\). The heterogeneity is expressed with two opposite lines; \\(-3x_1\\) when \\(x_1 \\leq 0\\) and \\(3x_1\\) when \\(x_1 &gt;0\\)</li> <li>the average effect of \\(x_2\\) to be \\(0\\) without heterogeneity</li> <li>the average effect of \\(x_3\\) to be \\(x_3\\) with some heterogeneity due to the interaction with \\(x_1\\). In contrast with other methods, SHAP spread the heterogeneity along the x-axis.</li> </ul> <p>Regional SHAP-DP:</p>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#correlated-setting_2","title":"Correlated setting","text":""},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#global-shap-dp_1","title":"Global SHAP-DP","text":"<pre><code>shap = effector.SHAPDependence(data=X_cor_train, model=model, feature_names=['x1','x2','x3'], target_name=\"Y\")\n\nshap.plot(feature=0, centering=True, heterogeneity=\"shap_values\", show_avg_output=False, y_limits=[-3, 3])\nshap.plot(feature=1, centering=True, heterogeneity=\"shap_values\", show_avg_output=False, y_limits=[-3, 3])\nshap.plot(feature=2, centering=True, heterogeneity=\"shap_values\", show_avg_output=False, y_limits=[-3, 3])\n</code></pre>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#regional-shap","title":"Regional SHAP","text":"<pre><code>regional_shap = effector.RegionalSHAP(\n    data=X_cor_train, \n    model=model, \n    feature_names=['x1', 'x2', 'x3'],\n    axis_limits=np.array([[-1, 1], [-1, 1], [-1, 1]]).T) \n\nregional_shap.fit(\n    features=\"all\",\n    heter_pcg_drop_thres=0.6,\n    nof_candidate_splits_for_numerical=11\n)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:05&lt;00:00,  1.77s/it]\n</code></pre> <pre><code>regional_shap.show_partitioning(0)\n</code></pre> <pre><code>Feature 0 - Full partition tree:\nNode id: 0, name: x1, heter: 0.09 || nof_instances:   100 || weight: 1.00\n--------------------------------------------------\nFeature 0 - Statistics per tree level:\nLevel 0, heter: 0.09\n</code></pre> <pre><code>regional_shap.show_partitioning(1)\n</code></pre> <pre><code>Feature 1 - Full partition tree:\nNode id: 0, name: x2, heter: 0.00 || nof_instances:   100 || weight: 1.00\n--------------------------------------------------\nFeature 1 - Statistics per tree level:\nLevel 0, heter: 0.00\n</code></pre> <pre><code>regional_shap.show_partitioning(2)\n</code></pre> <pre><code>Feature 2 - Full partition tree:\nNode id: 0, name: x3, heter: 0.09 || nof_instances:   100 || weight: 1.00\n--------------------------------------------------\nFeature 2 - Statistics per tree level:\nLevel 0, heter: 0.09\n</code></pre>"},{"location":"Tutorials/synthetic-examples/03_regional_effects_synthetic_f/#conclusion_5","title":"Conclusion","text":""},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/","title":"Regional Effects (unknown black-box function)","text":"<p>This tutorial use the same dataset with the previous tutorial, but instead of explaining the known (synthetic) predictive function, we fit a neural network on the data and explain the neural network. This is a more realistic scenario, since in real-world applications we do not know the underlying function and we only have access to the data. We advise the reader to first read the previous tutorial.</p> <pre><code>import numpy as np\nimport effector\nimport keras\nimport tensorflow as tf\n\nnp.random.seed(12345)\ntf.random.set_seed(12345)\n</code></pre> <pre><code>2024-01-08 14:12:18.742402: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2024-01-08 14:12:18.776175: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-01-08 14:12:18.776228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-01-08 14:12:18.777514: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-01-08 14:12:18.784727: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2024-01-08 14:12:18.785319: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-01-08 14:12:19.597983: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#simulation-example","title":"Simulation example","text":""},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#data-generating-distribution","title":"Data Generating Distribution","text":"<p>We will generate \\(N=500\\) examples with \\(D=3\\) features, which are in the uncorrelated setting all uniformly distributed as follows:</p> <p> Feature Description Distribution \\(x_1\\) Uniformly distributed between \\(-1\\) and \\(1\\) \\(x_1 \\sim \\mathcal{U}(-1,1)\\) \\(x_2\\) Uniformly distributed between \\(-1\\) and \\(1\\) \\(x_2 \\sim \\mathcal{U}(-1,1)\\) \\(x_3\\) Uniformly distributed between \\(-1\\) and \\(1\\) \\(x_3 \\sim \\mathcal{U}(-1,1)\\) <p></p> <p>For the correlated setting we keep the distributional assumptions for \\(x_2\\) and \\(x_3\\) but define \\(x_1\\) such that it is highly correlated with \\(x_3\\) by: \\(x_1 = x_3 + \\delta\\) with \\(\\delta \\sim \\mathcal{N}(0,0.0625)\\).</p> <pre><code>def generate_dataset_uncorrelated(N):\n    x1 = np.random.uniform(-1, 1, size=N)\n    x2 = np.random.uniform(-1, 1, size=N)\n    x3 = np.random.uniform(-1, 1, size=N)\n    return np.stack((x1, x2, x3), axis=-1)\n\ndef generate_dataset_correlated(N):\n    x3 = np.random.uniform(-1, 1, size=N)\n    x2 = np.random.uniform(-1, 1, size=N)\n    x1 = x3 + np.random.normal(loc = np.zeros_like(x3), scale = 0.25)\n    return np.stack((x1, x2, x3), axis=-1)\n\n# generate the dataset for the uncorrelated and correlated setting\nN = 1000\nX_uncor_train = generate_dataset_uncorrelated(N)\nX_uncor_test = generate_dataset_uncorrelated(10000)\nX_cor_train = generate_dataset_correlated(N)\nX_cor_test = generate_dataset_correlated(10000)\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#black-box-function","title":"Black-box function","text":"<p>We will use the following linear model with a subgroup-specific interaction term:  $$ y = 3x_1I_{x_3&gt;0} - 3x_1I_{x_3\\leq0} + x_3$$ </p> <p>On a global level, there is a high heterogeneity for the features \\(x_1\\) and \\(x_3\\) due to their interaction with each other. However, this heterogeneity vanishes to 0 if the feature space is separated into subregions:</p> <p> Feature Region Average Effect Heterogeneity \\(x_1\\) \\(x_3&gt;0\\) \\(3x_1\\) 0 \\(x_1\\) \\(x_3\\leq 0\\) \\(-3x_1\\) 0 \\(x_2\\) all 0 0 \\(x_3\\) \\(x_3&gt;0\\) \\(x_3\\) 0 \\(x_3\\) \\(x_3\\leq 0\\) \\(x_3\\) 0 <p></p> <pre><code>def generate_target(X):\n    f = np.where(X[:,2] &gt; 0, 3*X[:,0] + X[:,2], -3*X[:,0] + X[:,2])\n    epsilon = np.random.normal(loc = np.zeros_like(X[:,0]), scale = 0.1)\n    Y = f + epsilon\n    return(Y)\n\n# generate target for uncorrelated and correlated setting\nY_uncor_train = generate_target(X_uncor_train)\nY_uncor_test = generate_target(X_uncor_test)\nY_cor_train = generate_target(X_cor_train)\nY_cor_test = generate_target(X_cor_test)      \n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#fit-a-neural-network","title":"Fit a Neural Network","text":"<p>We create a two-layer feedforward Neural Network, a weight decay of 0.01 for 100 epochs. We train two instances of this NN, one on the uncorrelated and one on the correlated setting. In both cases, the NN achieves a Mean Squared Error of about \\(0.17\\) units.</p> <pre><code># Train - Evaluate - Explain a neural network\nmodel_uncor = keras.Sequential([\n    keras.layers.Dense(10, activation=\"relu\", input_shape=(3,)),\n    keras.layers.Dense(10, activation=\"relu\", input_shape=(3,)),\n    keras.layers.Dense(1)\n])\n\noptimizer = keras.optimizers.Adam(learning_rate=0.01)\nmodel_uncor.compile(optimizer=optimizer, loss=\"mse\")\nmodel_uncor.fit(X_uncor_train, Y_uncor_train, epochs=100)\nmodel_uncor.evaluate(X_uncor_test, Y_uncor_test)\n</code></pre> <pre><code>Epoch 1/100\n\n\n2024-01-08 14:12:20.220663: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n\n\n32/32 [==============================] - 0s 891us/step - loss: 2.4907\nEpoch 2/100\n32/32 [==============================] - 0s 791us/step - loss: 0.8437\nEpoch 3/100\n32/32 [==============================] - 0s 792us/step - loss: 0.5186\nEpoch 4/100\n32/32 [==============================] - 0s 761us/step - loss: 0.3794\nEpoch 5/100\n32/32 [==============================] - 0s 764us/step - loss: 0.2954\nEpoch 6/100\n32/32 [==============================] - 0s 812us/step - loss: 0.2609\nEpoch 7/100\n32/32 [==============================] - 0s 852us/step - loss: 0.2211\nEpoch 8/100\n32/32 [==============================] - 0s 777us/step - loss: 0.1949\nEpoch 9/100\n32/32 [==============================] - 0s 758us/step - loss: 0.1847\nEpoch 10/100\n32/32 [==============================] - 0s 780us/step - loss: 0.2012\nEpoch 11/100\n32/32 [==============================] - 0s 845us/step - loss: 0.1805\nEpoch 12/100\n32/32 [==============================] - 0s 823us/step - loss: 0.1663\nEpoch 13/100\n32/32 [==============================] - 0s 786us/step - loss: 0.1980\nEpoch 14/100\n32/32 [==============================] - 0s 766us/step - loss: 0.1649\nEpoch 15/100\n32/32 [==============================] - 0s 762us/step - loss: 0.1340\nEpoch 16/100\n32/32 [==============================] - 0s 765us/step - loss: 0.1337\nEpoch 17/100\n32/32 [==============================] - 0s 743us/step - loss: 0.1661\nEpoch 18/100\n32/32 [==============================] - 0s 717us/step - loss: 0.1326\nEpoch 19/100\n32/32 [==============================] - 0s 751us/step - loss: 0.1386\nEpoch 20/100\n32/32 [==============================] - 0s 898us/step - loss: 0.1447\nEpoch 21/100\n32/32 [==============================] - 0s 756us/step - loss: 0.1212\nEpoch 22/100\n32/32 [==============================] - 0s 737us/step - loss: 0.1406\nEpoch 23/100\n32/32 [==============================] - 0s 731us/step - loss: 0.1160\nEpoch 24/100\n32/32 [==============================] - 0s 739us/step - loss: 0.1454\nEpoch 25/100\n32/32 [==============================] - 0s 727us/step - loss: 0.1220\nEpoch 26/100\n32/32 [==============================] - 0s 748us/step - loss: 0.1124\nEpoch 27/100\n32/32 [==============================] - 0s 751us/step - loss: 0.1195\nEpoch 28/100\n32/32 [==============================] - 0s 742us/step - loss: 0.1236\nEpoch 29/100\n32/32 [==============================] - 0s 753us/step - loss: 0.1169\nEpoch 30/100\n32/32 [==============================] - 0s 754us/step - loss: 0.0987\nEpoch 31/100\n32/32 [==============================] - 0s 751us/step - loss: 0.0964\nEpoch 32/100\n32/32 [==============================] - 0s 772us/step - loss: 0.1026\nEpoch 33/100\n32/32 [==============================] - 0s 749us/step - loss: 0.1159\nEpoch 34/100\n32/32 [==============================] - 0s 756us/step - loss: 0.1041\nEpoch 35/100\n32/32 [==============================] - 0s 744us/step - loss: 0.1432\nEpoch 36/100\n32/32 [==============================] - 0s 735us/step - loss: 0.1284\nEpoch 37/100\n32/32 [==============================] - 0s 756us/step - loss: 0.1035\nEpoch 38/100\n32/32 [==============================] - 0s 731us/step - loss: 0.0984\nEpoch 39/100\n32/32 [==============================] - 0s 735us/step - loss: 0.1099\nEpoch 40/100\n32/32 [==============================] - 0s 762us/step - loss: 0.0957\nEpoch 41/100\n32/32 [==============================] - 0s 739us/step - loss: 0.0974\nEpoch 42/100\n32/32 [==============================] - 0s 726us/step - loss: 0.0954\nEpoch 43/100\n32/32 [==============================] - 0s 741us/step - loss: 0.0915\nEpoch 44/100\n32/32 [==============================] - 0s 751us/step - loss: 0.0961\nEpoch 45/100\n32/32 [==============================] - 0s 734us/step - loss: 0.1016\nEpoch 46/100\n32/32 [==============================] - 0s 726us/step - loss: 0.0962\nEpoch 47/100\n32/32 [==============================] - 0s 723us/step - loss: 0.0918\nEpoch 48/100\n32/32 [==============================] - 0s 743us/step - loss: 0.0980\nEpoch 49/100\n32/32 [==============================] - 0s 742us/step - loss: 0.1181\nEpoch 50/100\n32/32 [==============================] - 0s 753us/step - loss: 0.0937\nEpoch 51/100\n32/32 [==============================] - 0s 772us/step - loss: 0.0934\nEpoch 52/100\n32/32 [==============================] - 0s 807us/step - loss: 0.0810\nEpoch 53/100\n32/32 [==============================] - 0s 766us/step - loss: 0.0965\nEpoch 54/100\n32/32 [==============================] - 0s 732us/step - loss: 0.0959\nEpoch 55/100\n32/32 [==============================] - 0s 759us/step - loss: 0.0760\nEpoch 56/100\n32/32 [==============================] - 0s 788us/step - loss: 0.0959\nEpoch 57/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.0865\nEpoch 58/100\n32/32 [==============================] - 0s 991us/step - loss: 0.1034\nEpoch 59/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.0995\nEpoch 60/100\n32/32 [==============================] - 0s 790us/step - loss: 0.1211\nEpoch 61/100\n32/32 [==============================] - 0s 845us/step - loss: 0.0783\nEpoch 62/100\n32/32 [==============================] - 0s 803us/step - loss: 0.0759\nEpoch 63/100\n32/32 [==============================] - 0s 800us/step - loss: 0.0879\nEpoch 64/100\n32/32 [==============================] - 0s 830us/step - loss: 0.0796\nEpoch 65/100\n32/32 [==============================] - 0s 822us/step - loss: 0.0804\nEpoch 66/100\n32/32 [==============================] - 0s 825us/step - loss: 0.0807\nEpoch 67/100\n32/32 [==============================] - 0s 752us/step - loss: 0.0741\nEpoch 68/100\n32/32 [==============================] - 0s 943us/step - loss: 0.0786\nEpoch 69/100\n32/32 [==============================] - 0s 767us/step - loss: 0.0860\nEpoch 70/100\n32/32 [==============================] - 0s 787us/step - loss: 0.0822\nEpoch 71/100\n32/32 [==============================] - 0s 792us/step - loss: 0.0747\nEpoch 72/100\n32/32 [==============================] - 0s 843us/step - loss: 0.0927\nEpoch 73/100\n32/32 [==============================] - 0s 835us/step - loss: 0.1242\nEpoch 74/100\n32/32 [==============================] - 0s 828us/step - loss: 0.1386\nEpoch 75/100\n32/32 [==============================] - 0s 762us/step - loss: 0.1029\nEpoch 76/100\n32/32 [==============================] - 0s 840us/step - loss: 0.0873\nEpoch 77/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.0866\nEpoch 78/100\n32/32 [==============================] - 0s 804us/step - loss: 0.0747\nEpoch 79/100\n32/32 [==============================] - 0s 821us/step - loss: 0.0761\nEpoch 80/100\n32/32 [==============================] - 0s 865us/step - loss: 0.0700\nEpoch 81/100\n32/32 [==============================] - 0s 857us/step - loss: 0.0852\nEpoch 82/100\n32/32 [==============================] - 0s 896us/step - loss: 0.0893\nEpoch 83/100\n32/32 [==============================] - 0s 942us/step - loss: 0.0639\nEpoch 84/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.0904\nEpoch 85/100\n32/32 [==============================] - 0s 847us/step - loss: 0.0718\nEpoch 86/100\n32/32 [==============================] - 0s 872us/step - loss: 0.1000\nEpoch 87/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.0802\nEpoch 88/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.0724\nEpoch 89/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.0723\nEpoch 90/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.0649\nEpoch 91/100\n32/32 [==============================] - 0s 947us/step - loss: 0.0633\nEpoch 92/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.0752\nEpoch 93/100\n32/32 [==============================] - 0s 900us/step - loss: 0.0855\nEpoch 94/100\n32/32 [==============================] - 0s 957us/step - loss: 0.0737\nEpoch 95/100\n32/32 [==============================] - 0s 924us/step - loss: 0.0627\nEpoch 96/100\n32/32 [==============================] - 0s 915us/step - loss: 0.0958\nEpoch 97/100\n32/32 [==============================] - 0s 821us/step - loss: 0.0704\nEpoch 98/100\n32/32 [==============================] - 0s 872us/step - loss: 0.0679\nEpoch 99/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.1009\nEpoch 100/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.0669\n313/313 [==============================] - 0s 674us/step - loss: 0.0735\n\n\n\n\n\n0.07348253577947617\n</code></pre> <pre><code>model_cor = keras.Sequential([\n    keras.layers.Dense(10, activation=\"relu\", input_shape=(3,)),\n    keras.layers.Dense(10, activation=\"relu\", input_shape=(3,)),\n    keras.layers.Dense(1)\n])\n\noptimizer = keras.optimizers.Adam(learning_rate=0.01)\nmodel_cor.compile(optimizer=optimizer, loss=\"mse\")\nmodel_cor.fit(X_cor_train, Y_cor_train, epochs=100)\nmodel_cor.evaluate(X_cor_test, Y_cor_test)\n</code></pre> <pre><code>Epoch 1/100\n32/32 [==============================] - 0s 781us/step - loss: 0.6460\nEpoch 2/100\n32/32 [==============================] - 0s 751us/step - loss: 0.1858\nEpoch 3/100\n32/32 [==============================] - 0s 745us/step - loss: 0.1112\nEpoch 4/100\n32/32 [==============================] - 0s 712us/step - loss: 0.0832\nEpoch 5/100\n32/32 [==============================] - 0s 716us/step - loss: 0.0742\nEpoch 6/100\n32/32 [==============================] - 0s 703us/step - loss: 0.0716\nEpoch 7/100\n32/32 [==============================] - 0s 726us/step - loss: 0.0695\nEpoch 8/100\n32/32 [==============================] - 0s 772us/step - loss: 0.0629\nEpoch 9/100\n32/32 [==============================] - 0s 744us/step - loss: 0.0614\nEpoch 10/100\n32/32 [==============================] - 0s 779us/step - loss: 0.0578\nEpoch 11/100\n32/32 [==============================] - 0s 772us/step - loss: 0.0564\nEpoch 12/100\n32/32 [==============================] - 0s 778us/step - loss: 0.0529\nEpoch 13/100\n32/32 [==============================] - 0s 833us/step - loss: 0.0532\nEpoch 14/100\n32/32 [==============================] - 0s 948us/step - loss: 0.0480\nEpoch 15/100\n32/32 [==============================] - 0s 794us/step - loss: 0.0482\nEpoch 16/100\n32/32 [==============================] - 0s 767us/step - loss: 0.0462\nEpoch 17/100\n32/32 [==============================] - 0s 794us/step - loss: 0.0422\nEpoch 18/100\n32/32 [==============================] - 0s 818us/step - loss: 0.0411\nEpoch 19/100\n32/32 [==============================] - 0s 857us/step - loss: 0.0416\nEpoch 20/100\n32/32 [==============================] - 0s 933us/step - loss: 0.0394\nEpoch 21/100\n32/32 [==============================] - 0s 822us/step - loss: 0.0403\nEpoch 22/100\n32/32 [==============================] - 0s 814us/step - loss: 0.0378\nEpoch 23/100\n32/32 [==============================] - 0s 800us/step - loss: 0.0366\nEpoch 24/100\n32/32 [==============================] - 0s 787us/step - loss: 0.0352\nEpoch 25/100\n32/32 [==============================] - 0s 751us/step - loss: 0.0450\nEpoch 26/100\n32/32 [==============================] - 0s 780us/step - loss: 0.0431\nEpoch 27/100\n32/32 [==============================] - 0s 774us/step - loss: 0.0345\nEpoch 28/100\n32/32 [==============================] - 0s 754us/step - loss: 0.0348\nEpoch 29/100\n32/32 [==============================] - 0s 751us/step - loss: 0.0334\nEpoch 30/100\n32/32 [==============================] - 0s 830us/step - loss: 0.0372\nEpoch 31/100\n32/32 [==============================] - 0s 919us/step - loss: 0.0347\nEpoch 32/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.0322\nEpoch 33/100\n32/32 [==============================] - 0s 892us/step - loss: 0.0319\nEpoch 34/100\n32/32 [==============================] - 0s 913us/step - loss: 0.0350\nEpoch 35/100\n32/32 [==============================] - 0s 959us/step - loss: 0.0325\nEpoch 36/100\n32/32 [==============================] - 0s 913us/step - loss: 0.0303\nEpoch 37/100\n32/32 [==============================] - 0s 973us/step - loss: 0.0293\nEpoch 38/100\n32/32 [==============================] - 0s 946us/step - loss: 0.0339\nEpoch 39/100\n32/32 [==============================] - 0s 909us/step - loss: 0.0289\nEpoch 40/100\n32/32 [==============================] - 0s 995us/step - loss: 0.0303\nEpoch 41/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.0297\nEpoch 42/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.0302\nEpoch 43/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.0258\nEpoch 44/100\n32/32 [==============================] - 0s 795us/step - loss: 0.0304\nEpoch 45/100\n32/32 [==============================] - 0s 803us/step - loss: 0.0279\nEpoch 46/100\n32/32 [==============================] - 0s 815us/step - loss: 0.0307\nEpoch 47/100\n32/32 [==============================] - 0s 782us/step - loss: 0.0251\nEpoch 48/100\n32/32 [==============================] - 0s 835us/step - loss: 0.0289\nEpoch 49/100\n32/32 [==============================] - 0s 896us/step - loss: 0.0305\nEpoch 50/100\n32/32 [==============================] - 0s 932us/step - loss: 0.0265\nEpoch 51/100\n32/32 [==============================] - 0s 954us/step - loss: 0.0257\nEpoch 52/100\n32/32 [==============================] - 0s 931us/step - loss: 0.0284\nEpoch 53/100\n32/32 [==============================] - 0s 871us/step - loss: 0.0260\nEpoch 54/100\n32/32 [==============================] - 0s 1ms/step - loss: 0.0273\nEpoch 55/100\n32/32 [==============================] - 0s 852us/step - loss: 0.0268\nEpoch 56/100\n32/32 [==============================] - 0s 846us/step - loss: 0.0267\nEpoch 57/100\n32/32 [==============================] - 0s 776us/step - loss: 0.0256\nEpoch 58/100\n32/32 [==============================] - 0s 807us/step - loss: 0.0262\nEpoch 59/100\n32/32 [==============================] - 0s 796us/step - loss: 0.0395\nEpoch 60/100\n32/32 [==============================] - 0s 884us/step - loss: 0.0253\nEpoch 61/100\n32/32 [==============================] - 0s 961us/step - loss: 0.0251\nEpoch 62/100\n32/32 [==============================] - 0s 790us/step - loss: 0.0250\nEpoch 63/100\n32/32 [==============================] - 0s 798us/step - loss: 0.0239\nEpoch 64/100\n32/32 [==============================] - 0s 793us/step - loss: 0.0257\nEpoch 65/100\n32/32 [==============================] - 0s 841us/step - loss: 0.0241\nEpoch 66/100\n32/32 [==============================] - 0s 799us/step - loss: 0.0290\nEpoch 67/100\n32/32 [==============================] - 0s 850us/step - loss: 0.0225\nEpoch 68/100\n32/32 [==============================] - 0s 790us/step - loss: 0.0225\nEpoch 69/100\n32/32 [==============================] - 0s 829us/step - loss: 0.0248\nEpoch 70/100\n32/32 [==============================] - 0s 810us/step - loss: 0.0245\nEpoch 71/100\n32/32 [==============================] - 0s 796us/step - loss: 0.0249\nEpoch 72/100\n32/32 [==============================] - 0s 807us/step - loss: 0.0258\nEpoch 73/100\n32/32 [==============================] - 0s 800us/step - loss: 0.0231\nEpoch 74/100\n32/32 [==============================] - 0s 856us/step - loss: 0.0242\nEpoch 75/100\n32/32 [==============================] - 0s 896us/step - loss: 0.0230\nEpoch 76/100\n32/32 [==============================] - 0s 880us/step - loss: 0.0292\nEpoch 77/100\n32/32 [==============================] - 0s 886us/step - loss: 0.0239\nEpoch 78/100\n32/32 [==============================] - 0s 826us/step - loss: 0.0235\nEpoch 79/100\n32/32 [==============================] - 0s 802us/step - loss: 0.0251\nEpoch 80/100\n32/32 [==============================] - 0s 740us/step - loss: 0.0243\nEpoch 81/100\n32/32 [==============================] - 0s 749us/step - loss: 0.0269\nEpoch 82/100\n32/32 [==============================] - 0s 892us/step - loss: 0.0272\nEpoch 83/100\n32/32 [==============================] - 0s 798us/step - loss: 0.0226\nEpoch 84/100\n32/32 [==============================] - 0s 765us/step - loss: 0.0241\nEpoch 85/100\n32/32 [==============================] - 0s 765us/step - loss: 0.0277\nEpoch 86/100\n32/32 [==============================] - 0s 753us/step - loss: 0.0232\nEpoch 87/100\n32/32 [==============================] - 0s 763us/step - loss: 0.0238\nEpoch 88/100\n32/32 [==============================] - 0s 828us/step - loss: 0.0210\nEpoch 89/100\n32/32 [==============================] - 0s 781us/step - loss: 0.0228\nEpoch 90/100\n32/32 [==============================] - 0s 763us/step - loss: 0.0232\nEpoch 91/100\n32/32 [==============================] - 0s 769us/step - loss: 0.0214\nEpoch 92/100\n32/32 [==============================] - 0s 753us/step - loss: 0.0204\nEpoch 93/100\n32/32 [==============================] - 0s 711us/step - loss: 0.0352\nEpoch 94/100\n32/32 [==============================] - 0s 718us/step - loss: 0.0225\nEpoch 95/100\n32/32 [==============================] - 0s 745us/step - loss: 0.0209\nEpoch 96/100\n32/32 [==============================] - 0s 750us/step - loss: 0.0253\nEpoch 97/100\n32/32 [==============================] - 0s 845us/step - loss: 0.0244\nEpoch 98/100\n32/32 [==============================] - 0s 813us/step - loss: 0.0289\nEpoch 99/100\n32/32 [==============================] - 0s 805us/step - loss: 0.0221\nEpoch 100/100\n32/32 [==============================] - 0s 764us/step - loss: 0.0239\n313/313 [==============================] - 0s 740us/step - loss: 0.0282\n\n\n\n\n\n0.02815949358046055\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#pdp","title":"PDP","text":""},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#uncorrelated-setting","title":"Uncorrelated setting","text":""},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#global-pdp","title":"Global PDP","text":"<pre><code>pdp = effector.PDP(data=X_uncor_train, model=model_uncor, feature_names=['x1','x2','x3'], target_name=\"Y\")\npdp.plot(feature=0, centering=True, show_avg_output=False, heterogeneity=\"ice\", y_limits=[-5, 5])\npdp.plot(feature=1, centering=True, show_avg_output=False, heterogeneity=\"ice\", y_limits=[-5, 5])\npdp.plot(feature=2, centering=True, show_avg_output=False, heterogeneity=\"ice\", y_limits=[-5, 5])\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#regional-pdp","title":"Regional PDP","text":"<pre><code>regional_pdp = effector.RegionalPDP(data=X_uncor_train, model=model_uncor, feature_names=['x1','x2','x3'], axis_limits=np.array([[-1,1],[-1,1],[-1,1]]).T)\nregional_pdp.fit(features=\"all\", heter_pcg_drop_thres=0.3, nof_candidate_splits_for_numerical=11)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01&lt;00:00,  2.12it/s]\n</code></pre> <pre><code>regional_pdp.show_partitioning(features=0)\n</code></pre> <pre><code>Feature 0 - Full partition tree:\nNode id: 0, name: x1, heter: 1.71 || nof_instances:  1000 || weight: 1.00\n        Node id: 1, name: x1 | x3 &lt;= -0.0, heter: 0.35 || nof_instances:   498 || weight: 0.50\n        Node id: 2, name: x1 | x3  &gt; -0.0, heter: 0.35 || nof_instances:   502 || weight: 0.50\n--------------------------------------------------\nFeature 0 - Statistics per tree level:\nLevel 0, heter: 1.71\n        Level 1, heter: 0.35 || heter drop: 1.36 (79.49%)\n</code></pre> <pre><code>regional_pdp.plot(feature=0, node_idx=1, heterogeneity=\"ice\", y_limits=[-5, 5])\nregional_pdp.plot(feature=0, node_idx=2, heterogeneity=\"ice\", y_limits=[-5, 5])\n</code></pre> <pre><code>regional_pdp.show_partitioning(features=1)\n</code></pre> <pre><code>Feature 1 - Full partition tree:\nNode id: 0, name: x2, heter: 1.81 || nof_instances:  1000 || weight: 1.00\n--------------------------------------------------\nFeature 1 - Statistics per tree level:\nLevel 0, heter: 1.81\n</code></pre> <pre><code>regional_pdp.show_partitioning(features=2)\n</code></pre> <pre><code>Feature 2 - Full partition tree:\nNode id: 0, name: x3, heter: 1.72 || nof_instances:  1000 || weight: 1.00\n        Node id: 1, name: x3 | x1 &lt;= -0.0, heter: 0.84 || nof_instances:   494 || weight: 0.49\n        Node id: 2, name: x3 | x1  &gt; -0.0, heter: 0.87 || nof_instances:   506 || weight: 0.51\n--------------------------------------------------\nFeature 2 - Statistics per tree level:\nLevel 0, heter: 1.72\n        Level 1, heter: 0.85 || heter drop: 0.86 (50.17%)\n</code></pre> <pre><code>regional_pdp.plot(feature=2, node_idx=1, heterogeneity=\"ice\", centering=True, y_limits=[-5, 5])\nregional_pdp.plot(feature=2, node_idx=2, heterogeneity=\"ice\", centering=True, y_limits=[-5, 5])\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#conclusion","title":"Conclusion","text":"<p>For the Global PDP:</p> <ul> <li>the average effect of \\(x_1\\) is \\(0\\) with some heterogeneity implied by the interaction with \\(x_1\\). The heterogeneity is expressed with two opposite lines; \\(-3x_1\\) when \\(x_1 \\leq 0\\) and \\(3x_1\\) when \\(x_1 &gt;0\\)</li> <li>the average effect of \\(x_2\\) to be \\(0\\) without heterogeneity</li> <li>the average effect of \\(x_3\\) to be \\(x_3\\) with some heterogeneity due to the interaction with \\(x_1\\). The heterogeneity is expressed with a discontinuity around \\(x_3=0\\), with either a positive or a negative offset depending on the value of \\(x_1^i\\)</li> </ul> <p>For the Regional PDP:</p> <ul> <li>For \\(x_1\\), the algorithm finds two regions, one for \\(x_3 \\leq 0\\) and one for \\(x_3 &gt; 0\\)</li> <li>when \\(x_3&gt;0\\) the effect is \\(3x_1\\)</li> <li>when \\(x_3 \\leq 0\\), the effect is \\(-3x_1\\)</li> <li>For \\(x_2\\) the algorithm does not find any subregion </li> <li>For \\(x_3\\), there is a change in the offset:</li> <li>when \\(x_1&gt;0\\) the line is \\(x_3 - 3x_1^i\\) in the first half and \\(x_3 + 3x_1^i\\) later</li> <li>when \\(x_1&lt;0\\) the line is \\(x_3 + 3x_1^i\\) in the first half and \\(x_3 - 3x_1^i\\) later</li> </ul>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#correlated-setting","title":"Correlated setting","text":""},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#global-pdp_1","title":"Global PDP","text":"<pre><code>pdp = effector.PDP(data=X_cor_train, model=model_cor, feature_names=['x1','x2','x3'], target_name=\"Y\")\npdp.plot(feature=0, centering=True, show_avg_output=False, heterogeneity=\"ice\", y_limits=[-5, 5])\npdp.plot(feature=1, centering=True, show_avg_output=False, heterogeneity=\"ice\", y_limits=[-5, 5])\npdp.plot(feature=2, centering=True, show_avg_output=False, heterogeneity=\"ice\", y_limits=[-5, 5])\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#regional-pdp_1","title":"Regional-PDP","text":"<pre><code>regional_pdp = effector.RegionalPDP(data=X_cor_train, model=model_cor, feature_names=['x1','x2','x3'], axis_limits=np.array([[-1,1],[-1,1],[-1,1]]).T)\nregional_pdp.fit(features=\"all\", heter_pcg_drop_thres=0.4, nof_candidate_splits_for_numerical=11)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01&lt;00:00,  2.16it/s]\n</code></pre> <pre><code>regional_pdp.show_partitioning(features=0)\n</code></pre> <pre><code>Feature 0 - Full partition tree:\nNode id: 0, name: x1, heter: 2.39 || nof_instances:  1000 || weight: 1.00\n        Node id: 1, name: x1 | x3 &lt;= -0.0, heter: 0.36 || nof_instances:   491 || weight: 0.49\n        Node id: 2, name: x1 | x3  &gt; -0.0, heter: 0.40 || nof_instances:   509 || weight: 0.51\n--------------------------------------------------\nFeature 0 - Statistics per tree level:\nLevel 0, heter: 2.39\n        Level 1, heter: 0.38 || heter drop: 2.01 (84.19%)\n</code></pre> <pre><code>regional_pdp.plot(feature=0, node_idx=1, heterogeneity=\"ice\", centering=True, y_limits=[-5, 5])\nregional_pdp.plot(feature=0, node_idx=2, heterogeneity=\"ice\", centering=True, y_limits=[-5, 5])\n</code></pre> <pre><code>regional_pdp.show_partitioning(features=1)\n</code></pre> <pre><code>Feature 1 - Full partition tree:\nNode id: 0, name: x2, heter: 1.24 || nof_instances:  1000 || weight: 1.00\n--------------------------------------------------\nFeature 1 - Statistics per tree level:\nLevel 0, heter: 1.24\n</code></pre> <pre><code>regional_pdp.show_partitioning(features=2)\n</code></pre> <pre><code>Feature 2 - Full partition tree:\nNode id: 0, name: x3, heter: 1.81 || nof_instances:  1000 || weight: 1.00\n        Node id: 1, name: x3 | x1 &lt;= 0.14, heter: 1.03 || nof_instances:   576 || weight: 0.58\n        Node id: 2, name: x3 | x1  &gt; 0.14, heter: 0.91 || nof_instances:   424 || weight: 0.42\n--------------------------------------------------\nFeature 2 - Statistics per tree level:\nLevel 0, heter: 1.81\n        Level 1, heter: 0.98 || heter drop: 0.83 (45.84%)\n</code></pre> <pre><code>regional_pdp.plot(feature=2, node_idx=1, heterogeneity=\"ice\", centering=True, y_limits=[-5, 5])\nregional_pdp.plot(feature=2, node_idx=2, heterogeneity=\"ice\", centering=True, y_limits=[-5, 5])\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#conclusion_1","title":"Conclusion","text":""},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#rhale","title":"(RH)ALE","text":"<pre><code>def model_uncor_jac(x):\n    x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n    with tf.GradientTape() as t:\n        t.watch(x_tensor)\n        pred = model_uncor(x_tensor)\n        grads = t.gradient(pred, x_tensor)\n    return grads.numpy()\n\ndef model_cor_jac(x):\n    x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n    with tf.GradientTape() as t:\n        t.watch(x_tensor)\n        pred = model_cor(x_tensor)\n        grads = t.gradient(pred, x_tensor)\n    return grads.numpy()\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#uncorrelated-setting_1","title":"Uncorrelated setting","text":""},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#global-rhale","title":"Global RHALE","text":"<pre><code>rhale = effector.RHALE(data=X_uncor_train, model=model_uncor, model_jac=model_uncor_jac, feature_names=['x1','x2','x3'], target_name=\"Y\")\n\nbinning_method = effector.binning_methods.Fixed(10, min_points_per_bin=0)\nrhale.fit(features=\"all\", binning_method=binning_method, centering=True)\n\nrhale.plot(feature=0, centering=True, heterogeneity=\"std\", show_avg_output=False, y_limits=[-5, 5], dy_limits=[-5, 5])\nrhale.plot(feature=1, centering=True, heterogeneity=\"std\", show_avg_output=False, y_limits=[-5, 5], dy_limits=[-5, 5])\nrhale.plot(feature=2, centering=True, heterogeneity=\"std\", show_avg_output=False, y_limits=[-5, 5], dy_limits=[-5, 5])\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#regional-rhale","title":"Regional RHALE","text":"<pre><code>regional_rhale = effector.RegionalRHALE(\n    data=X_uncor_train, \n    model=model_uncor, \n    model_jac= model_uncor_jac, \n    feature_names=['x1', 'x2', 'x3'],\n    axis_limits=np.array([[-1, 1], [-1, 1], [-1, 1]]).T) \n\nbinning_method = effector.binning_methods.Fixed(11, min_points_per_bin=0)\nregional_rhale.fit(\n    features=\"all\",\n    heter_pcg_drop_thres=0.6,\n    binning_method=binning_method,\n    nof_candidate_splits_for_numerical=11\n)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00,  3.72it/s]\n</code></pre> <pre><code>regional_rhale.show_partitioning(features=0)\n</code></pre> <pre><code>Feature 0 - Full partition tree:\nNode id: 0, name: x1, heter: 5.81 || nof_instances:  1000 || weight: 1.00\n        Node id: 1, name: x1 | x3 &lt;= -0.0, heter: 0.95 || nof_instances:   498 || weight: 0.50\n        Node id: 2, name: x1 | x3  &gt; -0.0, heter: 0.77 || nof_instances:   502 || weight: 0.50\n--------------------------------------------------\nFeature 0 - Statistics per tree level:\nLevel 0, heter: 5.81\n        Level 1, heter: 0.86 || heter drop: 4.95 (85.21%)\n</code></pre> <pre><code>regional_rhale.plot(feature=0, node_idx=1, heterogeneity=\"std\", centering=True, y_limits=[-5, 5])\nregional_rhale.plot(feature=0, node_idx=2, heterogeneity=\"std\", centering=True, y_limits=[-5, 5])\n</code></pre> <pre><code>regional_rhale.show_partitioning(features=1)\n</code></pre> <pre><code>Feature 1 - Full partition tree:\nNode id: 0, name: x2, heter: 0.23 || nof_instances:  1000 || weight: 1.00\n--------------------------------------------------\nFeature 1 - Statistics per tree level:\nLevel 0, heter: 0.23\n</code></pre> <pre><code>regional_rhale.show_partitioning(features=2)\n</code></pre> <pre><code>Feature 2 - Full partition tree:\nNode id: 0, name: x3, heter: 5.48 || nof_instances:  1000 || weight: 1.00\n--------------------------------------------------\nFeature 2 - Statistics per tree level:\nLevel 0, heter: 5.48\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#conclusion_2","title":"Conclusion","text":""},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#correlated-setting_1","title":"Correlated setting","text":""},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#global-rhale_1","title":"Global RHALE","text":"<pre><code>rhale = effector.RHALE(data=X_cor_train, model=model_cor, model_jac=model_cor_jac, feature_names=['x1','x2','x3'], target_name=\"Y\")\n\nbinning_method = effector.binning_methods.Fixed(10, min_points_per_bin=0)\nrhale.fit(features=\"all\", binning_method=binning_method, centering=True)\n</code></pre> <pre><code>rhale.plot(feature=0, centering=True, heterogeneity=\"std\", show_avg_output=False, y_limits=[-5, 5], dy_limits=[-5, 5])\nrhale.plot(feature=1, centering=True, heterogeneity=\"std\", show_avg_output=False, y_limits=[-5, 5], dy_limits=[-5, 5])\nrhale.plot(feature=2, centering=True, heterogeneity=\"std\", show_avg_output=False, y_limits=[-5, 5], dy_limits=[-5, 5])\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#regional-rhale_1","title":"Regional RHALE","text":"<pre><code>regional_rhale = effector.RegionalRHALE(\n    data=X_cor_train, \n    model=model_cor, \n    model_jac= model_cor_jac, \n    feature_names=['x1', 'x2', 'x3'],\n    axis_limits=np.array([[-1, 1], [-1, 1], [-1, 1]]).T) \n\nbinning_method = effector.binning_methods.Fixed(11, min_points_per_bin=0)\nregional_rhale.fit(\n    features=\"all\",\n    heter_pcg_drop_thres=0.6,\n    binning_method=binning_method,\n    nof_candidate_splits_for_numerical=11\n)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00,  3.65it/s]\n</code></pre> <pre><code>regional_rhale.show_partitioning(features=0)\n</code></pre> <pre><code>Feature 0 - Full partition tree:\nNode id: 0, name: x1, heter: 2.20 || nof_instances:  1000 || weight: 1.00\n--------------------------------------------------\nFeature 0 - Statistics per tree level:\nLevel 0, heter: 2.20\n</code></pre> <pre><code>regional_rhale.show_partitioning(features=1)\n</code></pre> <pre><code>Feature 1 - Full partition tree:\nNode id: 0, name: x2, heter: 0.17 || nof_instances:  1000 || weight: 1.00\n--------------------------------------------------\nFeature 1 - Statistics per tree level:\nLevel 0, heter: 0.17\n</code></pre> <pre><code>regional_rhale.show_partitioning(features=2)\n</code></pre> <pre><code>Feature 2 - Full partition tree:\nNode id: 0, name: x3, heter: 2.26 || nof_instances:  1000 || weight: 1.00\n--------------------------------------------------\nFeature 2 - Statistics per tree level:\nLevel 0, heter: 2.26\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#conclusion_3","title":"Conclusion","text":""},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#shap-dp","title":"SHAP DP","text":""},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#uncorrelated-setting_2","title":"Uncorrelated setting","text":""},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#global-shap-dp","title":"Global SHAP DP","text":"<pre><code>shap = effector.SHAPDependence(data=X_uncor_train, model=model_uncor, feature_names=['x1', 'x2', 'x3'], target_name=\"Y\")\n\nshap.plot(feature=0, centering=True, heterogeneity=\"shap_values\", show_avg_output=False, y_limits=[-3, 3])\nshap.plot(feature=1, centering=True, heterogeneity=\"shap_values\", show_avg_output=False, y_limits=[-3, 3])\nshap.plot(feature=2, centering=True, heterogeneity=\"shap_values\", show_avg_output=False, y_limits=[-3, 3])\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#regional-shap-dp","title":"Regional SHAP-DP","text":"<pre><code>regional_shap = effector.RegionalSHAP(\n    data=X_uncor_train,\n    model=model_uncor,\n    feature_names=['x1', 'x2', 'x3'],\n    axis_limits=np.array([[-1, 1], [-1, 1], [-1, 1]]).T)\n\nregional_shap.fit(\n    features=\"all\",\n    heter_pcg_drop_thres=0.6,\n    nof_candidate_splits_for_numerical=11\n)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:47&lt;00:00, 15.80s/it]\n</code></pre> <pre><code>regional_shap.show_partitioning(0)\n</code></pre> <pre><code>Feature 0 - Full partition tree:\nNode id: 0, name: x1, heter: 0.78 || nof_instances:   100 || weight: 1.00\n        Node id: 1, name: x1 | x3 &lt;= -0.01, heter: 0.02 || nof_instances:    58 || weight: 0.58\n        Node id: 2, name: x1 | x3  &gt; -0.01, heter: 0.04 || nof_instances:    42 || weight: 0.42\n--------------------------------------------------\nFeature 0 - Statistics per tree level:\nLevel 0, heter: 0.78\n        Level 1, heter: 0.03 || heter drop: 0.75 (96.38%)\n</code></pre> <pre><code>regional_shap.plot(feature=0, node_idx=1, heterogeneity=\"std\", centering=True, y_limits=[-5, 5])\nregional_shap.plot(feature=0, node_idx=2, heterogeneity=\"std\", centering=True, y_limits=[-5, 5])\n</code></pre> <pre><code>regional_shap.show_partitioning(features=1)\n</code></pre> <pre><code>Feature 1 - Full partition tree:\nNode id: 0, name: x2, heter: 0.02 || nof_instances:   100 || weight: 1.00\n--------------------------------------------------\nFeature 1 - Statistics per tree level:\nLevel 0, heter: 0.02\n</code></pre> <pre><code>regional_shap.show_partitioning(features=2)\n</code></pre> <pre><code>Feature 2 - Full partition tree:\nNode id: 0, name: x3, heter: 0.72 || nof_instances:   100 || weight: 1.00\n--------------------------------------------------\nFeature 2 - Statistics per tree level:\nLevel 0, heter: 0.72\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#conclusion_4","title":"Conclusion","text":""},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#correlated-setting_2","title":"Correlated setting","text":""},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#global-shap-dp_1","title":"Global SHAP-DP","text":"<pre><code>shap = effector.SHAPDependence(data=X_cor_train, model=model_cor, feature_names=['x1', 'x2', 'x3'], target_name=\"Y\")\n\nshap.plot(feature=0, centering=True, heterogeneity=\"shap_values\", show_avg_output=False, y_limits=[-3, 3])\nshap.plot(feature=1, centering=True, heterogeneity=\"shap_values\", show_avg_output=False, y_limits=[-3, 3])\nshap.plot(feature=2, centering=True, heterogeneity=\"shap_values\", show_avg_output=False, y_limits=[-3, 3])\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#regional-shap","title":"Regional SHAP","text":"<pre><code>regional_shap = effector.RegionalSHAP(\n    data=X_cor_train,\n    model=model_cor,\n    feature_names=['x1', 'x2', 'x3'],\n    axis_limits=np.array([[-1, 1], [-1, 1], [-1, 1]]).T)\n\nregional_shap.fit(\n    features=\"all\",\n    heter_pcg_drop_thres=0.6,\n    nof_candidate_splits_for_numerical=11\n)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:46&lt;00:00, 15.62s/it]\n</code></pre> <pre><code>regional_shap.show_partitioning(0)\nregional_shap.show_partitioning(1)\nregional_shap.show_partitioning(2)\n</code></pre> <pre><code>Feature 0 - Full partition tree:\nNode id: 0, name: x1, heter: 0.17 || nof_instances:   100 || weight: 1.00\n--------------------------------------------------\nFeature 0 - Statistics per tree level:\nLevel 0, heter: 0.17\nFeature 1 - Full partition tree:\nNode id: 0, name: x2, heter: 0.01 || nof_instances:   100 || weight: 1.00\n--------------------------------------------------\nFeature 1 - Statistics per tree level:\nLevel 0, heter: 0.01\nFeature 2 - Full partition tree:\nNode id: 0, name: x3, heter: 0.29 || nof_instances:   100 || weight: 1.00\n--------------------------------------------------\nFeature 2 - Statistics per tree level:\nLevel 0, heter: 0.29\n</code></pre>"},{"location":"Tutorials/synthetic-examples/04_regional_effects_real_f/#conclusion_5","title":"Conclusion","text":""}]}