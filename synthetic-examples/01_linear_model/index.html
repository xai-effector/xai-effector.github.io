
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../../real-examples/01_bike_sharing_dataset/">
      
      
        <link rel="next" href="../02_rhale_vs_ale_vs_pdp/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.3">
    
    
      
        <title>Intro to Feature Effect methods with a linear model - Effector Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.50c56a3b.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Flex:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto Flex";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#intro-to-feature-effect-methods-with-a-linear-model" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Effector Documentation" class="md-header__button md-logo" aria-label="Effector Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Effector Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Intro to Feature Effect methods with a linear model
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Effector Documentation" class="md-nav__button md-logo" aria-label="Effector Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Effector Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting Started
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../01_global_effect_intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Global Feature Effect
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../02_regional_effect_intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Regional Feature Effect
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../03_API/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API reference
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Real examples
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Real examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../real-examples/01_bike_sharing_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bike-Sharing Dataset
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" checked>
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Synthetic examples
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Synthetic examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Intro to Feature Effect methods with a linear model
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Intro to Feature Effect methods with a linear model
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#global-feature-effect-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Global Feature Effect methods
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dataset-and-model" class="md-nav__link">
    <span class="md-ellipsis">
      Dataset and Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#partial-dependence-plot-pdp" class="md-nav__link">
    <span class="md-ellipsis">
      Partial Dependence Plot (PDP)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Partial Dependence Plot (PDP)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#feature-effect-interpretation" class="md-nav__link">
    <span class="md-ellipsis">
      Feature effect interpretation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneity" class="md-nav__link">
    <span class="md-ellipsis">
      Heterogeneity
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Heterogeneity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#option-a-ice-plots" class="md-nav__link">
    <span class="md-ellipsis">
      Option (a): ICE plots
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#option-b-std-of-the-residuals" class="md-nav__link">
    <span class="md-ellipsis">
      Option (b): STD of the residuals
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#derivative-pdp-d-pdp" class="md-nav__link">
    <span class="md-ellipsis">
      Derivative-PDP (d-PDP)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#accumulated-local-effects-ale" class="md-nav__link">
    <span class="md-ellipsis">
      Accumulated Local Effects (ALE)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Accumulated Local Effects (ALE)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fearure-effect-interpretation" class="md-nav__link">
    <span class="md-ellipsis">
      Fearure effect interpretation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneity_1" class="md-nav__link">
    <span class="md-ellipsis">
      Heterogeneity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bin-splitting" class="md-nav__link">
    <span class="md-ellipsis">
      Bin-Splitting
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robust-and-heterogeneity-aware-ale-rhale" class="md-nav__link">
    <span class="md-ellipsis">
      Robust and Heterogeneity-aware ALE (RHALE)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Robust and Heterogeneity-aware ALE (RHALE)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fearure-effect-interpretation_1" class="md-nav__link">
    <span class="md-ellipsis">
      Fearure effect interpretation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneity_2" class="md-nav__link">
    <span class="md-ellipsis">
      Heterogeneity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bin-splitting_1" class="md-nav__link">
    <span class="md-ellipsis">
      Bin-Splitting
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#shap-dependence-plot" class="md-nav__link">
    <span class="md-ellipsis">
      SHAP Dependence Plot
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SHAP Dependence Plot">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fearure-effect-interpretation_2" class="md-nav__link">
    <span class="md-ellipsis">
      Fearure effect interpretation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneity_3" class="md-nav__link">
    <span class="md-ellipsis">
      Heterogeneity
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02_rhale_vs_ale_vs_pdp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RHALE vs ALE vs PDP
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03_regional_effects_synthetic_f/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Regional Effects (known black-box function)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04_regional_effects_real_f/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Regional Effects (unknown black-box function)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#global-feature-effect-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Global Feature Effect methods
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dataset-and-model" class="md-nav__link">
    <span class="md-ellipsis">
      Dataset and Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#partial-dependence-plot-pdp" class="md-nav__link">
    <span class="md-ellipsis">
      Partial Dependence Plot (PDP)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Partial Dependence Plot (PDP)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#feature-effect-interpretation" class="md-nav__link">
    <span class="md-ellipsis">
      Feature effect interpretation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneity" class="md-nav__link">
    <span class="md-ellipsis">
      Heterogeneity
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Heterogeneity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#option-a-ice-plots" class="md-nav__link">
    <span class="md-ellipsis">
      Option (a): ICE plots
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#option-b-std-of-the-residuals" class="md-nav__link">
    <span class="md-ellipsis">
      Option (b): STD of the residuals
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#derivative-pdp-d-pdp" class="md-nav__link">
    <span class="md-ellipsis">
      Derivative-PDP (d-PDP)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#accumulated-local-effects-ale" class="md-nav__link">
    <span class="md-ellipsis">
      Accumulated Local Effects (ALE)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Accumulated Local Effects (ALE)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fearure-effect-interpretation" class="md-nav__link">
    <span class="md-ellipsis">
      Fearure effect interpretation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneity_1" class="md-nav__link">
    <span class="md-ellipsis">
      Heterogeneity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bin-splitting" class="md-nav__link">
    <span class="md-ellipsis">
      Bin-Splitting
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robust-and-heterogeneity-aware-ale-rhale" class="md-nav__link">
    <span class="md-ellipsis">
      Robust and Heterogeneity-aware ALE (RHALE)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Robust and Heterogeneity-aware ALE (RHALE)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fearure-effect-interpretation_1" class="md-nav__link">
    <span class="md-ellipsis">
      Fearure effect interpretation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneity_2" class="md-nav__link">
    <span class="md-ellipsis">
      Heterogeneity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bin-splitting_1" class="md-nav__link">
    <span class="md-ellipsis">
      Bin-Splitting
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#shap-dependence-plot" class="md-nav__link">
    <span class="md-ellipsis">
      SHAP Dependence Plot
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SHAP Dependence Plot">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fearure-effect-interpretation_2" class="md-nav__link">
    <span class="md-ellipsis">
      Fearure effect interpretation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneity_3" class="md-nav__link">
    <span class="md-ellipsis">
      Heterogeneity
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="intro-to-feature-effect-methods-with-a-linear-model">Intro to Feature Effect methods with a linear model</h1>
<p>This tutorial is a (slow and gentle) introduction to the basic global <a href="https://christophm.github.io/interpretable-ml-book/global-methods.html">feature effect methods</a> and the <code>Effector</code> package. If you only care about using <code>Effector</code>'s API, you can go directly to the <a href="#conclusion">Conclusion</a>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">effector</span>
</code></pre></div>
<h2 id="global-feature-effect-methods">Global Feature Effect methods</h2>
<p>Feature effect methods estimate the <strong>average</strong> effect of a specific feature on the model's output, i.e., they create a 1-1 mapping between the feature of interest <span class="arithmatex">\(x_s\)</span> and the output of the model <span class="arithmatex">\(y\)</span>. </p>
<p>Feature effect are perfect explainers for additive models, i.e., models of the form <span class="arithmatex">\(f(\mathbf{x}) = \sum_{i=1}^D f(x_i)\)</span>. Black-box models, however, are not additive; they have complex interaction terms between two <span class="arithmatex">\(f(x_i, x_j)\)</span>, three <span class="arithmatex">\(f(x_i, x_j, x_k)\)</span> or even all features <span class="arithmatex">\(f(x_1, \cdots, x_D)\)</span>. In these cases, feature effect methods simplify things by <em>distributing</em> the effect of interaction terms to the individual features. </p>
<p>This simplification is acceptable when the interaction terms are weak, i.e., they are not so important for the model's prediction.
However, when the interaction terms are very strong then the feature effect methods may provide an over-simplistic explanation.
Hopefully, there is a quantity called <strong>heterogeneity</strong> that can be used to check whether the feature effect methods are a good explanation for the features.</p>
<p><code>Effector</code> provides five different feature effect methods, which are summarized in the table below. In all methods, setting <code>heterogeneity=True</code> the methods show the level of heterogeneity, along with the average effect.</p>
<p><center></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
<th>API in <code>Effector</code></th>
<th>Paper</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#partial-dependence-plot-pdp">PDP</a></td>
<td>Partial Dependence Plot</td>
<td><a href="(./../../reference/#effector.pdp.DerivativePDP)">PDP</a></td>
<td><a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">Friedman, 2001</a></td>
</tr>
<tr>
<td><a href="#derivative-pdp-d-pdp">d-PDP</a></td>
<td>Derivative PDP</td>
<td><a href="./../../reference/#effector.pdp.DerivativePDP">DerivativePDP</a></td>
<td><a href="https://arxiv.org/pdf/1309.6392.pdf">Goldstein et. al, 2013</a></td>
</tr>
<tr>
<td><a href="#accumulated-local-effects-ale">ALE</a></td>
<td>Accumulated Local Effect</td>
<td><a href="./../../reference/#effector.ale.ALE">ALE</a></td>
<td><a href="https://arxiv.org/pdf/1612.08468">Apley et. al, 2016</a></td>
</tr>
<tr>
<td><a href="#robust-and-heterogeneity-aware-ale-rhale">RHALE</a></td>
<td>Robust and Heterogeneity-aware ALE</td>
<td><a href="./../../reference/#effector.ale.RHALE">RHALE</a></td>
<td><a href="https://arxiv.org/abs/2309.11193">Gkolemis et al, 2023</a></td>
</tr>
<tr>
<td><a href="#shap-dependence-plot">SHAP</a></td>
<td>SHAP Dependence Plot</td>
<td><a href="./../../reference/#effector.shap_dependence.SHAPDependence">SHAPDependence</a></td>
<td><a href="https://arxiv.org/pdf/1705.07874.pdf">Lundberg et. al, 2017</a></td>
</tr>
</tbody>
</table>
<p></center></p>
<p>For the rest of the tutorial, we will use the following notation for the rest of the tutorial:</p>
<p><center></p>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(f(\mathbf{x})\)</span></td>
<td>The black box model</td>
</tr>
<tr>
<td><span class="arithmatex">\(\mathcal{H}\)</span></td>
<td>The heterogeneity</td>
</tr>
<tr>
<td><span class="arithmatex">\(x_s\)</span></td>
<td>The feature of interest</td>
</tr>
<tr>
<td><span class="arithmatex">\(x_c\)</span></td>
<td>The remaining features, i.e., <span class="arithmatex">\(\mathbf{x} = (x_s, x_c)\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(\mathbf{x} = (x_s, x_c) = (x_1, x_2, ..., x_s, ..., x_D)\)</span></td>
<td>The input features</td>
</tr>
<tr>
<td><span class="arithmatex">\(\mathbf{x}^{(i)} = (x_s^{(i)}, x_c^{(i)})\)</span></td>
<td>The <span class="arithmatex">\(i\)</span>-th instance of the dataset</td>
</tr>
</tbody>
</table>
<p></center></p>
<hr />
<h2 id="dataset-and-model">Dataset and Model</h2>
<p>In this example, we will use as black-box function, a simple linear model <span class="arithmatex">\(y = 7x_1 - 3x_2 + 4x_3\)</span>. Since there are no interactions 
terms we expect <strong>all</strong> methods to provide the following feature effects and zero heterogeneity:</p>
<p><center></p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Feature Effect</th>
<th>Heterogeneity</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(x_1\)</span></td>
<td><span class="arithmatex">\(7x_1\)</span></td>
<td>0</td>
</tr>
<tr>
<td><span class="arithmatex">\(x_2\)</span></td>
<td><span class="arithmatex">\(-3x_2\)</span></td>
<td>0</td>
</tr>
<tr>
<td><span class="arithmatex">\(x_3\)</span></td>
<td><span class="arithmatex">\(4x_3\)</span></td>
<td>0</td>
</tr>
</tbody>
</table>
<p></center></p>
<p>As dataset, we will generate <span class="arithmatex">\(N=1000\)</span> examples comming from the following distribution:</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Description</th>
<th>Distribution</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(x_1\)</span></td>
<td>Uniformly distributed between <span class="arithmatex">\(0\)</span> and <span class="arithmatex">\(1\)</span></td>
<td><span class="arithmatex">\(x_1 \sim \mathcal{U}(0,1)\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(x_2\)</span></td>
<td>Follows <span class="arithmatex">\(x_1\)</span> with some added noise</td>
<td>$x_2 = x_1 + \epsilon $, <span class="arithmatex">\(\epsilon \sim \mathcal{N}(0, 0.1)\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(x_3\)</span></td>
<td>Uniformly distributed between <span class="arithmatex">\(0\)</span> and <span class="arithmatex">\(1\)</span></td>
<td><span class="arithmatex">\(x_3 \sim \mathcal{U}(0,1)\)</span></td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">generate_dataset</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span><span class="p">,</span> <span class="n">x2_sigma</span><span class="p">,</span> <span class="n">x3_sigma</span><span class="p">):</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">x1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">x2_sigma</span><span class="p">)</span>
    <span class="n">x3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># generate the dataset</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">21</span><span class="p">)</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x1_min</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">x1_max</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">x2_sigma</span> <span class="o">=</span> <span class="mf">.1</span>
<span class="n">x3_sigma</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">generate_dataset</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span><span class="p">,</span> <span class="n">x2_sigma</span><span class="p">,</span> <span class="n">x3_sigma</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="mi">7</span><span class="o">*</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">predict_grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">df_dx1</span> <span class="o">=</span> <span class="mi">7</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
    <span class="n">df_dx2</span> <span class="o">=</span> <span class="o">-</span><span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
    <span class="n">df_dx3</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">df_dx1</span><span class="p">,</span> <span class="n">df_dx2</span><span class="p">,</span> <span class="n">df_dx3</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<hr />
<h2 id="partial-dependence-plot-pdp">Partial Dependence Plot (PDP)</h2>
<p>The PDP is defined as <strong><em>the average prediction over the entire dataset when setting the feature of interest at a specific value.</em></strong>
For example, the effect of the <span class="arithmatex">\(s\)</span>-th feature at values <span class="arithmatex">\(x_s\)</span> is defined as:</p>
<div class="arithmatex">\[ \text{PDP}(x_s) = \mathbb{E}_{x_c}[f(x_s, x_c)] \]</div>
<p>and is approximated by </p>
<div class="arithmatex">\[ \hat{\text{PDP}}(x_s) = \frac{1}{N} \sum_{j=1}^N f(x_s, x^{(i)}_c) \]</div>
<p>In practice, for all the dataset instances, we set the feature of interest at a specific value <span class="arithmatex">\(x_s\)</span> and we average the model's predictions.
Let's check it out the PDP effect using <code>effector</code>.</p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">PDP</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">show_avg_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">y_limits</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
<span class="n">effector</span><span class="o">.</span><span class="n">PDP</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">show_avg_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">y_limits</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
<span class="n">effector</span><span class="o">.</span><span class="n">PDP</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">show_avg_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">y_limits</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_8_0.png" /></p>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_8_1.png" /></p>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_8_2.png" /></p>
<h3 id="feature-effect-interpretation">Feature effect interpretation</h3>
<p>As we expected, all feature effects are linear. Looking closer, we can also confirm the gradients of the effects: 
<span class="arithmatex">\(7\)</span> for <span class="arithmatex">\(x_1\)</span>, <span class="arithmatex">\(-3\)</span> for <span class="arithmatex">\(x_2\)</span> and <span class="arithmatex">\(4\)</span> for <span class="arithmatex">\(x_3\)</span>.
However, you may question why there are different alignments on the y axis? For example, why <span class="arithmatex">\(x_1\)</span> starts at <span class="arithmatex">\(y=-1.5\)</span>? Does this have a natural interpretation?</p>
<p>There is no global answer to this question. This is why many people prefer to <em>center</em> PDP plots manually, as we will see below.
For linear models, the PDP plot is <span class="arithmatex">\(\text{PDP}(x_s) = a_sx_s + c\)</span> where <span class="arithmatex">\(a_s\)</span> is the gradient of the line and <span class="arithmatex">\(c\)</span> is the intercept. 
For feature <span class="arithmatex">\(x_1\)</span>, the intercept is <span class="arithmatex">\(c \approx 0.5\)</span>. 
With a closer look at the formula we can understand why this happens:</p>
<div class="arithmatex">\[PDP(x_s) = \mathbb{E}_{x_c}[f(x_s, x_c)] = a_sx_s + \sum_{j \neq s} a_j \mathbb{E}_{x_j}[x_j] = a_sx_s - 3 * 0.5 + 4 * 0.5 = a_sx_s + 0.5\]</div>
<hr />
<p>The most convenient centering of the PDP plot depends on the underlying question.
If we compare the effect of two features, then it is better to center the PDP plots around <span class="arithmatex">\(y=0\)</span> to avoid the distraction of intercepts.
If we compare the effect of a specific feature on two different subgroups (check the tutorial about Regional Effect methods), then it is better to to leave the PDP plot uncentered.</p>
<p><code>Effector</code> has three <code>cenetering</code> alternatives:</p>
<table>
<thead>
<tr>
<th><code>centering</code></th>
<th>Description</th>
<th>Formula</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>False</code></td>
<td>Don't enforce any additional centering</td>
<td>-</td>
</tr>
<tr>
<td><code>True</code> or <code>zero_integral</code></td>
<td>Center around the <span class="arithmatex">\(y\)</span> axis</td>
<td><span class="arithmatex">\(c = \mathbb{E}_{x_s \sim \mathcal{U(x_{s,min},x_{s, max})}}[PDP(x_s)]\)</span></td>
</tr>
<tr>
<td><code>zero_start</code></td>
<td>Center around <span class="arithmatex">\(y=0\)</span></td>
<td><span class="arithmatex">\(c = 0\)</span></td>
</tr>
</tbody>
</table>
<p>Below, we observe that setting <code>centering=True</code> facilitates the comparisons. </p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">PDP</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centering</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">y_limits</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">effector</span><span class="o">.</span><span class="n">PDP</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">centering</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">y_limits</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">effector</span><span class="o">.</span><span class="n">PDP</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">centering</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">y_limits</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_10_0.png" /></p>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_10_1.png" /></p>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_10_2.png" /></p>
<h3 id="heterogeneity">Heterogeneity</h3>
<p>Feature effect methods output a 1-1 plot that visualizes the <strong>average</strong> effect of a specific feature on the output; 
the averaging is performed over the instance-level effects. 
It is important, therefore, to know to what extent the underlying local (instance-level) effects deviate from the average effect.
In other words, to what extent the average effects are a good explanation for the features.</p>
<p>In our example, due to zero interactions between the features, the heterogeneity should be zero.</p>
<hr />
<p>In PDP plots there are two ways to check that, either using the ICE plots or as a <span class="arithmatex">\(\pm\)</span> interval around the average plot.</p>
<h4 id="option-a-ice-plots">Option (a): ICE plots</h4>
<p>ICE plots show the ouput of instance <span class="arithmatex">\(i\)</span> if changing the feature of interest <span class="arithmatex">\(x_s\)</span>:</p>
<div class="arithmatex">\[\text{ICE}^{(i)}(x_s, x^{(i)}_c) = f(x_s, x^{(i)}_c)\]</div>
<p>Plotting the ICE plots of many instances <span class="arithmatex">\(i\)</span> on top of the PDP, we can visually observe the heterogeneity.
For example in the plot below, we can see that there is no heterogeneity in the instance-level effects, i.e., all instance-level effects are lines with gradient 7.</p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">PDP</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centering</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">heterogeneity</span><span class="o">=</span><span class="s2">&quot;ice&quot;</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_13_0.png" /></p>
<p>Keep in mind that it is important to define correctly the argument <code>centering</code>.
Setting <code>centering=True</code> centers the PDP plot around the <span class="arithmatex">\(y\)</span> axis, which facilitates the comparison of the underlying feature effects (as above).</p>
<p>However, there are cases where the intercept maybe useful.
Imagine a case where the salary of the employees depends only (a) on the number of working hours and (b) on the gender of a person. 
If the salary per working hour does not differ between male and female employees, but male employees in general earn 1000 Euros more per month, 
then we won’t see this difference in the centered ICE curves of the feature working hours.
In contrast, this difference will be visible under the uncentered ICE curves.</p>
<p>In our example, setting <code>centering=False</code> gives the following plot; ICE plots with different intercepts but identical gradient.</p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">PDP</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centering</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">heterogeneity</span><span class="o">=</span><span class="s2">&quot;ice&quot;</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_15_0.png" /></p>
<h4 id="option-b-std-of-the-residuals">Option (b): STD of the residuals</h4>
<p>A second way to check for heterogeneity is by plotting the standard deviation of the instance-level effects as <span class="arithmatex">\(\pm\)</span> interval around the PDP plot.
This is done setting <code>confidence_interval="std"</code> in the <code>plot</code> method. 
In practice, this approach simply plots the std of the ICE plots instead of the ICE plots themselves.</p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">PDP</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centering</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">heterogeneity</span><span class="o">=</span><span class="s2">&quot;std&quot;</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_17_0.png" /></p>
<p>In this case, if we do not perform centering, it is difficult to differentiate whether the heterogeneity is provoked by the gradient or in the intercept.
Therefore, we recommend to try both the centered and the uncentered version of the ICE, before coming to a conclusion. 
If the heterogeneity is only present on the latter, then it is due to different intercepts.</p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">PDP</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centering</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">heterogeneity</span><span class="o">=</span><span class="s2">&quot;std&quot;</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_19_0.png" /></p>
<h3 id="derivative-pdp-d-pdp">Derivative-PDP (d-PDP)</h3>
<p>A similar analysis can be done using the derivative of the model; the name of this approach is Derivative-PDP (d-PDP) and the equivalent of the ICE plots are the Derivative-ICE (d-ICE) plots. The d-PDP and d-ICE are defined as:</p>
<div class="arithmatex">\[ \text{d-PDP}(x_s) = \mathbb{E}_{x_c}[\frac{\partial f}{\partial x_s} (x_s, x_c)] \approx \frac{1}{N} \sum_{j=1}^N \frac{\partial f}{\partial x_s} (x_s, x_c^{(i)}) \]</div>
<p>and </p>
<div class="arithmatex">\[ \text{d-ICE}^{(i)}(x_s) = \frac{\partial f}{\partial x_s} (x_s, x^{(i)}_c) \]</div>
<p>We have to mention that:</p>
<ul>
<li>d-PDP needs the model's gradient, which is not always available.</li>
<li>Under normal circumstances, the d-PDP and d-ICE should not be centered because the absolute value of the derivative has a natural meaning for the interpretation. In practice, d-ICE plots show variation that is only due to difference in the shapes of the curves. This is because all terms that are not related (interact) with the feature of interest will become zero when taking the derivative. The same applies for the <span class="arithmatex">\(\pm\)</span> interval around the d-PDP plot.</li>
<li>The interpretation is given in the gradient-space, so it should be treated differently. In d-PDP the plots show how much the model's prediction <em>changes</em> given a change in the feature of interest. This is different from PDP, where the plots says how much the specific feature <em>contributes</em> to the prediction. </li>
<li>d-PDP is the gradient of the PDP, i.e., <span class="arithmatex">\(\text{d-PDP}(x) = \frac{\partial \text{PDP}}{\partial x_s} (x)\)</span></li>
<li>d-ICE is the gradient of the ICE, i.e., <span class="arithmatex">\(\text{d-ICE}^{(i)}(x) = \frac{\partial \text{ICE}^{(i)}}{\partial x_s} (x)\)</span></li>
</ul>
<p>As we can see below, the standard deviation of the ICE plots is zero, because they only measure the variation of the shapes of the curves; not the variation of the intercepts.</p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">DerivativePDP</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">,</span> <span class="n">model_jac</span><span class="o">=</span><span class="n">predict_grad</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">heterogeneity</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">effector</span><span class="o">.</span><span class="n">DerivativePDP</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">,</span> <span class="n">model_jac</span><span class="o">=</span><span class="n">predict_grad</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">heterogeneity</span><span class="o">=</span><span class="s2">&quot;ice&quot;</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_21_0.png" /></p>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_21_1.png" /></p>
<h2 id="accumulated-local-effects-ale">Accumulated Local Effects (ALE)</h2>
<p>The next major category of feature effect techniques is <a href="https://christophm.github.io/interpretable-ml-book/ale.html">Accumulated Local Effects (ALE)</a>. Before we go into the specifics, let's apply the ALE plot to our example.</p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">ALE</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">effector</span><span class="o">.</span><span class="n">ALE</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">effector</span><span class="o">.</span><span class="n">ALE</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_23_0.png" /></p>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_23_1.png" /></p>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_23_2.png" /></p>
<h3 id="fearure-effect-interpretation">Fearure effect interpretation</h3>
<p>In each of the figures above, there are two subfigures; the upper subfigure is the average feature effect (the typical ALE plot) and the lower subfigure is the derivative of the effect.
The upper subfigure shows how much the feature of interest <em>contributes</em> to the prediction (like PDP) while the bottom subplot shows how much a change in the feature of interest <em>changes</em> the prediction (like d-PDP). 
For example, for <span class="arithmatex">\(x_1\)</span> the upper subplot shows a linear effect and the lower subplot confirms that the gradient is constantly <span class="arithmatex">\(7\)</span>.
<code>Effector</code> offers two alternatives for centering the ALE plot.</p>
<p><center></p>
<table>
<thead>
<tr>
<th><code>centering</code></th>
<th>Description</th>
<th>Formula</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>False</code> or <code>zero_start</code></td>
<td>Don't enforce any additional centering</td>
<td>c=0</td>
</tr>
<tr>
<td><code>True</code> or <code>zero_integral</code></td>
<td>Center around the <span class="arithmatex">\(y\)</span> axis</td>
<td>c=<span class="arithmatex">\(\mathbb{E}_{x_s \sim \mathcal{U(x_{s,min},x_{s, max})}}[ALE(x_s)]\)</span></td>
</tr>
</tbody>
</table>
<p></center>
Let's see how centering works for <span class="arithmatex">\(x_1\)</span>:</p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">ALE</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centering</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_25_0.png" /></p>
<h3 id="heterogeneity_1">Heterogeneity</h3>
<p>In ALE plots, the only way to check the heterogeneity of the instance-level effects is by plotting the standard deviation of the instance-level effects as <span class="arithmatex">\(\pm\)</span> interval around the ALE plot. In <code>Effector</code> this can be done by setting <code>heterogeneity=True"</code>. The plot below shows that the heterogeneity is zero, which is correct. However, as we will see below <a href="#robust-and-heterogeneity-aware-ale-rhale">(RHALE section)</a>, ALE's fixed size bin-splitting is not the best way to estimate the heterogeneity. In contrast, the automatic bin-splitting introduced by <a href="https://arxiv.org/abs/2309.11193">RHALE</a> provides a better estimation of the heterogeneity.</p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">ALE</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centering</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">heterogeneity</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_27_0.png" /></p>
<h3 id="bin-splitting">Bin-Splitting</h3>
<p>As you may have noticed at the bottom plots of the figures above, <span class="arithmatex">\(x_1\)</span> axis has been split in <span class="arithmatex">\(K=20\)</span> bins (intervals) of equal size. The derivative-effect is provided per bin (bin-effect), which in our example is <span class="arithmatex">\(7\)</span> for all bins. </p>
<p>In fact, bin-splitting is apparent also at the top plot; the top plot is not a line, but a piecewise linear function, where each <em>piece</em> is a line in the are covered by each bin and gradient equal to the bin-effect. However, since the bin-effect is the same for all bins, the top plot looks like a line.</p>
<p>To explain the need for bin-splitting we have to go back to the definition of ALE. ALE is defined as: </p>
<div class="arithmatex">\[\text{ALE}(x_s) = \int_{z=0}^{x_s} \mathbb{E}_{x_c|x_s=z}\left [ \frac{\partial f}{\partial x_s} (z, x_c) \right ] \partial z\]</div>
<p>Apley et. al proposed approximating the above integral by:</p>
<div class="arithmatex">\[\hat{\text{ALE}}(x_s) = \sum_{k=1}^{k_{x_s}} \frac{1}{| \mathcal{S}_k |} \sum_{i: x^{(i)} \in \mathcal{S}_k} \left [ f(z_k, x_c) - f(z_{k-1}, x_c) \right ]\]</div>
<p>where <span class="arithmatex">\(k_{x_s}\)</span> the index of the bin such that <span class="arithmatex">\(z_{k_{x−1}} ≤ x_s &lt; z_{k_x}\)</span>, <span class="arithmatex">\(\mathcal{S}_k\)</span> is the set of the instances lying at the <span class="arithmatex">\(k\)</span>-th bin, i.e., <span class="arithmatex">\(\mathcal{S}_k = \{ x^{(i)} : z_{k−1} \neq x^{(i)}_s &lt; z_k \}\)</span> and <span class="arithmatex">\(\Delta x = \frac{x_{s, max} - x_{s, min}}{K}\)</span>.</p>
<p><span class="arithmatex">\(\hat{\text{ALE}}(x_s)\)</span> uses a Riemannian sum to approximate the integral of <span class="arithmatex">\(\text{ALE}(x_s)\)</span>. The axis of the <span class="arithmatex">\(s\)</span>-th feature is split in <span class="arithmatex">\(K\)</span> bins (intervals) of equal size. In each bin, the average effect of the feature of interest is estimated using the instances that fall in the bin. The average effect in each bin is called bin-effect. The default in <code>Effector</code> is to use <span class="arithmatex">\(K=20\)</span> bins but the user can change it using:</p>
<div class="highlight"><pre><span></span><code><span class="n">ale</span> <span class="o">=</span> <span class="n">effector</span><span class="o">.</span><span class="n">ALE</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span>

<span class="c1"># using 5 bins</span>
<span class="n">bm</span> <span class="o">=</span> <span class="n">effector</span><span class="o">.</span><span class="n">binning_methods</span><span class="o">.</span><span class="n">Fixed</span><span class="p">(</span><span class="n">nof_bins</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_points_per_bin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cat_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ale</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">binning_method</span><span class="o">=</span><span class="n">bm</span><span class="p">)</span>
<span class="n">ale</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># using 100 bins</span>
<span class="n">bm</span> <span class="o">=</span> <span class="n">effector</span><span class="o">.</span><span class="n">binning_methods</span><span class="o">.</span><span class="n">Fixed</span><span class="p">(</span><span class="n">nof_bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">min_points_per_bin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cat_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ale</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">binning_method</span><span class="o">=</span><span class="n">bm</span><span class="p">)</span>
<span class="n">ale</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_29_0.png" /></p>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_29_1.png" /></p>
<h2 id="robust-and-heterogeneity-aware-ale-rhale">Robust and Heterogeneity-aware ALE (RHALE)</h2>
<p>Robust and Heterogeneity-aware ALE (RHALE) is a variant of ALE, proposed by <a href="https://arxiv.org/abs/2309.11193">Gkolemis et. al</a>. In their paper, they showed that RHALE has specific advantages over ALE: (a) it ensures on-distribution sampling (b) an unbiased estimation of the heterogeneity and (c) an optimal trade-off between bias and variance. These are achieved using an automated variable-size binning splitting approach. Let's see how it works in practice.</p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">RHALE</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">,</span> <span class="n">model_jac</span><span class="o">=</span><span class="n">predict_grad</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centering</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_avg_output</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_31_0.png" /></p>
<h3 id="fearure-effect-interpretation_1">Fearure effect interpretation</h3>
<p>The interpretation is exactly the same as with the typical ALE; The top subplot is the average feature effect and the bottom subfigure is the derivative of the effect. 
The crucial difference, is that the automatic bin-splitting approach <em>optimally</em> creates a single bin that covers the whole area between <span class="arithmatex">\(x=0\)</span> and <span class="arithmatex">\(x=1\)</span>. As we saw above, the gradient of the feature effect is constant and equal to <span class="arithmatex">\(7\)</span> for all <span class="arithmatex">\(x_1\)</span> values. Therefore, merging all bins into one, reduces the variance of the estimation; the estimation is based on more instances, so the variance is lower. </p>
<p>In our example, this advantage is not evident; Since there are no interaction terms (linear model) the effect of all instances is always the same; so the variance of the estimation is zero. However, in more complex models, the variance of the estimation is not zero and the automatic bin-splitting approach reduces the variance of the estimation (check tutorial <a href="./ale.ipynb">ALE</a> for more details).</p>
<p>As with the ALE, there are two alternatives for centering the ALE plot.</p>
<p><center></p>
<table>
<thead>
<tr>
<th><code>centering</code></th>
<th>Description</th>
<th>Formula</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>False</code> or <code>zero_start</code></td>
<td>Don't enforce any additional centering</td>
<td>c=0</td>
</tr>
<tr>
<td><code>True</code> or <code>zero-integral</code></td>
<td>Center around the <span class="arithmatex">\(y\)</span> axis</td>
<td>c=<span class="arithmatex">\(\mathbb{E}_{x_s \sim \mathcal{U(x_{s,min},x_{s, max})}}[ALE(x_s)]\)</span></td>
</tr>
</tbody>
</table>
<p></center></p>
<p>Let's see how this works for <span class="arithmatex">\(x_1\)</span>:</p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">RHALE</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">,</span> <span class="n">model_jac</span><span class="o">=</span><span class="n">predict_grad</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centering</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_avg_output</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_33_0.png" /></p>
<h3 id="heterogeneity_2">Heterogeneity</h3>
<p>As before, the heterogeneity is given by the the standard deviation of the instance-level effects as <span class="arithmatex">\(\pm\)</span> interval around the ALE plot.
It is important to notice, that automatic bin-splitting provides a better estimation of the heterogeneity, compared to the equisized binning method used by ALE. (check tutorial <a href="./ale.ipynb">ALE</a> for more details). 
The plot below correctly informs shows that the heterogeneity is zero.</p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">RHALE</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">,</span> <span class="n">model_jac</span><span class="o">=</span><span class="n">predict_grad</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centering</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">heterogeneity</span><span class="o">=</span><span class="s2">&quot;std&quot;</span><span class="p">,</span> <span class="n">show_avg_output</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_35_0.png" /></p>
<h3 id="bin-splitting_1">Bin-Splitting</h3>
<p>So how the automatic bin-splitting works? </p>
<div class="arithmatex">\[\text{ALE}(x_s) = \int_{z=0}^{x_s} \mathbb{E}_{x_c|x_s=z}\left [ \frac{\partial f}{\partial x_s} (z, x_c) \right ] \partial z\]</div>
<p>and is approximated by:</p>
<div class="arithmatex">\[\hat{\text{RHALE}}(x_s) = \sum_{k=1}^{k_{x_s}} \frac{1}{ \left | \mathcal{S}_k \right |} \sum_{i: x^{(i)} \in \mathcal{S}_k} \frac{\partial f}{\partial x_s} (x_s^{(i)}, x_c^{(i)})\]</div>
<p>The above approximation uses a Riemannian sum to approximate the integral. The axis of the <span class="arithmatex">\(s\)</span>-th feature is split in <span class="arithmatex">\(K\)</span> bins (intervals) of equal size. In each bin, the average effect of the feature of interest is estimated using the instances that fall in the bin. The average effect in each bin is called bin-effect. </p>
<p>But what we saw above is different. In the figure above, only one bin has been created and covers the whole area between <span class="arithmatex">\(x=0\)</span> and <span class="arithmatex">\(x=1\)</span>. 
This is because the default behaviour of <code>Effector</code> is to use an automatic bin-splitting method, as it was proposed by <a href="https://arxiv.org/abs/2309.11193">Gkolemis et. al</a>.
For more details about that, you can check the in-depth <a href="./ale.ipynb">ALE tutorial</a>.</p>
<h2 id="shap-dependence-plot">SHAP Dependence Plot</h2>
<p>TODO add intro</p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">SHAPDependence</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centering</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_avg_output</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_38_0.png" /></p>
<h3 id="fearure-effect-interpretation_2">Fearure effect interpretation</h3>
<p>TODO add content </p>
<p><center></p>
<table>
<thead>
<tr>
<th><code>centering</code></th>
<th>Description</th>
<th>Formula</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>False</code> or <code>zero_start</code></td>
<td>Don't enforce any additional centering</td>
<td>c=0</td>
</tr>
<tr>
<td><code>True</code> or <code>zero-integral</code></td>
<td>Center around the <span class="arithmatex">\(y\)</span> axis</td>
<td>c=<span class="arithmatex">\(\mathbb{E}_{x_s \sim \mathcal{U(x_{s,min},x_{s, max})}}[ALE(x_s)]\)</span></td>
</tr>
</tbody>
</table>
<p></center></p>
<p>Let's see how this works for <span class="arithmatex">\(x_1\)</span>:</p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">SHAPDependence</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centering</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_avg_output</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_40_0.png" /></p>
<h3 id="heterogeneity_3">Heterogeneity</h3>
<p>As before, the heterogeneity is given by the the standard deviation of the instance-level effects as <span class="arithmatex">\(\pm\)</span> interval around the ALE plot.
It is important to notice, that automatic bin-splitting provides a better estimation of the heterogeneity, compared to the equisized binning method used by ALE. (check tutorial <a href="./ale.ipynb">ALE</a> for more details). 
The plot below correctly informs shows that the heterogeneity is zero.</p>
<div class="highlight"><pre><span></span><code><span class="n">effector</span><span class="o">.</span><span class="n">SHAPDependence</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">heterogeneity</span><span class="o">=</span><span class="s2">&quot;shap_values&quot;</span><span class="p">)</span>
<span class="n">effector</span><span class="o">.</span><span class="n">SHAPDependence</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">predict</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">heterogeneity</span><span class="o">=</span><span class="s2">&quot;std&quot;</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_42_0.png" /></p>
<p><img alt="png" src="../01_linear_model_files/01_linear_model_42_1.png" /></p>
<h2 id="conclusion">Conclusion</h2>
<p>In this tutorial, we introduced the various feature effect methods of <code>Effector</code> and used them to explain a linear model. </p>
<p>In summary, given a dataset <code>X: (N, D)</code> and a black-box model <code>model: (N, D) -&gt; (N)</code>,
the feature effect plot of the <span class="arithmatex">\(s\)</span>-th feature <code>feature=s</code> is given with the table below.
The argument <code>confidence_interval=True|False</code> indicates whether to plot the standard deviation of the instance-level effects as <span class="arithmatex">\(\pm\)</span> interval around the feature effect plot. Some methods also require the gradient of the model <code>model_jac: (N, D) -&gt; (N, D)</code>.</p>
<p><center></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>How to use</th>
</tr>
</thead>
<tbody>
<tr>
<td>PDP</td>
<td><a href="(./../../reference/#effector.pdp.PDP)"><code>effector.PDP(X, model).plot(feature, centering, confidence_interval)</code></a></td>
</tr>
<tr>
<td>d-PDP</td>
<td><a href="./../../reference/#effector.pdp.DerivativePDP"><code>effector.DerivativePDP(X, model, model_jac).plot(feature, centering, confidence_interval)</code></a></td>
</tr>
<tr>
<td>ALE</td>
<td><a href="./../../reference/#effector.ale.ALE"><code>effector.ALE(X, model).plot(feature, centering, confidence_interval)</code></a></td>
</tr>
<tr>
<td>RHALE</td>
<td><a href="./../../reference/#effector.ale.RHALE"><code>effector.RHALE(X, model, model_jac).plot(feature, centering, confidence_interval)</code></a></td>
</tr>
</tbody>
</table>
<p></center></p>
<div class="highlight"><pre><span></span><code>
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../../real-examples/01_bike_sharing_dataset/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Bike-Sharing Dataset">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Bike-Sharing Dataset
              </div>
            </div>
          </a>
        
        
          
          <a href="../02_rhale_vs_ale_vs_pdp/" class="md-footer__link md-footer__link--next" aria-label="Next: RHALE vs ALE vs PDP">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                RHALE vs ALE vs PDP
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/givasile/effector" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.footer", "header.autohide"], "search": "../../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.d7c377c4.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>